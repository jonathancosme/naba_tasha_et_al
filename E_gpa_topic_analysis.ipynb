{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "98b735a2-ca99-4d15-a306-874730cb483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ee33bb7-41f6-48c2-babc-c48ebd3ce363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "naba_filename = './output_files/naba_data_removed_duplicates.csv'\n",
    "reg_filename = './output_files/reg_inputs_df.csv'\n",
    "tar_col_a = 'Overall GPA (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")'\n",
    "tar_col_b = 'GPA for your Major (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")'\n",
    "tar_col_a_norm = 'overall_gpa'\n",
    "tar_col_b_norm = 'major_gpa'\n",
    "topics_filename ='./output_files/norm_topic_count_df.csv'\n",
    "\n",
    "p_alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b731f-14b3-4996-8371-83f69b0343bf",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1daf457-74ba-4230-9494-62581ed4b358",
   "metadata": {},
   "source": [
    "## DataFrame of normalized variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c08ce41d-7034-45ef-b952-77a44a4e17c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_male</th>\n",
       "      <th>clasf_freshman</th>\n",
       "      <th>clasf_graduate student</th>\n",
       "      <th>clasf_junior</th>\n",
       "      <th>clasf_other</th>\n",
       "      <th>clasf_senior</th>\n",
       "      <th>clasf_sophomore</th>\n",
       "      <th>is_accounting_major</th>\n",
       "      <th>is_business_major</th>\n",
       "      <th>is_finance_major</th>\n",
       "      <th>...</th>\n",
       "      <th>is_internship_interested</th>\n",
       "      <th>is_internship_applied</th>\n",
       "      <th>is_job_accept</th>\n",
       "      <th>is_internship_accept</th>\n",
       "      <th>did_cpa_review</th>\n",
       "      <th>major_gpa</th>\n",
       "      <th>overall_gpa</th>\n",
       "      <th>len_of_extra_curr_entry</th>\n",
       "      <th>len_of_honors_entry</th>\n",
       "      <th>len_of_para_entry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.171818</td>\n",
       "      <td>-0.286653</td>\n",
       "      <td>-0.712752</td>\n",
       "      <td>-0.494198</td>\n",
       "      <td>0.835078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.764447</td>\n",
       "      <td>-0.762381</td>\n",
       "      <td>1.883098</td>\n",
       "      <td>0.963549</td>\n",
       "      <td>-1.919349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.055400</td>\n",
       "      <td>-0.139157</td>\n",
       "      <td>-0.809452</td>\n",
       "      <td>-0.457995</td>\n",
       "      <td>-1.323172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.383338</td>\n",
       "      <td>0.068585</td>\n",
       "      <td>-0.727794</td>\n",
       "      <td>-0.433861</td>\n",
       "      <td>0.462275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.764447</td>\n",
       "      <td>0.629487</td>\n",
       "      <td>0.191927</td>\n",
       "      <td>-0.120107</td>\n",
       "      <td>0.976805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.108570</td>\n",
       "      <td>-0.049828</td>\n",
       "      <td>-0.207765</td>\n",
       "      <td>0.241916</td>\n",
       "      <td>0.941373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.039003</td>\n",
       "      <td>-0.243028</td>\n",
       "      <td>-0.278678</td>\n",
       "      <td>0.932173</td>\n",
       "      <td>0.184984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.124967</td>\n",
       "      <td>0.297101</td>\n",
       "      <td>-0.581670</td>\n",
       "      <td>-0.629353</td>\n",
       "      <td>0.514653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.334147</td>\n",
       "      <td>0.006262</td>\n",
       "      <td>2.486934</td>\n",
       "      <td>3.869388</td>\n",
       "      <td>0.679487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.764447</td>\n",
       "      <td>0.899551</td>\n",
       "      <td>1.900289</td>\n",
       "      <td>-0.629353</td>\n",
       "      <td>0.015528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     is_male  clasf_freshman  clasf_graduate student  clasf_junior  \\\n",
       "0          1               0                       0             0   \n",
       "1          0               1                       0             0   \n",
       "2          0               1                       0             0   \n",
       "3          0               0                       0             0   \n",
       "4          1               0                       0             0   \n",
       "..       ...             ...                     ...           ...   \n",
       "195        1               0                       1             0   \n",
       "196        0               0                       0             1   \n",
       "197        0               0                       0             1   \n",
       "198        0               0                       0             1   \n",
       "199        0               0                       1             0   \n",
       "\n",
       "     clasf_other  clasf_senior  clasf_sophomore  is_accounting_major  \\\n",
       "0              0             1                0                    1   \n",
       "1              0             0                0                    0   \n",
       "2              0             0                0                    1   \n",
       "3              0             0                1                    1   \n",
       "4              0             0                1                    1   \n",
       "..           ...           ...              ...                  ...   \n",
       "195            0             0                0                    0   \n",
       "196            0             0                0                    0   \n",
       "197            0             0                0                    1   \n",
       "198            0             0                0                    0   \n",
       "199            0             0                0                    0   \n",
       "\n",
       "     is_business_major  is_finance_major  ...  is_internship_interested  \\\n",
       "0                    0                 0  ...                         1   \n",
       "1                    1                 0  ...                         1   \n",
       "2                    0                 0  ...                         1   \n",
       "3                    0                 0  ...                         0   \n",
       "4                    0                 0  ...                         1   \n",
       "..                 ...               ...  ...                       ...   \n",
       "195                  1                 0  ...                         0   \n",
       "196                  1                 0  ...                         1   \n",
       "197                  0                 0  ...                         1   \n",
       "198                  0                 1  ...                         1   \n",
       "199                  1                 0  ...                         1   \n",
       "\n",
       "     is_internship_applied  is_job_accept  is_internship_accept  \\\n",
       "0                        0              0                     0   \n",
       "1                        1              0                     1   \n",
       "2                        1              0                     0   \n",
       "3                        1              0                     1   \n",
       "4                        0              0                     0   \n",
       "..                     ...            ...                   ...   \n",
       "195                      1              0                     1   \n",
       "196                      1              0                     1   \n",
       "197                      0              0                     0   \n",
       "198                      1              0                     1   \n",
       "199                      1              0                     1   \n",
       "\n",
       "     did_cpa_review  major_gpa  overall_gpa  len_of_extra_curr_entry  \\\n",
       "0                 0  -0.171818    -0.286653                -0.712752   \n",
       "1                 0   0.764447    -0.762381                 1.883098   \n",
       "2                 0  -0.055400    -0.139157                -0.809452   \n",
       "3                 0  -0.383338     0.068585                -0.727794   \n",
       "4                 0   0.764447     0.629487                 0.191927   \n",
       "..              ...        ...          ...                      ...   \n",
       "195               1   0.108570    -0.049828                -0.207765   \n",
       "196               0  -0.039003    -0.243028                -0.278678   \n",
       "197               0   0.124967     0.297101                -0.581670   \n",
       "198               0  -0.334147     0.006262                 2.486934   \n",
       "199               0   0.764447     0.899551                 1.900289   \n",
       "\n",
       "     len_of_honors_entry  len_of_para_entry  \n",
       "0              -0.494198           0.835078  \n",
       "1               0.963549          -1.919349  \n",
       "2              -0.457995          -1.323172  \n",
       "3              -0.433861           0.462275  \n",
       "4              -0.120107           0.976805  \n",
       "..                   ...                ...  \n",
       "195             0.241916           0.941373  \n",
       "196             0.932173           0.184984  \n",
       "197            -0.629353           0.514653  \n",
       "198             3.869388           0.679487  \n",
       "199            -0.629353           0.015528  \n",
       "\n",
       "[200 rows x 27 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_df = pd.read_csv(reg_filename)\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac535aed-23c4-4ee2-8007-5d6ca926e4fd",
   "metadata": {},
   "source": [
    "## DataFrame of raw variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3d954c8-beef-4549-b9a4-0de622d74a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>Gender:* Required fields are indicated with red symbol Permanent Contact Information</th>\n",
       "      <th>City (Permanent):* Required fields are indicated with red symbol Permanent Contact Information</th>\n",
       "      <th>State (Permanent):* Required fields are indicated with red symbol Permanent Contact Information</th>\n",
       "      <th>Zip (Permanent):* Required fields are indicated with red symbol Permanent Contact Information</th>\n",
       "      <th>Are you Black? (includes African, African American, Caribbean, etc.)</th>\n",
       "      <th>Preferred Mailing Address</th>\n",
       "      <th>College/University:Academic Profile</th>\n",
       "      <th>Classification (as of January 2022):Academic Profile</th>\n",
       "      <th>Major:Academic Profile</th>\n",
       "      <th>...</th>\n",
       "      <th>Please provide details (i.e. company name, location, etc.)</th>\n",
       "      <th>I have accepted an internship for the summer of 2022 (June - August)</th>\n",
       "      <th>I have accepted an internship for the fall of 2022 (September- December)</th>\n",
       "      <th>Please provide details (i.e. company name, location, etc.).1</th>\n",
       "      <th>I have accepted a permanent job offer</th>\n",
       "      <th>Please provide details (i.e. company name, location, etc.).2</th>\n",
       "      <th>Have you received a CPA Exam Review?</th>\n",
       "      <th>Which CPA Exam Review have you received?</th>\n",
       "      <th>Copy/ PasteÂ An Essay Response of 500 words or less (copy and paste in webform) using the following prompt: Community disruptions such as Covid-19 and other natural disasters can have deep lasting impacts. Discuss a challenge or barrier you have overcome during the Covid-19 pandemic.</th>\n",
       "      <th>Recipient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>NY</td>\n",
       "      <td>11216</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>Medgar Evers College</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>During the early parts of 2020 one of the dead...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>Bowie</td>\n",
       "      <td>MD</td>\n",
       "      <td>20720</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>University of Maryland</td>\n",
       "      <td>Freshman</td>\n",
       "      <td>Business Management</td>\n",
       "      <td>...</td>\n",
       "      <td>Received internship as a Summer 2022 Discovery...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One of the most challenging times has been dur...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chicago Heights</td>\n",
       "      <td>IL</td>\n",
       "      <td>60411</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>North Carolina A&amp;T State University</td>\n",
       "      <td>Freshman</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When covid restrictions took place I was just ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>60620</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>Loyola University Chicago</td>\n",
       "      <td>Sophomore</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>...</td>\n",
       "      <td>Ernst &amp; Young, Chicago, Summer 2022</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Ernst &amp; Young, Chicago, Summer 2022</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the Covid-19 pandemic first began, I was ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>Baton Rouge</td>\n",
       "      <td>LA</td>\n",
       "      <td>70806</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Campus/Temporary</td>\n",
       "      <td>Penn State University</td>\n",
       "      <td>Sophomore</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Community disruptions such as Covid-19 and oth...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>199</td>\n",
       "      <td>Male</td>\n",
       "      <td>Middletown</td>\n",
       "      <td>DE</td>\n",
       "      <td>19709</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>West Chester University</td>\n",
       "      <td>Graduate Student</td>\n",
       "      <td>Masters of Business Administration</td>\n",
       "      <td>...</td>\n",
       "      <td>Ernst and Young, Philadelphia, Pa, Financial C...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Ernst and Young, Philadelphia, Pa, Financial C...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Becker</td>\n",
       "      <td>There are many issues that are impacting the a...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>200</td>\n",
       "      <td>Female</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>NY</td>\n",
       "      <td>11210</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>University at Albany, SUNY</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Business Administration</td>\n",
       "      <td>...</td>\n",
       "      <td>JPMorgan Chase &amp; Co., NYC</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>JPMorgan Chase &amp; Co., NYC</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My name is Wunmi Surakat, a current 1st semest...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>201</td>\n",
       "      <td>Female</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>NY</td>\n",
       "      <td>11212</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>Medgar Evers College</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No one expected the Covid- 19 pandemic. Since ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>202</td>\n",
       "      <td>Female</td>\n",
       "      <td>Valdosta</td>\n",
       "      <td>GA</td>\n",
       "      <td>31601</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>Valdosta State University</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Finance</td>\n",
       "      <td>...</td>\n",
       "      <td>J.P Morgan Summer 2022, Atlanta office, Summer...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>J.P. Morgan, Middle-Market Bank, Atlanta offic...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>During the COVID-19 pandemic, my entire life c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>203</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>FL</td>\n",
       "      <td>32221</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Campus/Temporary</td>\n",
       "      <td>Florida A&amp;M University</td>\n",
       "      <td>Graduate Student</td>\n",
       "      <td>Business Administration</td>\n",
       "      <td>...</td>\n",
       "      <td>Deloitte LLP - Houston</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Deloitte LLP - Houston</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Covid-19 pandemic is something that has af...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     app_id  \\\n",
       "0         0   \n",
       "1         1   \n",
       "2         2   \n",
       "3         3   \n",
       "4         4   \n",
       "..      ...   \n",
       "195     199   \n",
       "196     200   \n",
       "197     201   \n",
       "198     202   \n",
       "199     203   \n",
       "\n",
       "    Gender:* Required fields are indicated with red symbol Permanent Contact Information  \\\n",
       "0                                                 Male                                     \n",
       "1                                               Female                                     \n",
       "2                                               Female                                     \n",
       "3                                               Female                                     \n",
       "4                                                 Male                                     \n",
       "..                                                 ...                                     \n",
       "195                                               Male                                     \n",
       "196                                             Female                                     \n",
       "197                                             Female                                     \n",
       "198                                             Female                                     \n",
       "199                                             Female                                     \n",
       "\n",
       "    City (Permanent):* Required fields are indicated with red symbol Permanent Contact Information   \\\n",
       "0                                             Brooklyn                                                \n",
       "1                                                Bowie                                                \n",
       "2                                      Chicago Heights                                                \n",
       "3                                              Chicago                                                \n",
       "4                                          Baton Rouge                                                \n",
       "..                                                 ...                                                \n",
       "195                                         Middletown                                                \n",
       "196                                           Brooklyn                                                \n",
       "197                                           Brooklyn                                                \n",
       "198                                           Valdosta                                                \n",
       "199                                       Jacksonville                                                \n",
       "\n",
       "    State (Permanent):* Required fields are indicated with red symbol Permanent Contact Information  \\\n",
       "0                                                   NY                                                \n",
       "1                                                   MD                                                \n",
       "2                                                   IL                                                \n",
       "3                                                   IL                                                \n",
       "4                                                   LA                                                \n",
       "..                                                 ...                                                \n",
       "195                                                 DE                                                \n",
       "196                                                 NY                                                \n",
       "197                                                 NY                                                \n",
       "198                                                 GA                                                \n",
       "199                                                 FL                                                \n",
       "\n",
       "    Zip (Permanent):* Required fields are indicated with red symbol Permanent Contact Information  \\\n",
       "0                                                11216                                              \n",
       "1                                                20720                                              \n",
       "2                                                60411                                              \n",
       "3                                                60620                                              \n",
       "4                                                70806                                              \n",
       "..                                                 ...                                              \n",
       "195                                              19709                                              \n",
       "196                                              11210                                              \n",
       "197                                              11212                                              \n",
       "198                                              31601                                              \n",
       "199                                              32221                                              \n",
       "\n",
       "    Are you Black? (includes African, African American, Caribbean, etc.)  \\\n",
       "0                                                  Yes                     \n",
       "1                                                  Yes                     \n",
       "2                                                  Yes                     \n",
       "3                                                  Yes                     \n",
       "4                                                  Yes                     \n",
       "..                                                 ...                     \n",
       "195                                                Yes                     \n",
       "196                                                Yes                     \n",
       "197                                                Yes                     \n",
       "198                                                Yes                     \n",
       "199                                                Yes                     \n",
       "\n",
       "    Preferred Mailing Address  College/University:Academic Profile  \\\n",
       "0                   Permanent                 Medgar Evers College   \n",
       "1                   Permanent               University of Maryland   \n",
       "2                   Permanent  North Carolina A&T State University   \n",
       "3                   Permanent            Loyola University Chicago   \n",
       "4            Campus/Temporary                Penn State University   \n",
       "..                        ...                                  ...   \n",
       "195                 Permanent              West Chester University   \n",
       "196                 Permanent           University at Albany, SUNY   \n",
       "197                 Permanent                 Medgar Evers College   \n",
       "198                 Permanent            Valdosta State University   \n",
       "199          Campus/Temporary               Florida A&M University   \n",
       "\n",
       "    Classification (as of January 2022):Academic Profile  \\\n",
       "0                                               Senior     \n",
       "1                                             Freshman     \n",
       "2                                             Freshman     \n",
       "3                                            Sophomore     \n",
       "4                                            Sophomore     \n",
       "..                                                 ...     \n",
       "195                                   Graduate Student     \n",
       "196                                             Junior     \n",
       "197                                             Junior     \n",
       "198                                             Junior     \n",
       "199                                   Graduate Student     \n",
       "\n",
       "                 Major:Academic Profile  ...  \\\n",
       "0                            Accounting  ...   \n",
       "1                   Business Management  ...   \n",
       "2                            Accounting  ...   \n",
       "3                            Accounting  ...   \n",
       "4                            Accounting  ...   \n",
       "..                                  ...  ...   \n",
       "195  Masters of Business Administration  ...   \n",
       "196             Business Administration  ...   \n",
       "197                          Accounting  ...   \n",
       "198                             Finance  ...   \n",
       "199             Business Administration  ...   \n",
       "\n",
       "    Please provide details (i.e. company name, location, etc.)  \\\n",
       "0                                                  NaN           \n",
       "1    Received internship as a Summer 2022 Discovery...           \n",
       "2                                                  NaN           \n",
       "3                  Ernst & Young, Chicago, Summer 2022           \n",
       "4                                                  NaN           \n",
       "..                                                 ...           \n",
       "195  Ernst and Young, Philadelphia, Pa, Financial C...           \n",
       "196                          JPMorgan Chase & Co., NYC           \n",
       "197                                                NaN           \n",
       "198  J.P Morgan Summer 2022, Atlanta office, Summer...           \n",
       "199                             Deloitte LLP - Houston           \n",
       "\n",
       "    I have accepted an internship for the summer of 2022 (June - August)  \\\n",
       "0                                                  NaN                     \n",
       "1                                                  Yes                     \n",
       "2                                                   No                     \n",
       "3                                                  Yes                     \n",
       "4                                                  NaN                     \n",
       "..                                                 ...                     \n",
       "195                                                Yes                     \n",
       "196                                                Yes                     \n",
       "197                                                NaN                     \n",
       "198                                                Yes                     \n",
       "199                                                Yes                     \n",
       "\n",
       "    I have accepted an internship for the fall of 2022 (September- December)  \\\n",
       "0                                                  NaN                         \n",
       "1                                                   No                         \n",
       "2                                                   No                         \n",
       "3                                                   No                         \n",
       "4                                                  NaN                         \n",
       "..                                                 ...                         \n",
       "195                                                 No                         \n",
       "196                                                 No                         \n",
       "197                                                NaN                         \n",
       "198                                                 No                         \n",
       "199                                                 No                         \n",
       "\n",
       "     Please provide details (i.e. company name, location, etc.).1  \\\n",
       "0                                                  NaN              \n",
       "1                                                  NaN              \n",
       "2                                                  NaN              \n",
       "3                  Ernst & Young, Chicago, Summer 2022              \n",
       "4                                                  NaN              \n",
       "..                                                 ...              \n",
       "195  Ernst and Young, Philadelphia, Pa, Financial C...              \n",
       "196                          JPMorgan Chase & Co., NYC              \n",
       "197                                                NaN              \n",
       "198  J.P. Morgan, Middle-Market Bank, Atlanta offic...              \n",
       "199                             Deloitte LLP - Houston              \n",
       "\n",
       "     I have accepted a permanent job offer  \\\n",
       "0                                       No   \n",
       "1                                       No   \n",
       "2                                       No   \n",
       "3                                       No   \n",
       "4                                       No   \n",
       "..                                     ...   \n",
       "195                                     No   \n",
       "196                                     No   \n",
       "197                                     No   \n",
       "198                                     No   \n",
       "199                                     No   \n",
       "\n",
       "    Please provide details (i.e. company name, location, etc.).2  \\\n",
       "0                                                  NaN             \n",
       "1                                                  NaN             \n",
       "2                                                  NaN             \n",
       "3                                                  NaN             \n",
       "4                                                  NaN             \n",
       "..                                                 ...             \n",
       "195                                                NaN             \n",
       "196                                                NaN             \n",
       "197                                                NaN             \n",
       "198                                                NaN             \n",
       "199                                                NaN             \n",
       "\n",
       "    Have you received a CPA Exam Review?  \\\n",
       "0                                     No   \n",
       "1                                     No   \n",
       "2                                     No   \n",
       "3                                     No   \n",
       "4                                     No   \n",
       "..                                   ...   \n",
       "195                                  Yes   \n",
       "196                                   No   \n",
       "197                                   No   \n",
       "198                                   No   \n",
       "199                                   No   \n",
       "\n",
       "    Which CPA Exam Review have you received?  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "..                                       ...   \n",
       "195                                   Becker   \n",
       "196                                      NaN   \n",
       "197                                      NaN   \n",
       "198                                      NaN   \n",
       "199                                      NaN   \n",
       "\n",
       "    Copy/ PasteÂ An Essay Response of 500 words or less (copy and paste in webform) using the following prompt: Community disruptions such as Covid-19 and other natural disasters can have deep lasting impacts. Discuss a challenge or barrier you have overcome during the Covid-19 pandemic.  \\\n",
       "0    During the early parts of 2020 one of the dead...                                                                                                                                                                                                                                             \n",
       "1    One of the most challenging times has been dur...                                                                                                                                                                                                                                             \n",
       "2    When covid restrictions took place I was just ...                                                                                                                                                                                                                                             \n",
       "3    When the Covid-19 pandemic first began, I was ...                                                                                                                                                                                                                                             \n",
       "4    Community disruptions such as Covid-19 and oth...                                                                                                                                                                                                                                             \n",
       "..                                                 ...                                                                                                                                                                                                                                             \n",
       "195  There are many issues that are impacting the a...                                                                                                                                                                                                                                             \n",
       "196  My name is Wunmi Surakat, a current 1st semest...                                                                                                                                                                                                                                             \n",
       "197  No one expected the Covid- 19 pandemic. Since ...                                                                                                                                                                                                                                             \n",
       "198  During the COVID-19 pandemic, my entire life c...                                                                                                                                                                                                                                             \n",
       "199  The Covid-19 pandemic is something that has af...                                                                                                                                                                                                                                             \n",
       "\n",
       "    Recipient  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         Yes  \n",
       "..        ...  \n",
       "195       Yes  \n",
       "196       NaN  \n",
       "197       NaN  \n",
       "198       NaN  \n",
       "199       Yes  \n",
       "\n",
       "[200 rows x 37 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv(naba_filename)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284a440-f961-4649-9f80-668245c94471",
   "metadata": {},
   "source": [
    "## DataFrame of normalized topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56003dc1-b9d3-4510-bf51-643004d6315e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_64</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>topic_71</th>\n",
       "      <th>topic_72</th>\n",
       "      <th>topic_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "0    0.263158  0.052632  0.000000  0.052632  0.052632  0.000000  0.052632   \n",
       "1    0.416667  0.000000  0.000000  0.083333  0.083333  0.083333  0.000000   \n",
       "2    0.363636  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3    0.055556  0.000000  0.000000  0.000000  0.055556  0.000000  0.000000   \n",
       "4    0.333333  0.166667  0.000000  0.066667  0.000000  0.033333  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.043478  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "196  0.181818  0.045455  0.000000  0.000000  0.000000  0.000000  0.045455   \n",
       "197  0.173913  0.043478  0.000000  0.043478  0.000000  0.000000  0.000000   \n",
       "198  0.321429  0.035714  0.071429  0.000000  0.000000  0.000000  0.000000   \n",
       "199  0.125000  0.000000  0.000000  0.062500  0.000000  0.000000  0.000000   \n",
       "\n",
       "      topic_7   topic_8   topic_9  ...  topic_64  topic_65  topic_66  \\\n",
       "0    0.000000  0.052632  0.000000  ...  0.000000       0.0       0.0   \n",
       "1    0.000000  0.000000  0.000000  ...  0.000000       0.0       0.0   \n",
       "2    0.000000  0.000000  0.000000  ...  0.000000       0.0       0.0   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000       0.0       0.0   \n",
       "4    0.033333  0.033333  0.000000  ...  0.000000       0.0       0.0   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "195  0.000000  0.000000  0.000000  ...  0.043478       0.0       0.0   \n",
       "196  0.000000  0.045455  0.045455  ...  0.000000       0.0       0.0   \n",
       "197  0.000000  0.000000  0.000000  ...  0.000000       0.0       0.0   \n",
       "198  0.000000  0.000000  0.000000  ...  0.000000       0.0       0.0   \n",
       "199  0.062500  0.000000  0.000000  ...  0.000000       0.0       0.0   \n",
       "\n",
       "     topic_67  topic_68  topic_69  topic_70  topic_71  topic_72  topic_73  \n",
       "0    0.000000       0.0  0.000000  0.000000       0.0  0.000000  0.052632  \n",
       "1    0.000000       0.0  0.000000  0.000000       0.0  0.000000  0.000000  \n",
       "2    0.000000       0.0  0.000000  0.000000       0.0  0.000000  0.000000  \n",
       "3    0.000000       0.0  0.055556  0.000000       0.0  0.055556  0.000000  \n",
       "4    0.000000       0.0  0.000000  0.033333       0.0  0.000000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  0.000000       0.0  0.000000  0.086957       0.0  0.000000  0.000000  \n",
       "196  0.045455       0.0  0.000000  0.000000       0.0  0.000000  0.000000  \n",
       "197  0.000000       0.0  0.043478  0.000000       0.0  0.000000  0.000000  \n",
       "198  0.000000       0.0  0.000000  0.000000       0.0  0.000000  0.000000  \n",
       "199  0.000000       0.0  0.000000  0.000000       0.0  0.000000  0.000000  \n",
       "\n",
       "[200 rows x 74 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df = pd.read_csv(topics_filename)\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af10d9-3b2a-4f50-a8ab-2b32f54b0b4c",
   "metadata": {},
   "source": [
    "# Create variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6865c6-5262-4c6d-a11d-b72a6245d05b",
   "metadata": {},
   "source": [
    "## boolean topics dataframe (1 if topic is included, 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3bea3330-8b01-4704-a49a-9ed72bf7748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_64</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>topic_71</th>\n",
       "      <th>topic_72</th>\n",
       "      <th>topic_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7  \\\n",
       "0        1.0      1.0      0.0      1.0      1.0      0.0      1.0      0.0   \n",
       "1        1.0      0.0      0.0      1.0      1.0      1.0      0.0      0.0   \n",
       "2        1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3        1.0      0.0      0.0      0.0      1.0      0.0      0.0      0.0   \n",
       "4        1.0      1.0      0.0      1.0      0.0      1.0      0.0      1.0   \n",
       "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "195      1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "196      1.0      1.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "197      1.0      1.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "198      1.0      1.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "199      1.0      0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "     topic_8  topic_9  ...  topic_64  topic_65  topic_66  topic_67  topic_68  \\\n",
       "0        1.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1        0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "2        0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "3        0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4        1.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "..       ...      ...  ...       ...       ...       ...       ...       ...   \n",
       "195      0.0      0.0  ...       1.0       0.0       0.0       0.0       0.0   \n",
       "196      1.0      1.0  ...       0.0       0.0       0.0       1.0       0.0   \n",
       "197      0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "198      0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "199      0.0      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "     topic_69  topic_70  topic_71  topic_72  topic_73  \n",
       "0         0.0       0.0       0.0       0.0       1.0  \n",
       "1         0.0       0.0       0.0       0.0       0.0  \n",
       "2         0.0       0.0       0.0       0.0       0.0  \n",
       "3         1.0       0.0       0.0       1.0       0.0  \n",
       "4         0.0       1.0       0.0       0.0       0.0  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "195       0.0       1.0       0.0       0.0       0.0  \n",
       "196       0.0       0.0       0.0       0.0       0.0  \n",
       "197       1.0       0.0       0.0       0.0       0.0  \n",
       "198       0.0       0.0       0.0       0.0       0.0  \n",
       "199       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[200 rows x 74 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_bool_df = (topics_df > 0).astype(float)\n",
    "topics_bool_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6fc27-77b4-47a8-8f59-40e327b07f21",
   "metadata": {},
   "source": [
    "## target variables for *normalized* **overall** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "894984f5-8665-4dbd-856f-c4416e41da60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.286653\n",
       "1     -0.762381\n",
       "2     -0.139157\n",
       "3      0.068585\n",
       "4      0.629487\n",
       "         ...   \n",
       "195   -0.049828\n",
       "196   -0.243028\n",
       "197    0.297101\n",
       "198    0.006262\n",
       "199    0.899551\n",
       "Name: overall_gpa, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_a_norm_df = reg_df[tar_col_a_norm]\n",
    "tar_a_norm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a3cf1-86eb-4495-ad98-3123689fa824",
   "metadata": {},
   "source": [
    "## target variable for *normalized* **major** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2a596fa7-519a-4faa-bb89-769f9c0c833b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.171818\n",
       "1      0.764447\n",
       "2     -0.055400\n",
       "3     -0.383338\n",
       "4      0.764447\n",
       "         ...   \n",
       "195    0.108570\n",
       "196   -0.039003\n",
       "197    0.124967\n",
       "198   -0.334147\n",
       "199    0.764447\n",
       "Name: major_gpa, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_b_norm_df = reg_df[tar_col_b_norm]\n",
    "tar_b_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee410665-4ea2-4f5a-aef0-cde2bf82dd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042d795a-80a3-45be-a1ee-ba8c071256e2",
   "metadata": {},
   "source": [
    "# target variables for *raw* **overall** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2bbc3cc9-75eb-44d9-a9ce-96b0922bc07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3.429\n",
       "1      3.200\n",
       "2      3.500\n",
       "3      3.600\n",
       "4      3.870\n",
       "       ...  \n",
       "195    3.543\n",
       "196    3.450\n",
       "197    3.710\n",
       "198    3.570\n",
       "199    4.000\n",
       "Name: Overall GPA (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\"), Length: 200, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_a_df = raw_df[tar_col_a]\n",
    "tar_a_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d117c-974f-46f9-bf93-cb7f49c28f8c",
   "metadata": {},
   "source": [
    "## target variable for *normalized* **major** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96fcaa7b-268e-491f-be75-049b48c66e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3.429\n",
       "1      4.000\n",
       "2      3.500\n",
       "3      3.300\n",
       "4      4.000\n",
       "       ...  \n",
       "195    3.600\n",
       "196    3.510\n",
       "197    3.610\n",
       "198    3.330\n",
       "199    4.000\n",
       "Name: GPA for your Major (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\"), Length: 200, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_b_df = raw_df[tar_col_b]\n",
    "tar_b_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d822a-2e0d-42af-b2cd-6edb2aefb743",
   "metadata": {},
   "source": [
    "# Regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56a5d6-a639-4b14-b4cd-3e27e911792b",
   "metadata": {},
   "source": [
    "## Using *normalized* topics as inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d9f80-a51e-440f-ba46-273589dc6859",
   "metadata": {},
   "source": [
    "### predicting *normalized* GPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4344f82-94f1-441a-ad0e-b261493b99fe",
   "metadata": {},
   "source": [
    "#### predicting **overall** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "591fc302-6111-483a-abed-9ad7c1f55ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>overall_gpa</td>   <th>  R-squared:         </th> <td>   0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.9045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 15 Feb 2023</td> <th>  Prob (F-statistic):</th>  <td> 0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:50:18</td>     <th>  Log-Likelihood:    </th> <td> -240.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   630.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   125</td>      <th>  BIC:               </th> <td>   878.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    74</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    1.0429</td> <td>    0.501</td> <td>    2.081</td> <td> 0.039</td> <td>    0.051</td> <td>    2.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_0</th>  <td>   -0.7608</td> <td>    0.969</td> <td>   -0.785</td> <td> 0.434</td> <td>   -2.678</td> <td>    1.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_1</th>  <td>   -0.4640</td> <td>    1.492</td> <td>   -0.311</td> <td> 0.756</td> <td>   -3.418</td> <td>    2.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_2</th>  <td>   -0.0257</td> <td>    3.041</td> <td>   -0.008</td> <td> 0.993</td> <td>   -6.044</td> <td>    5.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_3</th>  <td>   -2.6835</td> <td>    2.648</td> <td>   -1.013</td> <td> 0.313</td> <td>   -7.924</td> <td>    2.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_4</th>  <td>   -5.3060</td> <td>    2.409</td> <td>   -2.203</td> <td> 0.029</td> <td>  -10.073</td> <td>   -0.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_5</th>  <td>   -1.0061</td> <td>    2.866</td> <td>   -0.351</td> <td> 0.726</td> <td>   -6.679</td> <td>    4.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_6</th>  <td>   -5.2514</td> <td>    3.485</td> <td>   -1.507</td> <td> 0.134</td> <td>  -12.148</td> <td>    1.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_7</th>  <td>   -1.0080</td> <td>    3.529</td> <td>   -0.286</td> <td> 0.776</td> <td>   -7.992</td> <td>    5.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_8</th>  <td>    3.1336</td> <td>    3.810</td> <td>    0.822</td> <td> 0.412</td> <td>   -4.408</td> <td>   10.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_9</th>  <td>    0.3469</td> <td>    3.156</td> <td>    0.110</td> <td> 0.913</td> <td>   -5.900</td> <td>    6.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_10</th> <td>   -1.3593</td> <td>    3.619</td> <td>   -0.376</td> <td> 0.708</td> <td>   -8.521</td> <td>    5.802</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_11</th> <td>    0.0163</td> <td>    3.833</td> <td>    0.004</td> <td> 0.997</td> <td>   -7.569</td> <td>    7.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_12</th> <td>   -3.7580</td> <td>    4.074</td> <td>   -0.922</td> <td> 0.358</td> <td>  -11.821</td> <td>    4.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_13</th> <td>   -7.4903</td> <td>    4.010</td> <td>   -1.868</td> <td> 0.064</td> <td>  -15.426</td> <td>    0.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_14</th> <td>   -7.1430</td> <td>    3.363</td> <td>   -2.124</td> <td> 0.036</td> <td>  -13.798</td> <td>   -0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_15</th> <td>   -2.0183</td> <td>    4.103</td> <td>   -0.492</td> <td> 0.624</td> <td>  -10.139</td> <td>    6.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_16</th> <td>    3.9962</td> <td>    4.464</td> <td>    0.895</td> <td> 0.372</td> <td>   -4.839</td> <td>   12.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_17</th> <td>    4.2057</td> <td>    4.624</td> <td>    0.910</td> <td> 0.365</td> <td>   -4.946</td> <td>   13.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_18</th> <td>   -3.3197</td> <td>    3.833</td> <td>   -0.866</td> <td> 0.388</td> <td>  -10.905</td> <td>    4.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_19</th> <td>   -1.0402</td> <td>    4.054</td> <td>   -0.257</td> <td> 0.798</td> <td>   -9.063</td> <td>    6.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_20</th> <td>   -4.7350</td> <td>    3.894</td> <td>   -1.216</td> <td> 0.226</td> <td>  -12.442</td> <td>    2.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_21</th> <td>  -12.1693</td> <td>    4.166</td> <td>   -2.921</td> <td> 0.004</td> <td>  -20.415</td> <td>   -3.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_22</th> <td>    2.9203</td> <td>    4.135</td> <td>    0.706</td> <td> 0.481</td> <td>   -5.264</td> <td>   11.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_23</th> <td>   -2.6888</td> <td>    3.968</td> <td>   -0.678</td> <td> 0.499</td> <td>  -10.541</td> <td>    5.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_24</th> <td>   -2.6760</td> <td>    3.939</td> <td>   -0.679</td> <td> 0.498</td> <td>  -10.472</td> <td>    5.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_25</th> <td>    0.0077</td> <td>    5.144</td> <td>    0.001</td> <td> 0.999</td> <td>  -10.173</td> <td>   10.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_26</th> <td>   -3.9819</td> <td>    4.381</td> <td>   -0.909</td> <td> 0.365</td> <td>  -12.653</td> <td>    4.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_27</th> <td>   -3.7599</td> <td>    4.585</td> <td>   -0.820</td> <td> 0.414</td> <td>  -12.834</td> <td>    5.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_28</th> <td>   -1.4967</td> <td>    5.442</td> <td>   -0.275</td> <td> 0.784</td> <td>  -12.268</td> <td>    9.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_29</th> <td>    3.2306</td> <td>    5.908</td> <td>    0.547</td> <td> 0.585</td> <td>   -8.462</td> <td>   14.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_30</th> <td>   -3.9621</td> <td>    4.671</td> <td>   -0.848</td> <td> 0.398</td> <td>  -13.206</td> <td>    5.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_31</th> <td>   -9.7103</td> <td>    5.323</td> <td>   -1.824</td> <td> 0.071</td> <td>  -20.245</td> <td>    0.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_32</th> <td>   -5.7001</td> <td>    5.003</td> <td>   -1.139</td> <td> 0.257</td> <td>  -15.602</td> <td>    4.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_33</th> <td>    1.8472</td> <td>    5.497</td> <td>    0.336</td> <td> 0.737</td> <td>   -9.031</td> <td>   12.726</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_34</th> <td>    3.1441</td> <td>    4.834</td> <td>    0.650</td> <td> 0.517</td> <td>   -6.423</td> <td>   12.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_35</th> <td>    5.8928</td> <td>    4.846</td> <td>    1.216</td> <td> 0.226</td> <td>   -3.698</td> <td>   15.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_36</th> <td>   -5.3132</td> <td>    5.393</td> <td>   -0.985</td> <td> 0.326</td> <td>  -15.988</td> <td>    5.361</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_37</th> <td>   -9.4434</td> <td>    5.506</td> <td>   -1.715</td> <td> 0.089</td> <td>  -20.340</td> <td>    1.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_38</th> <td>    1.3050</td> <td>    4.662</td> <td>    0.280</td> <td> 0.780</td> <td>   -7.922</td> <td>   10.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_39</th> <td>    9.3360</td> <td>    5.392</td> <td>    1.731</td> <td> 0.086</td> <td>   -1.336</td> <td>   20.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_40</th> <td>   -4.5054</td> <td>    4.927</td> <td>   -0.914</td> <td> 0.362</td> <td>  -14.257</td> <td>    5.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_41</th> <td>    4.6974</td> <td>    5.054</td> <td>    0.929</td> <td> 0.354</td> <td>   -5.305</td> <td>   14.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_42</th> <td>   -1.5698</td> <td>    5.310</td> <td>   -0.296</td> <td> 0.768</td> <td>  -12.079</td> <td>    8.940</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_43</th> <td>   -4.4856</td> <td>    4.890</td> <td>   -0.917</td> <td> 0.361</td> <td>  -14.163</td> <td>    5.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_44</th> <td>    2.1921</td> <td>    4.972</td> <td>    0.441</td> <td> 0.660</td> <td>   -7.648</td> <td>   12.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_45</th> <td>    2.0933</td> <td>    5.790</td> <td>    0.362</td> <td> 0.718</td> <td>   -9.366</td> <td>   13.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_46</th> <td>    1.9130</td> <td>    4.858</td> <td>    0.394</td> <td> 0.694</td> <td>   -7.702</td> <td>   11.528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_47</th> <td>   -5.9730</td> <td>    7.425</td> <td>   -0.804</td> <td> 0.423</td> <td>  -20.667</td> <td>    8.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_48</th> <td>    8.4383</td> <td>    5.804</td> <td>    1.454</td> <td> 0.148</td> <td>   -3.048</td> <td>   19.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_49</th> <td>  -10.2137</td> <td>    7.103</td> <td>   -1.438</td> <td> 0.153</td> <td>  -24.271</td> <td>    3.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_50</th> <td>    0.2042</td> <td>    6.763</td> <td>    0.030</td> <td> 0.976</td> <td>  -13.181</td> <td>   13.590</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_51</th> <td>    1.5421</td> <td>    5.697</td> <td>    0.271</td> <td> 0.787</td> <td>   -9.733</td> <td>   12.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_52</th> <td>    2.8979</td> <td>    4.873</td> <td>    0.595</td> <td> 0.553</td> <td>   -6.746</td> <td>   12.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_53</th> <td>    1.2181</td> <td>    5.242</td> <td>    0.232</td> <td> 0.817</td> <td>   -9.156</td> <td>   11.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_54</th> <td>    0.3341</td> <td>    5.919</td> <td>    0.056</td> <td> 0.955</td> <td>  -11.380</td> <td>   12.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_55</th> <td>    0.6359</td> <td>    6.251</td> <td>    0.102</td> <td> 0.919</td> <td>  -11.736</td> <td>   13.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_56</th> <td>   -2.0160</td> <td>    6.433</td> <td>   -0.313</td> <td> 0.755</td> <td>  -14.747</td> <td>   10.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_57</th> <td>   -6.4235</td> <td>    7.289</td> <td>   -0.881</td> <td> 0.380</td> <td>  -20.850</td> <td>    8.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_58</th> <td>   -6.3286</td> <td>    6.179</td> <td>   -1.024</td> <td> 0.308</td> <td>  -18.557</td> <td>    5.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_59</th> <td>  -10.9520</td> <td>    7.692</td> <td>   -1.424</td> <td> 0.157</td> <td>  -26.176</td> <td>    4.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_60</th> <td>    1.9633</td> <td>    6.435</td> <td>    0.305</td> <td> 0.761</td> <td>  -10.772</td> <td>   14.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_61</th> <td>    1.5972</td> <td>    6.668</td> <td>    0.240</td> <td> 0.811</td> <td>  -11.599</td> <td>   14.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_62</th> <td>   -6.5178</td> <td>    7.409</td> <td>   -0.880</td> <td> 0.381</td> <td>  -21.180</td> <td>    8.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_63</th> <td>    7.5382</td> <td>    8.126</td> <td>    0.928</td> <td> 0.355</td> <td>   -8.545</td> <td>   23.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_64</th> <td>   -5.0984</td> <td>    7.073</td> <td>   -0.721</td> <td> 0.472</td> <td>  -19.096</td> <td>    8.899</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_65</th> <td>   -1.5689</td> <td>    3.043</td> <td>   -0.516</td> <td> 0.607</td> <td>   -7.591</td> <td>    4.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_66</th> <td>    9.1502</td> <td>    6.804</td> <td>    1.345</td> <td> 0.181</td> <td>   -4.316</td> <td>   22.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_67</th> <td>   -3.7816</td> <td>    5.791</td> <td>   -0.653</td> <td> 0.515</td> <td>  -15.242</td> <td>    7.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_68</th> <td>   -1.5085</td> <td>    8.284</td> <td>   -0.182</td> <td> 0.856</td> <td>  -17.904</td> <td>   14.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_69</th> <td>   -2.6801</td> <td>    6.829</td> <td>   -0.392</td> <td> 0.695</td> <td>  -16.195</td> <td>   10.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_70</th> <td>    0.7187</td> <td>    8.918</td> <td>    0.081</td> <td> 0.936</td> <td>  -16.931</td> <td>   18.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_71</th> <td>   -4.4433</td> <td>    7.290</td> <td>   -0.609</td> <td> 0.543</td> <td>  -18.872</td> <td>    9.985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_72</th> <td>   -5.8506</td> <td>    6.464</td> <td>   -0.905</td> <td> 0.367</td> <td>  -18.643</td> <td>    6.941</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_73</th> <td>   -7.0851</td> <td>    7.660</td> <td>   -0.925</td> <td> 0.357</td> <td>  -22.245</td> <td>    8.074</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>142.102</td> <th>  Durbin-Watson:     </th> <td>   1.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1788.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.537</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.745</td>  <th>  Cond. No.          </th> <td>    156.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            overall_gpa   R-squared:                       0.349\n",
       "Model:                            OLS   Adj. R-squared:                 -0.037\n",
       "Method:                 Least Squares   F-statistic:                    0.9045\n",
       "Date:                Wed, 15 Feb 2023   Prob (F-statistic):              0.678\n",
       "Time:                        13:50:18   Log-Likelihood:                -240.40\n",
       "No. Observations:                 200   AIC:                             630.8\n",
       "Df Residuals:                     125   BIC:                             878.2\n",
       "Df Model:                          74                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.0429      0.501      2.081      0.039       0.051       2.035\n",
       "topic_0       -0.7608      0.969     -0.785      0.434      -2.678       1.157\n",
       "topic_1       -0.4640      1.492     -0.311      0.756      -3.418       2.490\n",
       "topic_2       -0.0257      3.041     -0.008      0.993      -6.044       5.993\n",
       "topic_3       -2.6835      2.648     -1.013      0.313      -7.924       2.557\n",
       "topic_4       -5.3060      2.409     -2.203      0.029     -10.073      -0.539\n",
       "topic_5       -1.0061      2.866     -0.351      0.726      -6.679       4.667\n",
       "topic_6       -5.2514      3.485     -1.507      0.134     -12.148       1.646\n",
       "topic_7       -1.0080      3.529     -0.286      0.776      -7.992       5.976\n",
       "topic_8        3.1336      3.810      0.822      0.412      -4.408      10.675\n",
       "topic_9        0.3469      3.156      0.110      0.913      -5.900       6.594\n",
       "topic_10      -1.3593      3.619     -0.376      0.708      -8.521       5.802\n",
       "topic_11       0.0163      3.833      0.004      0.997      -7.569       7.602\n",
       "topic_12      -3.7580      4.074     -0.922      0.358     -11.821       4.305\n",
       "topic_13      -7.4903      4.010     -1.868      0.064     -15.426       0.446\n",
       "topic_14      -7.1430      3.363     -2.124      0.036     -13.798      -0.488\n",
       "topic_15      -2.0183      4.103     -0.492      0.624     -10.139       6.102\n",
       "topic_16       3.9962      4.464      0.895      0.372      -4.839      12.832\n",
       "topic_17       4.2057      4.624      0.910      0.365      -4.946      13.357\n",
       "topic_18      -3.3197      3.833     -0.866      0.388     -10.905       4.265\n",
       "topic_19      -1.0402      4.054     -0.257      0.798      -9.063       6.983\n",
       "topic_20      -4.7350      3.894     -1.216      0.226     -12.442       2.972\n",
       "topic_21     -12.1693      4.166     -2.921      0.004     -20.415      -3.923\n",
       "topic_22       2.9203      4.135      0.706      0.481      -5.264      11.105\n",
       "topic_23      -2.6888      3.968     -0.678      0.499     -10.541       5.164\n",
       "topic_24      -2.6760      3.939     -0.679      0.498     -10.472       5.120\n",
       "topic_25       0.0077      5.144      0.001      0.999     -10.173      10.188\n",
       "topic_26      -3.9819      4.381     -0.909      0.365     -12.653       4.690\n",
       "topic_27      -3.7599      4.585     -0.820      0.414     -12.834       5.314\n",
       "topic_28      -1.4967      5.442     -0.275      0.784     -12.268       9.274\n",
       "topic_29       3.2306      5.908      0.547      0.585      -8.462      14.923\n",
       "topic_30      -3.9621      4.671     -0.848      0.398     -13.206       5.282\n",
       "topic_31      -9.7103      5.323     -1.824      0.071     -20.245       0.824\n",
       "topic_32      -5.7001      5.003     -1.139      0.257     -15.602       4.202\n",
       "topic_33       1.8472      5.497      0.336      0.737      -9.031      12.726\n",
       "topic_34       3.1441      4.834      0.650      0.517      -6.423      12.712\n",
       "topic_35       5.8928      4.846      1.216      0.226      -3.698      15.484\n",
       "topic_36      -5.3132      5.393     -0.985      0.326     -15.988       5.361\n",
       "topic_37      -9.4434      5.506     -1.715      0.089     -20.340       1.453\n",
       "topic_38       1.3050      4.662      0.280      0.780      -7.922      10.532\n",
       "topic_39       9.3360      5.392      1.731      0.086      -1.336      20.008\n",
       "topic_40      -4.5054      4.927     -0.914      0.362     -14.257       5.246\n",
       "topic_41       4.6974      5.054      0.929      0.354      -5.305      14.700\n",
       "topic_42      -1.5698      5.310     -0.296      0.768     -12.079       8.940\n",
       "topic_43      -4.4856      4.890     -0.917      0.361     -14.163       5.192\n",
       "topic_44       2.1921      4.972      0.441      0.660      -7.648      12.032\n",
       "topic_45       2.0933      5.790      0.362      0.718      -9.366      13.553\n",
       "topic_46       1.9130      4.858      0.394      0.694      -7.702      11.528\n",
       "topic_47      -5.9730      7.425     -0.804      0.423     -20.667       8.721\n",
       "topic_48       8.4383      5.804      1.454      0.148      -3.048      19.924\n",
       "topic_49     -10.2137      7.103     -1.438      0.153     -24.271       3.844\n",
       "topic_50       0.2042      6.763      0.030      0.976     -13.181      13.590\n",
       "topic_51       1.5421      5.697      0.271      0.787      -9.733      12.817\n",
       "topic_52       2.8979      4.873      0.595      0.553      -6.746      12.541\n",
       "topic_53       1.2181      5.242      0.232      0.817      -9.156      11.592\n",
       "topic_54       0.3341      5.919      0.056      0.955     -11.380      12.048\n",
       "topic_55       0.6359      6.251      0.102      0.919     -11.736      13.008\n",
       "topic_56      -2.0160      6.433     -0.313      0.755     -14.747      10.715\n",
       "topic_57      -6.4235      7.289     -0.881      0.380     -20.850       8.003\n",
       "topic_58      -6.3286      6.179     -1.024      0.308     -18.557       5.900\n",
       "topic_59     -10.9520      7.692     -1.424      0.157     -26.176       4.272\n",
       "topic_60       1.9633      6.435      0.305      0.761     -10.772      14.699\n",
       "topic_61       1.5972      6.668      0.240      0.811     -11.599      14.793\n",
       "topic_62      -6.5178      7.409     -0.880      0.381     -21.180       8.145\n",
       "topic_63       7.5382      8.126      0.928      0.355      -8.545      23.621\n",
       "topic_64      -5.0984      7.073     -0.721      0.472     -19.096       8.899\n",
       "topic_65      -1.5689      3.043     -0.516      0.607      -7.591       4.453\n",
       "topic_66       9.1502      6.804      1.345      0.181      -4.316      22.616\n",
       "topic_67      -3.7816      5.791     -0.653      0.515     -15.242       7.679\n",
       "topic_68      -1.5085      8.284     -0.182      0.856     -17.904      14.887\n",
       "topic_69      -2.6801      6.829     -0.392      0.695     -16.195      10.834\n",
       "topic_70       0.7187      8.918      0.081      0.936     -16.931      18.368\n",
       "topic_71      -4.4433      7.290     -0.609      0.543     -18.872       9.985\n",
       "topic_72      -5.8506      6.464     -0.905      0.367     -18.643       6.941\n",
       "topic_73      -7.0851      7.660     -0.925      0.357     -22.245       8.074\n",
       "==============================================================================\n",
       "Omnibus:                      142.102   Durbin-Watson:                   1.907\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1788.859\n",
       "Skew:                          -2.537   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.745   Cond. No.                         156.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(topics_df.astype(float))\n",
    "old_reg = sm.OLS(\n",
    "    tar_a_norm_df.astype(float),\n",
    "    X,\n",
    "# ).fit(maxiter=1000, method='bfgs') # bfgs nm\n",
    ").fit(maxiter=1000) # bfgs nm\n",
    "old_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c195d3f2-1eef-4f1c-bdfb-ecf22f6fad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with statistical significance with alpha of 0.1 (p-values less than this):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const        1.042852\n",
       "topic_4     -5.305997\n",
       "topic_13    -7.490329\n",
       "topic_14    -7.143012\n",
       "topic_21   -12.169260\n",
       "topic_31    -9.710299\n",
       "topic_37    -9.443399\n",
       "topic_39     9.335962\n",
       "dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"variables with statistical significance with alpha of {p_alpha} (p-values less than this):\")\n",
    "significant_var_coeffs = old_reg.params[old_reg.pvalues < p_alpha]\n",
    "significant_var_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd9fde16-3d23-4690-b8a6-76e1c4aa6184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with negative effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_4     -5.305997\n",
       "topic_13    -7.490329\n",
       "topic_14    -7.143012\n",
       "topic_21   -12.169260\n",
       "topic_31    -9.710299\n",
       "topic_37    -9.443399\n",
       "dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with negative effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs < 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5c5d72a0-dda5-4ef8-8105-37cd90b20ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with positive effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const       1.042852\n",
       "topic_39    9.335962\n",
       "dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with positive effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f6d26-ffd6-465f-a6fd-6490d20788e4",
   "metadata": {},
   "source": [
    "#### predicting **major** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6bf30af6-fd8f-45ba-9c83-5f5e3f2c0711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>major_gpa</td>    <th>  R-squared:         </th> <td>   0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.8460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 15 Feb 2023</td> <th>  Prob (F-statistic):</th>  <td> 0.782</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:50:18</td>     <th>  Log-Likelihood:    </th> <td> -242.68</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   635.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   125</td>      <th>  BIC:               </th> <td>   882.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    74</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    0.9140</td> <td>    0.507</td> <td>    1.803</td> <td> 0.074</td> <td>   -0.089</td> <td>    1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_0</th>  <td>   -0.5444</td> <td>    0.980</td> <td>   -0.556</td> <td> 0.579</td> <td>   -2.484</td> <td>    1.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_1</th>  <td>   -1.3941</td> <td>    1.510</td> <td>   -0.924</td> <td> 0.358</td> <td>   -4.382</td> <td>    1.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_2</th>  <td>   -4.5188</td> <td>    3.076</td> <td>   -1.469</td> <td> 0.144</td> <td>  -10.607</td> <td>    1.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_3</th>  <td>    0.7379</td> <td>    2.678</td> <td>    0.276</td> <td> 0.783</td> <td>   -4.563</td> <td>    6.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_4</th>  <td>   -3.2212</td> <td>    2.436</td> <td>   -1.322</td> <td> 0.189</td> <td>   -8.043</td> <td>    1.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_5</th>  <td>   -3.6655</td> <td>    2.899</td> <td>   -1.264</td> <td> 0.208</td> <td>   -9.404</td> <td>    2.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_6</th>  <td>   -6.8908</td> <td>    3.525</td> <td>   -1.955</td> <td> 0.053</td> <td>  -13.867</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_7</th>  <td>   -1.7178</td> <td>    3.569</td> <td>   -0.481</td> <td> 0.631</td> <td>   -8.782</td> <td>    5.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_8</th>  <td>    5.3370</td> <td>    3.854</td> <td>    1.385</td> <td> 0.169</td> <td>   -2.291</td> <td>   12.965</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_9</th>  <td>   -4.9151</td> <td>    3.193</td> <td>   -1.539</td> <td> 0.126</td> <td>  -11.234</td> <td>    1.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_10</th> <td>   -4.9114</td> <td>    3.660</td> <td>   -1.342</td> <td> 0.182</td> <td>  -12.155</td> <td>    2.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_11</th> <td>   -4.6729</td> <td>    3.877</td> <td>   -1.205</td> <td> 0.230</td> <td>  -12.345</td> <td>    2.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_12</th> <td>    1.9241</td> <td>    4.121</td> <td>    0.467</td> <td> 0.641</td> <td>   -6.232</td> <td>   10.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_13</th> <td>   -0.7259</td> <td>    4.056</td> <td>   -0.179</td> <td> 0.858</td> <td>   -8.753</td> <td>    7.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_14</th> <td>   -9.4868</td> <td>    3.401</td> <td>   -2.789</td> <td> 0.006</td> <td>  -16.218</td> <td>   -2.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_15</th> <td>    3.0730</td> <td>    4.150</td> <td>    0.740</td> <td> 0.460</td> <td>   -5.141</td> <td>   11.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_16</th> <td>    5.9313</td> <td>    4.516</td> <td>    1.313</td> <td> 0.191</td> <td>   -3.006</td> <td>   14.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_17</th> <td>    6.1537</td> <td>    4.677</td> <td>    1.316</td> <td> 0.191</td> <td>   -3.103</td> <td>   15.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_18</th> <td>   -3.5923</td> <td>    3.877</td> <td>   -0.927</td> <td> 0.356</td> <td>  -11.264</td> <td>    4.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_19</th> <td>   -0.6772</td> <td>    4.100</td> <td>   -0.165</td> <td> 0.869</td> <td>   -8.792</td> <td>    7.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_20</th> <td>   -4.7353</td> <td>    3.939</td> <td>   -1.202</td> <td> 0.232</td> <td>  -12.531</td> <td>    3.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_21</th> <td>  -12.2042</td> <td>    4.214</td> <td>   -2.896</td> <td> 0.004</td> <td>  -20.545</td> <td>   -3.864</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_22</th> <td>    5.8033</td> <td>    4.183</td> <td>    1.387</td> <td> 0.168</td> <td>   -2.475</td> <td>   14.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_23</th> <td>    3.3267</td> <td>    4.013</td> <td>    0.829</td> <td> 0.409</td> <td>   -4.616</td> <td>   11.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_24</th> <td>   -2.0827</td> <td>    3.984</td> <td>   -0.523</td> <td> 0.602</td> <td>   -9.968</td> <td>    5.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_25</th> <td>   -3.5239</td> <td>    5.203</td> <td>   -0.677</td> <td> 0.499</td> <td>  -13.821</td> <td>    6.773</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_26</th> <td>   -2.4764</td> <td>    4.432</td> <td>   -0.559</td> <td> 0.577</td> <td>  -11.247</td> <td>    6.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_27</th> <td>   -5.6887</td> <td>    4.637</td> <td>   -1.227</td> <td> 0.222</td> <td>  -14.867</td> <td>    3.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_28</th> <td>   -8.2632</td> <td>    5.505</td> <td>   -1.501</td> <td> 0.136</td> <td>  -19.158</td> <td>    2.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_29</th> <td>    3.7047</td> <td>    5.976</td> <td>    0.620</td> <td> 0.536</td> <td>   -8.122</td> <td>   15.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_30</th> <td>   -3.9361</td> <td>    4.724</td> <td>   -0.833</td> <td> 0.406</td> <td>  -13.286</td> <td>    5.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_31</th> <td>  -13.2311</td> <td>    5.384</td> <td>   -2.458</td> <td> 0.015</td> <td>  -23.887</td> <td>   -2.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_32</th> <td>    0.1544</td> <td>    5.061</td> <td>    0.031</td> <td> 0.976</td> <td>   -9.861</td> <td>   10.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_33</th> <td>   -3.3032</td> <td>    5.560</td> <td>   -0.594</td> <td> 0.554</td> <td>  -14.307</td> <td>    7.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_34</th> <td>    2.6471</td> <td>    4.890</td> <td>    0.541</td> <td> 0.589</td> <td>   -7.030</td> <td>   12.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_35</th> <td>    5.2065</td> <td>    4.902</td> <td>    1.062</td> <td> 0.290</td> <td>   -4.494</td> <td>   14.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_36</th> <td>    3.7731</td> <td>    5.455</td> <td>    0.692</td> <td> 0.490</td> <td>   -7.024</td> <td>   14.570</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_37</th> <td>   -3.7637</td> <td>    5.569</td> <td>   -0.676</td> <td> 0.500</td> <td>  -14.786</td> <td>    7.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_38</th> <td>    7.5063</td> <td>    4.716</td> <td>    1.592</td> <td> 0.114</td> <td>   -1.827</td> <td>   16.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_39</th> <td>    1.5412</td> <td>    5.454</td> <td>    0.283</td> <td> 0.778</td> <td>   -9.253</td> <td>   12.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_40</th> <td>   -1.0711</td> <td>    4.984</td> <td>   -0.215</td> <td> 0.830</td> <td>  -10.935</td> <td>    8.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_41</th> <td>    4.1955</td> <td>    5.112</td> <td>    0.821</td> <td> 0.413</td> <td>   -5.922</td> <td>   14.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_42</th> <td>    2.6361</td> <td>    5.371</td> <td>    0.491</td> <td> 0.624</td> <td>   -7.994</td> <td>   13.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_43</th> <td>   -5.0507</td> <td>    4.946</td> <td>   -1.021</td> <td> 0.309</td> <td>  -14.839</td> <td>    4.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_44</th> <td>    0.6292</td> <td>    5.029</td> <td>    0.125</td> <td> 0.901</td> <td>   -9.324</td> <td>   10.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_45</th> <td>    0.8326</td> <td>    5.857</td> <td>    0.142</td> <td> 0.887</td> <td>  -10.758</td> <td>   12.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_46</th> <td>    0.2574</td> <td>    4.914</td> <td>    0.052</td> <td> 0.958</td> <td>   -9.468</td> <td>    9.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_47</th> <td>  -11.3463</td> <td>    7.510</td> <td>   -1.511</td> <td> 0.133</td> <td>  -26.209</td> <td>    3.517</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_48</th> <td>    5.3113</td> <td>    5.870</td> <td>    0.905</td> <td> 0.367</td> <td>   -6.306</td> <td>   16.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_49</th> <td>   -7.9024</td> <td>    7.184</td> <td>   -1.100</td> <td> 0.273</td> <td>  -22.121</td> <td>    6.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_50</th> <td>    3.4947</td> <td>    6.841</td> <td>    0.511</td> <td> 0.610</td> <td>  -10.044</td> <td>   17.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_51</th> <td>    2.4274</td> <td>    5.762</td> <td>    0.421</td> <td> 0.674</td> <td>   -8.977</td> <td>   13.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_52</th> <td>    3.9512</td> <td>    4.928</td> <td>    0.802</td> <td> 0.424</td> <td>   -5.803</td> <td>   13.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_53</th> <td>    0.2389</td> <td>    5.302</td> <td>    0.045</td> <td> 0.964</td> <td>  -10.254</td> <td>   10.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_54</th> <td>    2.4017</td> <td>    5.987</td> <td>    0.401</td> <td> 0.689</td> <td>   -9.447</td> <td>   14.250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_55</th> <td>   -0.2979</td> <td>    6.323</td> <td>   -0.047</td> <td> 0.962</td> <td>  -12.812</td> <td>   12.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_56</th> <td>   -3.2669</td> <td>    6.507</td> <td>   -0.502</td> <td> 0.616</td> <td>  -16.144</td> <td>    9.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_57</th> <td>   -0.7852</td> <td>    7.373</td> <td>   -0.106</td> <td> 0.915</td> <td>  -15.377</td> <td>   13.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_58</th> <td>   -0.2814</td> <td>    6.250</td> <td>   -0.045</td> <td> 0.964</td> <td>  -12.650</td> <td>   12.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_59</th> <td>   -1.5079</td> <td>    7.780</td> <td>   -0.194</td> <td> 0.847</td> <td>  -16.906</td> <td>   13.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_60</th> <td>    3.3123</td> <td>    6.509</td> <td>    0.509</td> <td> 0.612</td> <td>   -9.569</td> <td>   16.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_61</th> <td>    6.2302</td> <td>    6.744</td> <td>    0.924</td> <td> 0.357</td> <td>   -7.117</td> <td>   19.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_62</th> <td>  -15.1303</td> <td>    7.494</td> <td>   -2.019</td> <td> 0.046</td> <td>  -29.961</td> <td>   -0.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_63</th> <td>    8.7159</td> <td>    8.220</td> <td>    1.060</td> <td> 0.291</td> <td>   -7.552</td> <td>   24.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_64</th> <td>   -3.9370</td> <td>    7.154</td> <td>   -0.550</td> <td> 0.583</td> <td>  -18.095</td> <td>   10.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_65</th> <td>   -1.0916</td> <td>    3.078</td> <td>   -0.355</td> <td> 0.723</td> <td>   -7.182</td> <td>    4.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_66</th> <td>   10.6375</td> <td>    6.882</td> <td>    1.546</td> <td> 0.125</td> <td>   -2.983</td> <td>   24.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_67</th> <td>   -1.0061</td> <td>    5.857</td> <td>   -0.172</td> <td> 0.864</td> <td>  -12.598</td> <td>   10.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_68</th> <td>   -2.9802</td> <td>    8.379</td> <td>   -0.356</td> <td> 0.723</td> <td>  -19.564</td> <td>   13.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_69</th> <td>    1.0526</td> <td>    6.907</td> <td>    0.152</td> <td> 0.879</td> <td>  -12.617</td> <td>   14.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_70</th> <td>   -5.8267</td> <td>    9.020</td> <td>   -0.646</td> <td> 0.519</td> <td>  -23.679</td> <td>   12.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_71</th> <td>   -5.7003</td> <td>    7.374</td> <td>   -0.773</td> <td> 0.441</td> <td>  -20.294</td> <td>    8.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_72</th> <td>  -10.3546</td> <td>    6.538</td> <td>   -1.584</td> <td> 0.116</td> <td>  -23.293</td> <td>    2.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_73</th> <td>   -8.3410</td> <td>    7.748</td> <td>   -1.077</td> <td> 0.284</td> <td>  -23.675</td> <td>    6.992</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>131.147</td> <th>  Durbin-Watson:     </th> <td>   1.896</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1296.854</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.368</td>  <th>  Prob(JB):          </th> <td>2.46e-282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.541</td>  <th>  Cond. No.          </th> <td>    156.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              major_gpa   R-squared:                       0.334\n",
       "Model:                            OLS   Adj. R-squared:                 -0.061\n",
       "Method:                 Least Squares   F-statistic:                    0.8460\n",
       "Date:                Wed, 15 Feb 2023   Prob (F-statistic):              0.782\n",
       "Time:                        13:50:18   Log-Likelihood:                -242.68\n",
       "No. Observations:                 200   AIC:                             635.4\n",
       "Df Residuals:                     125   BIC:                             882.7\n",
       "Df Model:                          74                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.9140      0.507      1.803      0.074      -0.089       1.917\n",
       "topic_0       -0.5444      0.980     -0.556      0.579      -2.484       1.395\n",
       "topic_1       -1.3941      1.510     -0.924      0.358      -4.382       1.593\n",
       "topic_2       -4.5188      3.076     -1.469      0.144     -10.607       1.569\n",
       "topic_3        0.7379      2.678      0.276      0.783      -4.563       6.038\n",
       "topic_4       -3.2212      2.436     -1.322      0.189      -8.043       1.600\n",
       "topic_5       -3.6655      2.899     -1.264      0.208      -9.404       2.073\n",
       "topic_6       -6.8908      3.525     -1.955      0.053     -13.867       0.085\n",
       "topic_7       -1.7178      3.569     -0.481      0.631      -8.782       5.346\n",
       "topic_8        5.3370      3.854      1.385      0.169      -2.291      12.965\n",
       "topic_9       -4.9151      3.193     -1.539      0.126     -11.234       1.404\n",
       "topic_10      -4.9114      3.660     -1.342      0.182     -12.155       2.332\n",
       "topic_11      -4.6729      3.877     -1.205      0.230     -12.345       2.999\n",
       "topic_12       1.9241      4.121      0.467      0.641      -6.232      10.080\n",
       "topic_13      -0.7259      4.056     -0.179      0.858      -8.753       7.301\n",
       "topic_14      -9.4868      3.401     -2.789      0.006     -16.218      -2.755\n",
       "topic_15       3.0730      4.150      0.740      0.460      -5.141      11.287\n",
       "topic_16       5.9313      4.516      1.313      0.191      -3.006      14.868\n",
       "topic_17       6.1537      4.677      1.316      0.191      -3.103      15.410\n",
       "topic_18      -3.5923      3.877     -0.927      0.356     -11.264       4.080\n",
       "topic_19      -0.6772      4.100     -0.165      0.869      -8.792       7.438\n",
       "topic_20      -4.7353      3.939     -1.202      0.232     -12.531       3.060\n",
       "topic_21     -12.2042      4.214     -2.896      0.004     -20.545      -3.864\n",
       "topic_22       5.8033      4.183      1.387      0.168      -2.475      14.081\n",
       "topic_23       3.3267      4.013      0.829      0.409      -4.616      11.269\n",
       "topic_24      -2.0827      3.984     -0.523      0.602      -9.968       5.803\n",
       "topic_25      -3.5239      5.203     -0.677      0.499     -13.821       6.773\n",
       "topic_26      -2.4764      4.432     -0.559      0.577     -11.247       6.295\n",
       "topic_27      -5.6887      4.637     -1.227      0.222     -14.867       3.489\n",
       "topic_28      -8.2632      5.505     -1.501      0.136     -19.158       2.631\n",
       "topic_29       3.7047      5.976      0.620      0.536      -8.122      15.531\n",
       "topic_30      -3.9361      4.724     -0.833      0.406     -13.286       5.414\n",
       "topic_31     -13.2311      5.384     -2.458      0.015     -23.887      -2.576\n",
       "topic_32       0.1544      5.061      0.031      0.976      -9.861      10.170\n",
       "topic_33      -3.3032      5.560     -0.594      0.554     -14.307       7.700\n",
       "topic_34       2.6471      4.890      0.541      0.589      -7.030      12.324\n",
       "topic_35       5.2065      4.902      1.062      0.290      -4.494      14.907\n",
       "topic_36       3.7731      5.455      0.692      0.490      -7.024      14.570\n",
       "topic_37      -3.7637      5.569     -0.676      0.500     -14.786       7.258\n",
       "topic_38       7.5063      4.716      1.592      0.114      -1.827      16.839\n",
       "topic_39       1.5412      5.454      0.283      0.778      -9.253      12.335\n",
       "topic_40      -1.0711      4.984     -0.215      0.830     -10.935       8.792\n",
       "topic_41       4.1955      5.112      0.821      0.413      -5.922      14.313\n",
       "topic_42       2.6361      5.371      0.491      0.624      -7.994      13.266\n",
       "topic_43      -5.0507      4.946     -1.021      0.309     -14.839       4.737\n",
       "topic_44       0.6292      5.029      0.125      0.901      -9.324      10.582\n",
       "topic_45       0.8326      5.857      0.142      0.887     -10.758      12.424\n",
       "topic_46       0.2574      4.914      0.052      0.958      -9.468       9.983\n",
       "topic_47     -11.3463      7.510     -1.511      0.133     -26.209       3.517\n",
       "topic_48       5.3113      5.870      0.905      0.367      -6.306      16.929\n",
       "topic_49      -7.9024      7.184     -1.100      0.273     -22.121       6.316\n",
       "topic_50       3.4947      6.841      0.511      0.610     -10.044      17.034\n",
       "topic_51       2.4274      5.762      0.421      0.674      -8.977      13.832\n",
       "topic_52       3.9512      4.928      0.802      0.424      -5.803      13.705\n",
       "topic_53       0.2389      5.302      0.045      0.964     -10.254      10.732\n",
       "topic_54       2.4017      5.987      0.401      0.689      -9.447      14.250\n",
       "topic_55      -0.2979      6.323     -0.047      0.962     -12.812      12.216\n",
       "topic_56      -3.2669      6.507     -0.502      0.616     -16.144       9.611\n",
       "topic_57      -0.7852      7.373     -0.106      0.915     -15.377      13.807\n",
       "topic_58      -0.2814      6.250     -0.045      0.964     -12.650      12.087\n",
       "topic_59      -1.5079      7.780     -0.194      0.847     -16.906      13.890\n",
       "topic_60       3.3123      6.509      0.509      0.612      -9.569      16.194\n",
       "topic_61       6.2302      6.744      0.924      0.357      -7.117      19.578\n",
       "topic_62     -15.1303      7.494     -2.019      0.046     -29.961      -0.299\n",
       "topic_63       8.7159      8.220      1.060      0.291      -7.552      24.984\n",
       "topic_64      -3.9370      7.154     -0.550      0.583     -18.095      10.221\n",
       "topic_65      -1.0916      3.078     -0.355      0.723      -7.182       4.999\n",
       "topic_66      10.6375      6.882      1.546      0.125      -2.983      24.258\n",
       "topic_67      -1.0061      5.857     -0.172      0.864     -12.598      10.586\n",
       "topic_68      -2.9802      8.379     -0.356      0.723     -19.564      13.604\n",
       "topic_69       1.0526      6.907      0.152      0.879     -12.617      14.722\n",
       "topic_70      -5.8267      9.020     -0.646      0.519     -23.679      12.026\n",
       "topic_71      -5.7003      7.374     -0.773      0.441     -20.294       8.894\n",
       "topic_72     -10.3546      6.538     -1.584      0.116     -23.293       2.584\n",
       "topic_73      -8.3410      7.748     -1.077      0.284     -23.675       6.992\n",
       "==============================================================================\n",
       "Omnibus:                      131.147   Durbin-Watson:                   1.896\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1296.854\n",
       "Skew:                          -2.368   Prob(JB):                    2.46e-282\n",
       "Kurtosis:                      14.541   Cond. No.                         156.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(topics_df.astype(float))\n",
    "old_reg = sm.OLS(\n",
    "    tar_b_norm_df.astype(float),\n",
    "    X,\n",
    "# ).fit(maxiter=1000, method='bfgs') # bfgs nm\n",
    ").fit(maxiter=1000) # bfgs nm\n",
    "old_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c3f289e-f747-4e35-955d-15399977de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with statistical significance with alpha of 0.1 (p-values less than this):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const        0.914002\n",
       "topic_6     -6.890775\n",
       "topic_14    -9.486783\n",
       "topic_21   -12.204194\n",
       "topic_31   -13.231113\n",
       "topic_62   -15.130315\n",
       "dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"variables with statistical significance with alpha of {p_alpha} (p-values less than this):\")\n",
    "significant_var_coeffs = old_reg.params[old_reg.pvalues < p_alpha]\n",
    "significant_var_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef1625a0-3910-4792-bf3a-ec4c9f451cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with negative effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const        0.914002\n",
       "topic_6     -6.890775\n",
       "topic_14    -9.486783\n",
       "topic_21   -12.204194\n",
       "topic_31   -13.231113\n",
       "topic_62   -15.130315\n",
       "dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with negative effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs < 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e448308-0a14-4d3a-a22f-16cf0d4900be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with positive effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with positive effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7176f1-ef4b-4e63-a7b8-a0be934cf1b9",
   "metadata": {},
   "source": [
    "### predicting *raw* GPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b1e5f-fb9c-4225-8eb1-f4ec6b005613",
   "metadata": {},
   "source": [
    "#### predicting **overall** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6406ad77-5559-45af-b614-2f3061e3b127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>Overall GPA (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")</td> <th>  R-squared:         </th> <td>   0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                                                      <td>OLS</td>                                           <th>  Adj. R-squared:    </th> <td>  -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                                                <td>Least Squares</td>                                      <th>  F-statistic:       </th> <td>  0.9045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                                                <td>Wed, 15 Feb 2023</td>                                     <th>  Prob (F-statistic):</th>  <td> 0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                                                    <td>13:50:19</td>                                         <th>  Log-Likelihood:    </th> <td> -94.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>                                         <td>   200</td>                                          <th>  AIC:               </th> <td>   338.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>                                             <td>   125</td>                                          <th>  BIC:               </th> <td>   585.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                                                 <td>    74</td>                                          <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>                                         <td>nonrobust</td>                                        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    4.0690</td> <td>    0.241</td> <td>   16.869</td> <td> 0.000</td> <td>    3.592</td> <td>    4.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_0</th>  <td>   -0.3662</td> <td>    0.466</td> <td>   -0.785</td> <td> 0.434</td> <td>   -1.289</td> <td>    0.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_1</th>  <td>   -0.2234</td> <td>    0.718</td> <td>   -0.311</td> <td> 0.756</td> <td>   -1.645</td> <td>    1.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_2</th>  <td>   -0.0124</td> <td>    1.464</td> <td>   -0.008</td> <td> 0.993</td> <td>   -2.910</td> <td>    2.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_3</th>  <td>   -1.2918</td> <td>    1.275</td> <td>   -1.013</td> <td> 0.313</td> <td>   -3.814</td> <td>    1.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_4</th>  <td>   -2.5541</td> <td>    1.159</td> <td>   -2.203</td> <td> 0.029</td> <td>   -4.849</td> <td>   -0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_5</th>  <td>   -0.4843</td> <td>    1.380</td> <td>   -0.351</td> <td> 0.726</td> <td>   -3.215</td> <td>    2.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_6</th>  <td>   -2.5279</td> <td>    1.678</td> <td>   -1.507</td> <td> 0.134</td> <td>   -5.848</td> <td>    0.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_7</th>  <td>   -0.4852</td> <td>    1.699</td> <td>   -0.286</td> <td> 0.776</td> <td>   -3.847</td> <td>    2.877</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_8</th>  <td>    1.5084</td> <td>    1.834</td> <td>    0.822</td> <td> 0.412</td> <td>   -2.122</td> <td>    5.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_9</th>  <td>    0.1670</td> <td>    1.519</td> <td>    0.110</td> <td> 0.913</td> <td>   -2.840</td> <td>    3.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_10</th> <td>   -0.6543</td> <td>    1.742</td> <td>   -0.376</td> <td> 0.708</td> <td>   -4.102</td> <td>    2.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_11</th> <td>    0.0078</td> <td>    1.845</td> <td>    0.004</td> <td> 0.997</td> <td>   -3.643</td> <td>    3.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_12</th> <td>   -1.8090</td> <td>    1.961</td> <td>   -0.922</td> <td> 0.358</td> <td>   -5.690</td> <td>    2.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_13</th> <td>   -3.6056</td> <td>    1.930</td> <td>   -1.868</td> <td> 0.064</td> <td>   -7.426</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_14</th> <td>   -3.4384</td> <td>    1.619</td> <td>   -2.124</td> <td> 0.036</td> <td>   -6.642</td> <td>   -0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_15</th> <td>   -0.9715</td> <td>    1.975</td> <td>   -0.492</td> <td> 0.624</td> <td>   -4.881</td> <td>    2.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_16</th> <td>    1.9236</td> <td>    2.149</td> <td>    0.895</td> <td> 0.372</td> <td>   -2.330</td> <td>    6.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_17</th> <td>    2.0245</td> <td>    2.226</td> <td>    0.910</td> <td> 0.365</td> <td>   -2.381</td> <td>    6.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_18</th> <td>   -1.5980</td> <td>    1.845</td> <td>   -0.866</td> <td> 0.388</td> <td>   -5.249</td> <td>    2.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_19</th> <td>   -0.5007</td> <td>    1.951</td> <td>   -0.257</td> <td> 0.798</td> <td>   -4.363</td> <td>    3.361</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_20</th> <td>   -2.2793</td> <td>    1.875</td> <td>   -1.216</td> <td> 0.226</td> <td>   -5.989</td> <td>    1.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_21</th> <td>   -5.8579</td> <td>    2.006</td> <td>   -2.921</td> <td> 0.004</td> <td>   -9.827</td> <td>   -1.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_22</th> <td>    1.4057</td> <td>    1.991</td> <td>    0.706</td> <td> 0.481</td> <td>   -2.534</td> <td>    5.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_23</th> <td>   -1.2943</td> <td>    1.910</td> <td>   -0.678</td> <td> 0.499</td> <td>   -5.074</td> <td>    2.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_24</th> <td>   -1.2881</td> <td>    1.896</td> <td>   -0.679</td> <td> 0.498</td> <td>   -5.041</td> <td>    2.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_25</th> <td>    0.0037</td> <td>    2.476</td> <td>    0.001</td> <td> 0.999</td> <td>   -4.897</td> <td>    4.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_26</th> <td>   -1.9167</td> <td>    2.109</td> <td>   -0.909</td> <td> 0.365</td> <td>   -6.091</td> <td>    2.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_27</th> <td>   -1.8099</td> <td>    2.207</td> <td>   -0.820</td> <td> 0.414</td> <td>   -6.178</td> <td>    2.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_28</th> <td>   -0.7205</td> <td>    2.620</td> <td>   -0.275</td> <td> 0.784</td> <td>   -5.905</td> <td>    4.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_29</th> <td>    1.5551</td> <td>    2.844</td> <td>    0.547</td> <td> 0.585</td> <td>   -4.073</td> <td>    7.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_30</th> <td>   -1.9072</td> <td>    2.248</td> <td>   -0.848</td> <td> 0.398</td> <td>   -6.357</td> <td>    2.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_31</th> <td>   -4.6742</td> <td>    2.562</td> <td>   -1.824</td> <td> 0.071</td> <td>   -9.745</td> <td>    0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_32</th> <td>   -2.7438</td> <td>    2.408</td> <td>   -1.139</td> <td> 0.257</td> <td>   -7.510</td> <td>    2.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_33</th> <td>    0.8892</td> <td>    2.646</td> <td>    0.336</td> <td> 0.737</td> <td>   -4.347</td> <td>    6.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_34</th> <td>    1.5135</td> <td>    2.327</td> <td>    0.650</td> <td> 0.517</td> <td>   -3.092</td> <td>    6.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_35</th> <td>    2.8366</td> <td>    2.333</td> <td>    1.216</td> <td> 0.226</td> <td>   -1.780</td> <td>    7.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_36</th> <td>   -2.5576</td> <td>    2.596</td> <td>   -0.985</td> <td> 0.326</td> <td>   -7.696</td> <td>    2.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_37</th> <td>   -4.5457</td> <td>    2.650</td> <td>   -1.715</td> <td> 0.089</td> <td>   -9.791</td> <td>    0.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_38</th> <td>    0.6282</td> <td>    2.244</td> <td>    0.280</td> <td> 0.780</td> <td>   -3.813</td> <td>    5.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_39</th> <td>    4.4940</td> <td>    2.596</td> <td>    1.731</td> <td> 0.086</td> <td>   -0.643</td> <td>    9.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_40</th> <td>   -2.1688</td> <td>    2.372</td> <td>   -0.914</td> <td> 0.362</td> <td>   -6.863</td> <td>    2.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_41</th> <td>    2.2612</td> <td>    2.433</td> <td>    0.929</td> <td> 0.354</td> <td>   -2.554</td> <td>    7.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_42</th> <td>   -0.7557</td> <td>    2.556</td> <td>   -0.296</td> <td> 0.768</td> <td>   -5.815</td> <td>    4.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_43</th> <td>   -2.1592</td> <td>    2.354</td> <td>   -0.917</td> <td> 0.361</td> <td>   -6.817</td> <td>    2.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_44</th> <td>    1.0552</td> <td>    2.393</td> <td>    0.441</td> <td> 0.660</td> <td>   -3.681</td> <td>    5.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_45</th> <td>    1.0077</td> <td>    2.787</td> <td>    0.362</td> <td> 0.718</td> <td>   -4.509</td> <td>    6.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_46</th> <td>    0.9208</td> <td>    2.339</td> <td>    0.394</td> <td> 0.694</td> <td>   -3.708</td> <td>    5.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_47</th> <td>   -2.8752</td> <td>    3.574</td> <td>   -0.804</td> <td> 0.423</td> <td>   -9.949</td> <td>    4.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_48</th> <td>    4.0619</td> <td>    2.794</td> <td>    1.454</td> <td> 0.148</td> <td>   -1.467</td> <td>    9.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_49</th> <td>   -4.9166</td> <td>    3.419</td> <td>   -1.438</td> <td> 0.153</td> <td>  -11.683</td> <td>    1.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_50</th> <td>    0.0983</td> <td>    3.256</td> <td>    0.030</td> <td> 0.976</td> <td>   -6.345</td> <td>    6.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_51</th> <td>    0.7423</td> <td>    2.742</td> <td>    0.271</td> <td> 0.787</td> <td>   -4.685</td> <td>    6.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_52</th> <td>    1.3950</td> <td>    2.346</td> <td>    0.595</td> <td> 0.553</td> <td>   -3.247</td> <td>    6.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_53</th> <td>    0.5864</td> <td>    2.523</td> <td>    0.232</td> <td> 0.817</td> <td>   -4.407</td> <td>    5.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_54</th> <td>    0.1608</td> <td>    2.849</td> <td>    0.056</td> <td> 0.955</td> <td>   -5.478</td> <td>    5.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_55</th> <td>    0.3061</td> <td>    3.009</td> <td>    0.102</td> <td> 0.919</td> <td>   -5.649</td> <td>    6.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_56</th> <td>   -0.9704</td> <td>    3.097</td> <td>   -0.313</td> <td> 0.755</td> <td>   -7.099</td> <td>    5.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_57</th> <td>   -3.0921</td> <td>    3.509</td> <td>   -0.881</td> <td> 0.380</td> <td>  -10.037</td> <td>    3.852</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_58</th> <td>   -3.0464</td> <td>    2.974</td> <td>   -1.024</td> <td> 0.308</td> <td>   -8.933</td> <td>    2.840</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_59</th> <td>   -5.2719</td> <td>    3.703</td> <td>   -1.424</td> <td> 0.157</td> <td>  -12.600</td> <td>    2.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_60</th> <td>    0.9450</td> <td>    3.098</td> <td>    0.305</td> <td> 0.761</td> <td>   -5.185</td> <td>    7.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_61</th> <td>    0.7688</td> <td>    3.210</td> <td>    0.240</td> <td> 0.811</td> <td>   -5.583</td> <td>    7.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_62</th> <td>   -3.1375</td> <td>    3.566</td> <td>   -0.880</td> <td> 0.381</td> <td>  -10.196</td> <td>    3.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_63</th> <td>    3.6286</td> <td>    3.912</td> <td>    0.928</td> <td> 0.355</td> <td>   -4.113</td> <td>   11.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_64</th> <td>   -2.4542</td> <td>    3.405</td> <td>   -0.721</td> <td> 0.472</td> <td>   -9.192</td> <td>    4.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_65</th> <td>   -0.7552</td> <td>    1.465</td> <td>   -0.516</td> <td> 0.607</td> <td>   -3.654</td> <td>    2.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_66</th> <td>    4.4046</td> <td>    3.275</td> <td>    1.345</td> <td> 0.181</td> <td>   -2.077</td> <td>   10.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_67</th> <td>   -1.8203</td> <td>    2.787</td> <td>   -0.653</td> <td> 0.515</td> <td>   -7.337</td> <td>    3.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_68</th> <td>   -0.7261</td> <td>    3.988</td> <td>   -0.182</td> <td> 0.856</td> <td>   -8.619</td> <td>    7.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_69</th> <td>   -1.2901</td> <td>    3.287</td> <td>   -0.392</td> <td> 0.695</td> <td>   -7.796</td> <td>    5.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_70</th> <td>    0.3460</td> <td>    4.293</td> <td>    0.081</td> <td> 0.936</td> <td>   -8.150</td> <td>    8.842</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_71</th> <td>   -2.1388</td> <td>    3.509</td> <td>   -0.609</td> <td> 0.543</td> <td>   -9.084</td> <td>    4.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_72</th> <td>   -2.8163</td> <td>    3.111</td> <td>   -0.905</td> <td> 0.367</td> <td>   -8.974</td> <td>    3.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_73</th> <td>   -3.4105</td> <td>    3.687</td> <td>   -0.925</td> <td> 0.357</td> <td>  -10.708</td> <td>    3.887</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>142.102</td> <th>  Durbin-Watson:     </th> <td>   1.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1788.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.537</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.745</td>  <th>  Cond. No.          </th> <td>    156.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                                               OLS Regression Results                                                              \n",
       "===================================================================================================================================================\n",
       "Dep. Variable:     Overall GPA (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")   R-squared:                       0.349\n",
       "Model:                                                                                                 OLS   Adj. R-squared:                 -0.037\n",
       "Method:                                                                                      Least Squares   F-statistic:                    0.9045\n",
       "Date:                                                                                     Wed, 15 Feb 2023   Prob (F-statistic):              0.678\n",
       "Time:                                                                                             13:50:19   Log-Likelihood:                -94.178\n",
       "No. Observations:                                                                                      200   AIC:                             338.4\n",
       "Df Residuals:                                                                                          125   BIC:                             585.7\n",
       "Df Model:                                                                                               74                                         \n",
       "Covariance Type:                                                                                 nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          4.0690      0.241     16.869      0.000       3.592       4.546\n",
       "topic_0       -0.3662      0.466     -0.785      0.434      -1.289       0.557\n",
       "topic_1       -0.2234      0.718     -0.311      0.756      -1.645       1.198\n",
       "topic_2       -0.0124      1.464     -0.008      0.993      -2.910       2.885\n",
       "topic_3       -1.2918      1.275     -1.013      0.313      -3.814       1.231\n",
       "topic_4       -2.5541      1.159     -2.203      0.029      -4.849      -0.260\n",
       "topic_5       -0.4843      1.380     -0.351      0.726      -3.215       2.247\n",
       "topic_6       -2.5279      1.678     -1.507      0.134      -5.848       0.792\n",
       "topic_7       -0.4852      1.699     -0.286      0.776      -3.847       2.877\n",
       "topic_8        1.5084      1.834      0.822      0.412      -2.122       5.138\n",
       "topic_9        0.1670      1.519      0.110      0.913      -2.840       3.174\n",
       "topic_10      -0.6543      1.742     -0.376      0.708      -4.102       2.793\n",
       "topic_11       0.0078      1.845      0.004      0.997      -3.643       3.659\n",
       "topic_12      -1.8090      1.961     -0.922      0.358      -5.690       2.072\n",
       "topic_13      -3.6056      1.930     -1.868      0.064      -7.426       0.215\n",
       "topic_14      -3.4384      1.619     -2.124      0.036      -6.642      -0.235\n",
       "topic_15      -0.9715      1.975     -0.492      0.624      -4.881       2.937\n",
       "topic_16       1.9236      2.149      0.895      0.372      -2.330       6.177\n",
       "topic_17       2.0245      2.226      0.910      0.365      -2.381       6.430\n",
       "topic_18      -1.5980      1.845     -0.866      0.388      -5.249       2.053\n",
       "topic_19      -0.5007      1.951     -0.257      0.798      -4.363       3.361\n",
       "topic_20      -2.2793      1.875     -1.216      0.226      -5.989       1.431\n",
       "topic_21      -5.8579      2.006     -2.921      0.004      -9.827      -1.889\n",
       "topic_22       1.4057      1.991      0.706      0.481      -2.534       5.345\n",
       "topic_23      -1.2943      1.910     -0.678      0.499      -5.074       2.486\n",
       "topic_24      -1.2881      1.896     -0.679      0.498      -5.041       2.465\n",
       "topic_25       0.0037      2.476      0.001      0.999      -4.897       4.904\n",
       "topic_26      -1.9167      2.109     -0.909      0.365      -6.091       2.257\n",
       "topic_27      -1.8099      2.207     -0.820      0.414      -6.178       2.558\n",
       "topic_28      -0.7205      2.620     -0.275      0.784      -5.905       4.464\n",
       "topic_29       1.5551      2.844      0.547      0.585      -4.073       7.184\n",
       "topic_30      -1.9072      2.248     -0.848      0.398      -6.357       2.543\n",
       "topic_31      -4.6742      2.562     -1.824      0.071      -9.745       0.397\n",
       "topic_32      -2.7438      2.408     -1.139      0.257      -7.510       2.023\n",
       "topic_33       0.8892      2.646      0.336      0.737      -4.347       6.126\n",
       "topic_34       1.5135      2.327      0.650      0.517      -3.092       6.119\n",
       "topic_35       2.8366      2.333      1.216      0.226      -1.780       7.453\n",
       "topic_36      -2.5576      2.596     -0.985      0.326      -7.696       2.581\n",
       "topic_37      -4.5457      2.650     -1.715      0.089      -9.791       0.700\n",
       "topic_38       0.6282      2.244      0.280      0.780      -3.813       5.070\n",
       "topic_39       4.4940      2.596      1.731      0.086      -0.643       9.631\n",
       "topic_40      -2.1688      2.372     -0.914      0.362      -6.863       2.525\n",
       "topic_41       2.2612      2.433      0.929      0.354      -2.554       7.076\n",
       "topic_42      -0.7557      2.556     -0.296      0.768      -5.815       4.303\n",
       "topic_43      -2.1592      2.354     -0.917      0.361      -6.817       2.499\n",
       "topic_44       1.0552      2.393      0.441      0.660      -3.681       5.792\n",
       "topic_45       1.0077      2.787      0.362      0.718      -4.509       6.524\n",
       "topic_46       0.9208      2.339      0.394      0.694      -3.708       5.549\n",
       "topic_47      -2.8752      3.574     -0.804      0.423      -9.949       4.198\n",
       "topic_48       4.0619      2.794      1.454      0.148      -1.467       9.591\n",
       "topic_49      -4.9166      3.419     -1.438      0.153     -11.683       1.850\n",
       "topic_50       0.0983      3.256      0.030      0.976      -6.345       6.542\n",
       "topic_51       0.7423      2.742      0.271      0.787      -4.685       6.170\n",
       "topic_52       1.3950      2.346      0.595      0.553      -3.247       6.037\n",
       "topic_53       0.5864      2.523      0.232      0.817      -4.407       5.580\n",
       "topic_54       0.1608      2.849      0.056      0.955      -5.478       5.800\n",
       "topic_55       0.3061      3.009      0.102      0.919      -5.649       6.262\n",
       "topic_56      -0.9704      3.097     -0.313      0.755      -7.099       5.158\n",
       "topic_57      -3.0921      3.509     -0.881      0.380     -10.037       3.852\n",
       "topic_58      -3.0464      2.974     -1.024      0.308      -8.933       2.840\n",
       "topic_59      -5.2719      3.703     -1.424      0.157     -12.600       2.056\n",
       "topic_60       0.9450      3.098      0.305      0.761      -5.185       7.075\n",
       "topic_61       0.7688      3.210      0.240      0.811      -5.583       7.121\n",
       "topic_62      -3.1375      3.566     -0.880      0.381     -10.196       3.921\n",
       "topic_63       3.6286      3.912      0.928      0.355      -4.113      11.371\n",
       "topic_64      -2.4542      3.405     -0.721      0.472      -9.192       4.284\n",
       "topic_65      -0.7552      1.465     -0.516      0.607      -3.654       2.143\n",
       "topic_66       4.4046      3.275      1.345      0.181      -2.077      10.887\n",
       "topic_67      -1.8203      2.787     -0.653      0.515      -7.337       3.696\n",
       "topic_68      -0.7261      3.988     -0.182      0.856      -8.619       7.166\n",
       "topic_69      -1.2901      3.287     -0.392      0.695      -7.796       5.215\n",
       "topic_70       0.3460      4.293      0.081      0.936      -8.150       8.842\n",
       "topic_71      -2.1388      3.509     -0.609      0.543      -9.084       4.806\n",
       "topic_72      -2.8163      3.111     -0.905      0.367      -8.974       3.341\n",
       "topic_73      -3.4105      3.687     -0.925      0.357     -10.708       3.887\n",
       "==============================================================================\n",
       "Omnibus:                      142.102   Durbin-Watson:                   1.907\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1788.859\n",
       "Skew:                          -2.537   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.745   Cond. No.                         156.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(topics_df.astype(float))\n",
    "old_reg = sm.OLS(\n",
    "    tar_a_df.astype(float),\n",
    "    X,\n",
    "# ).fit(maxiter=1000, method='bfgs') # bfgs nm\n",
    ").fit(maxiter=1000) # bfgs nm\n",
    "old_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "40ca9272-7ca1-41bc-bbb1-8f1b342bc959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with statistical significance with alpha of 0.1 (p-values less than this):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const       4.068980\n",
       "topic_4    -2.554134\n",
       "topic_13   -3.605600\n",
       "topic_14   -3.438412\n",
       "topic_21   -5.857884\n",
       "topic_31   -4.674221\n",
       "topic_37   -4.545744\n",
       "topic_39    4.494027\n",
       "dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"variables with statistical significance with alpha of {p_alpha} (p-values less than this):\")\n",
    "significant_var_coeffs = old_reg.params[old_reg.pvalues < p_alpha]\n",
    "significant_var_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b3870e9e-7f7d-450c-a00f-dc30056e87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with negative effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_4    -2.554134\n",
       "topic_13   -3.605600\n",
       "topic_14   -3.438412\n",
       "topic_21   -5.857884\n",
       "topic_31   -4.674221\n",
       "topic_37   -4.545744\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with negative effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs < 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f5ce8e1a-2da2-4c8d-9ac1-ebc1a80efe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with positive effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const       4.068980\n",
       "topic_39    4.494027\n",
       "dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with positive effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582c3b6-6ccd-496b-a4d8-2ae8cd5ef6a3",
   "metadata": {},
   "source": [
    "#### predicting **major** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0eb16e04-c70c-402a-a6f2-40306fac766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>GPA for your Major (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")</td> <th>  R-squared:         </th> <td>   0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                                                          <td>OLS</td>                                              <th>  Adj. R-squared:    </th> <td>  -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                                                    <td>Least Squares</td>                                         <th>  F-statistic:       </th> <td>  0.8460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                                                    <td>Wed, 15 Feb 2023</td>                                        <th>  Prob (F-statistic):</th>  <td> 0.782</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                                                        <td>13:50:19</td>                                            <th>  Log-Likelihood:    </th> <td> -143.78</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>                                             <td>   200</td>                                             <th>  AIC:               </th> <td>   437.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>                                                 <td>   125</td>                                             <th>  BIC:               </th> <td>   684.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                                                     <td>    74</td>                                             <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>                                             <td>nonrobust</td>                                           <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    4.0912</td> <td>    0.309</td> <td>   13.236</td> <td> 0.000</td> <td>    3.479</td> <td>    4.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_0</th>  <td>   -0.3320</td> <td>    0.598</td> <td>   -0.556</td> <td> 0.579</td> <td>   -1.515</td> <td>    0.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_1</th>  <td>   -0.8502</td> <td>    0.921</td> <td>   -0.924</td> <td> 0.358</td> <td>   -2.672</td> <td>    0.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_2</th>  <td>   -2.7559</td> <td>    1.876</td> <td>   -1.469</td> <td> 0.144</td> <td>   -6.469</td> <td>    0.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_3</th>  <td>    0.4500</td> <td>    1.633</td> <td>    0.276</td> <td> 0.783</td> <td>   -2.783</td> <td>    3.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_4</th>  <td>   -1.9645</td> <td>    1.486</td> <td>   -1.322</td> <td> 0.189</td> <td>   -4.905</td> <td>    0.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_5</th>  <td>   -2.2355</td> <td>    1.768</td> <td>   -1.264</td> <td> 0.208</td> <td>   -5.735</td> <td>    1.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_6</th>  <td>   -4.2025</td> <td>    2.150</td> <td>   -1.955</td> <td> 0.053</td> <td>   -8.457</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_7</th>  <td>   -1.0477</td> <td>    2.177</td> <td>   -0.481</td> <td> 0.631</td> <td>   -5.356</td> <td>    3.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_8</th>  <td>    3.2549</td> <td>    2.350</td> <td>    1.385</td> <td> 0.169</td> <td>   -1.397</td> <td>    7.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_9</th>  <td>   -2.9976</td> <td>    1.947</td> <td>   -1.539</td> <td> 0.126</td> <td>   -6.851</td> <td>    0.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_10</th> <td>   -2.9953</td> <td>    2.232</td> <td>   -1.342</td> <td> 0.182</td> <td>   -7.413</td> <td>    1.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_11</th> <td>   -2.8498</td> <td>    2.364</td> <td>   -1.205</td> <td> 0.230</td> <td>   -7.529</td> <td>    1.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_12</th> <td>    1.1735</td> <td>    2.513</td> <td>    0.467</td> <td> 0.641</td> <td>   -3.801</td> <td>    6.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_13</th> <td>   -0.4427</td> <td>    2.474</td> <td>   -0.179</td> <td> 0.858</td> <td>   -5.338</td> <td>    4.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_14</th> <td>   -5.7857</td> <td>    2.074</td> <td>   -2.789</td> <td> 0.006</td> <td>   -9.891</td> <td>   -1.680</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_15</th> <td>    1.8741</td> <td>    2.531</td> <td>    0.740</td> <td> 0.460</td> <td>   -3.135</td> <td>    6.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_16</th> <td>    3.6173</td> <td>    2.754</td> <td>    1.313</td> <td> 0.191</td> <td>   -1.833</td> <td>    9.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_17</th> <td>    3.7529</td> <td>    2.852</td> <td>    1.316</td> <td> 0.191</td> <td>   -1.892</td> <td>    9.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_18</th> <td>   -2.1909</td> <td>    2.364</td> <td>   -0.927</td> <td> 0.356</td> <td>   -6.870</td> <td>    2.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_19</th> <td>   -0.4130</td> <td>    2.501</td> <td>   -0.165</td> <td> 0.869</td> <td>   -5.362</td> <td>    4.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_20</th> <td>   -2.8879</td> <td>    2.402</td> <td>   -1.202</td> <td> 0.232</td> <td>   -7.642</td> <td>    1.866</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_21</th> <td>   -7.4430</td> <td>    2.570</td> <td>   -2.896</td> <td> 0.004</td> <td>  -12.530</td> <td>   -2.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_22</th> <td>    3.5392</td> <td>    2.551</td> <td>    1.387</td> <td> 0.168</td> <td>   -1.509</td> <td>    8.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_23</th> <td>    2.0289</td> <td>    2.447</td> <td>    0.829</td> <td> 0.409</td> <td>   -2.815</td> <td>    6.873</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_24</th> <td>   -1.2702</td> <td>    2.430</td> <td>   -0.523</td> <td> 0.602</td> <td>   -6.079</td> <td>    3.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_25</th> <td>   -2.1491</td> <td>    3.173</td> <td>   -0.677</td> <td> 0.499</td> <td>   -8.429</td> <td>    4.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_26</th> <td>   -1.5103</td> <td>    2.703</td> <td>   -0.559</td> <td> 0.577</td> <td>   -6.859</td> <td>    3.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_27</th> <td>   -3.4693</td> <td>    2.828</td> <td>   -1.227</td> <td> 0.222</td> <td>   -9.067</td> <td>    2.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_28</th> <td>   -5.0395</td> <td>    3.357</td> <td>   -1.501</td> <td> 0.136</td> <td>  -11.684</td> <td>    1.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_29</th> <td>    2.2594</td> <td>    3.644</td> <td>    0.620</td> <td> 0.536</td> <td>   -4.953</td> <td>    9.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_30</th> <td>   -2.4005</td> <td>    2.881</td> <td>   -0.833</td> <td> 0.406</td> <td>   -8.103</td> <td>    3.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_31</th> <td>   -8.0693</td> <td>    3.284</td> <td>   -2.458</td> <td> 0.015</td> <td>  -14.568</td> <td>   -1.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_32</th> <td>    0.0941</td> <td>    3.086</td> <td>    0.031</td> <td> 0.976</td> <td>   -6.014</td> <td>    6.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_33</th> <td>   -2.0145</td> <td>    3.391</td> <td>   -0.594</td> <td> 0.554</td> <td>   -8.725</td> <td>    4.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_34</th> <td>    1.6144</td> <td>    2.982</td> <td>    0.541</td> <td> 0.589</td> <td>   -4.287</td> <td>    7.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_35</th> <td>    3.1753</td> <td>    2.989</td> <td>    1.062</td> <td> 0.290</td> <td>   -2.741</td> <td>    9.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_36</th> <td>    2.3011</td> <td>    3.327</td> <td>    0.692</td> <td> 0.490</td> <td>   -4.284</td> <td>    8.886</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_37</th> <td>   -2.2953</td> <td>    3.396</td> <td>   -0.676</td> <td> 0.500</td> <td>   -9.017</td> <td>    4.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_38</th> <td>    4.5779</td> <td>    2.876</td> <td>    1.592</td> <td> 0.114</td> <td>   -1.114</td> <td>   10.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_39</th> <td>    0.9399</td> <td>    3.326</td> <td>    0.283</td> <td> 0.778</td> <td>   -5.643</td> <td>    7.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_40</th> <td>   -0.6532</td> <td>    3.039</td> <td>   -0.215</td> <td> 0.830</td> <td>   -6.669</td> <td>    5.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_41</th> <td>    2.5587</td> <td>    3.118</td> <td>    0.821</td> <td> 0.413</td> <td>   -3.611</td> <td>    8.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_42</th> <td>    1.6077</td> <td>    3.276</td> <td>    0.491</td> <td> 0.624</td> <td>   -4.875</td> <td>    8.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_43</th> <td>   -3.0803</td> <td>    3.016</td> <td>   -1.021</td> <td> 0.309</td> <td>   -9.050</td> <td>    2.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_44</th> <td>    0.3837</td> <td>    3.067</td> <td>    0.125</td> <td> 0.901</td> <td>   -5.686</td> <td>    6.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_45</th> <td>    0.5078</td> <td>    3.572</td> <td>    0.142</td> <td> 0.887</td> <td>   -6.561</td> <td>    7.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_46</th> <td>    0.1570</td> <td>    2.997</td> <td>    0.052</td> <td> 0.958</td> <td>   -5.774</td> <td>    6.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_47</th> <td>   -6.9198</td> <td>    4.580</td> <td>   -1.511</td> <td> 0.133</td> <td>  -15.984</td> <td>    2.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_48</th> <td>    3.2392</td> <td>    3.580</td> <td>    0.905</td> <td> 0.367</td> <td>   -3.846</td> <td>   10.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_49</th> <td>   -4.8195</td> <td>    4.381</td> <td>   -1.100</td> <td> 0.273</td> <td>  -13.491</td> <td>    3.852</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_50</th> <td>    2.1313</td> <td>    4.172</td> <td>    0.511</td> <td> 0.610</td> <td>   -6.126</td> <td>   10.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_51</th> <td>    1.4804</td> <td>    3.514</td> <td>    0.421</td> <td> 0.674</td> <td>   -5.475</td> <td>    8.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_52</th> <td>    2.4097</td> <td>    3.006</td> <td>    0.802</td> <td> 0.424</td> <td>   -3.539</td> <td>    8.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_53</th> <td>    0.1457</td> <td>    3.233</td> <td>    0.045</td> <td> 0.964</td> <td>   -6.254</td> <td>    6.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_54</th> <td>    1.4647</td> <td>    3.651</td> <td>    0.401</td> <td> 0.689</td> <td>   -5.761</td> <td>    8.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_55</th> <td>   -0.1817</td> <td>    3.856</td> <td>   -0.047</td> <td> 0.962</td> <td>   -7.814</td> <td>    7.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_56</th> <td>   -1.9924</td> <td>    3.968</td> <td>   -0.502</td> <td> 0.616</td> <td>   -9.846</td> <td>    5.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_57</th> <td>   -0.4789</td> <td>    4.497</td> <td>   -0.106</td> <td> 0.915</td> <td>   -9.378</td> <td>    8.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_58</th> <td>   -0.1716</td> <td>    3.812</td> <td>   -0.045</td> <td> 0.964</td> <td>   -7.715</td> <td>    7.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_59</th> <td>   -0.9196</td> <td>    4.745</td> <td>   -0.194</td> <td> 0.847</td> <td>  -10.311</td> <td>    8.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_60</th> <td>    2.0201</td> <td>    3.969</td> <td>    0.509</td> <td> 0.612</td> <td>   -5.836</td> <td>    9.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_61</th> <td>    3.7996</td> <td>    4.113</td> <td>    0.924</td> <td> 0.357</td> <td>   -4.341</td> <td>   11.940</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_62</th> <td>   -9.2275</td> <td>    4.570</td> <td>   -2.019</td> <td> 0.046</td> <td>  -18.272</td> <td>   -0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_63</th> <td>    5.3156</td> <td>    5.013</td> <td>    1.060</td> <td> 0.291</td> <td>   -4.606</td> <td>   15.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_64</th> <td>   -2.4010</td> <td>    4.363</td> <td>   -0.550</td> <td> 0.583</td> <td>  -11.036</td> <td>    6.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_65</th> <td>   -0.6657</td> <td>    1.877</td> <td>   -0.355</td> <td> 0.723</td> <td>   -4.380</td> <td>    3.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_66</th> <td>    6.4875</td> <td>    4.197</td> <td>    1.546</td> <td> 0.125</td> <td>   -1.819</td> <td>   14.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_67</th> <td>   -0.6136</td> <td>    3.572</td> <td>   -0.172</td> <td> 0.864</td> <td>   -7.683</td> <td>    6.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_68</th> <td>   -1.8175</td> <td>    5.110</td> <td>   -0.356</td> <td> 0.723</td> <td>  -11.932</td> <td>    8.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_69</th> <td>    0.6419</td> <td>    4.212</td> <td>    0.152</td> <td> 0.879</td> <td>   -7.695</td> <td>    8.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_70</th> <td>   -3.5535</td> <td>    5.501</td> <td>   -0.646</td> <td> 0.519</td> <td>  -14.441</td> <td>    7.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_71</th> <td>   -3.4764</td> <td>    4.497</td> <td>   -0.773</td> <td> 0.441</td> <td>  -12.377</td> <td>    5.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_72</th> <td>   -6.3149</td> <td>    3.987</td> <td>   -1.584</td> <td> 0.116</td> <td>  -14.206</td> <td>    1.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_73</th> <td>   -5.0869</td> <td>    4.725</td> <td>   -1.077</td> <td> 0.284</td> <td>  -14.438</td> <td>    4.264</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>131.147</td> <th>  Durbin-Watson:     </th> <td>   1.896</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1296.854</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.368</td>  <th>  Prob(JB):          </th> <td>2.46e-282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.541</td>  <th>  Cond. No.          </th> <td>    156.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                                                  OLS Regression Results                                                                  \n",
       "==========================================================================================================================================================\n",
       "Dep. Variable:     GPA for your Major (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")   R-squared:                       0.334\n",
       "Model:                                                                                                        OLS   Adj. R-squared:                 -0.061\n",
       "Method:                                                                                             Least Squares   F-statistic:                    0.8460\n",
       "Date:                                                                                            Wed, 15 Feb 2023   Prob (F-statistic):              0.782\n",
       "Time:                                                                                                    13:50:19   Log-Likelihood:                -143.78\n",
       "No. Observations:                                                                                             200   AIC:                             437.6\n",
       "Df Residuals:                                                                                                 125   BIC:                             684.9\n",
       "Df Model:                                                                                                      74                                         \n",
       "Covariance Type:                                                                                        nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          4.0912      0.309     13.236      0.000       3.479       4.703\n",
       "topic_0       -0.3320      0.598     -0.556      0.579      -1.515       0.851\n",
       "topic_1       -0.8502      0.921     -0.924      0.358      -2.672       0.972\n",
       "topic_2       -2.7559      1.876     -1.469      0.144      -6.469       0.957\n",
       "topic_3        0.4500      1.633      0.276      0.783      -2.783       3.683\n",
       "topic_4       -1.9645      1.486     -1.322      0.189      -4.905       0.976\n",
       "topic_5       -2.2355      1.768     -1.264      0.208      -5.735       1.264\n",
       "topic_6       -4.2025      2.150     -1.955      0.053      -8.457       0.052\n",
       "topic_7       -1.0477      2.177     -0.481      0.631      -5.356       3.260\n",
       "topic_8        3.2549      2.350      1.385      0.169      -1.397       7.907\n",
       "topic_9       -2.9976      1.947     -1.539      0.126      -6.851       0.856\n",
       "topic_10      -2.9953      2.232     -1.342      0.182      -7.413       1.422\n",
       "topic_11      -2.8498      2.364     -1.205      0.230      -7.529       1.829\n",
       "topic_12       1.1735      2.513      0.467      0.641      -3.801       6.147\n",
       "topic_13      -0.4427      2.474     -0.179      0.858      -5.338       4.453\n",
       "topic_14      -5.7857      2.074     -2.789      0.006      -9.891      -1.680\n",
       "topic_15       1.8741      2.531      0.740      0.460      -3.135       6.883\n",
       "topic_16       3.6173      2.754      1.313      0.191      -1.833       9.068\n",
       "topic_17       3.7529      2.852      1.316      0.191      -1.892       9.398\n",
       "topic_18      -2.1909      2.364     -0.927      0.356      -6.870       2.488\n",
       "topic_19      -0.4130      2.501     -0.165      0.869      -5.362       4.536\n",
       "topic_20      -2.8879      2.402     -1.202      0.232      -7.642       1.866\n",
       "topic_21      -7.4430      2.570     -2.896      0.004     -12.530      -2.356\n",
       "topic_22       3.5392      2.551      1.387      0.168      -1.509       8.588\n",
       "topic_23       2.0289      2.447      0.829      0.409      -2.815       6.873\n",
       "topic_24      -1.2702      2.430     -0.523      0.602      -6.079       3.539\n",
       "topic_25      -2.1491      3.173     -0.677      0.499      -8.429       4.131\n",
       "topic_26      -1.5103      2.703     -0.559      0.577      -6.859       3.839\n",
       "topic_27      -3.4693      2.828     -1.227      0.222      -9.067       2.128\n",
       "topic_28      -5.0395      3.357     -1.501      0.136     -11.684       1.605\n",
       "topic_29       2.2594      3.644      0.620      0.536      -4.953       9.472\n",
       "topic_30      -2.4005      2.881     -0.833      0.406      -8.103       3.302\n",
       "topic_31      -8.0693      3.284     -2.458      0.015     -14.568      -1.571\n",
       "topic_32       0.0941      3.086      0.031      0.976      -6.014       6.202\n",
       "topic_33      -2.0145      3.391     -0.594      0.554      -8.725       4.696\n",
       "topic_34       1.6144      2.982      0.541      0.589      -4.287       7.516\n",
       "topic_35       3.1753      2.989      1.062      0.290      -2.741       9.092\n",
       "topic_36       2.3011      3.327      0.692      0.490      -4.284       8.886\n",
       "topic_37      -2.2953      3.396     -0.676      0.500      -9.017       4.427\n",
       "topic_38       4.5779      2.876      1.592      0.114      -1.114      10.270\n",
       "topic_39       0.9399      3.326      0.283      0.778      -5.643       7.523\n",
       "topic_40      -0.6532      3.039     -0.215      0.830      -6.669       5.362\n",
       "topic_41       2.5587      3.118      0.821      0.413      -3.611       8.729\n",
       "topic_42       1.6077      3.276      0.491      0.624      -4.875       8.091\n",
       "topic_43      -3.0803      3.016     -1.021      0.309      -9.050       2.889\n",
       "topic_44       0.3837      3.067      0.125      0.901      -5.686       6.454\n",
       "topic_45       0.5078      3.572      0.142      0.887      -6.561       7.577\n",
       "topic_46       0.1570      2.997      0.052      0.958      -5.774       6.088\n",
       "topic_47      -6.9198      4.580     -1.511      0.133     -15.984       2.145\n",
       "topic_48       3.2392      3.580      0.905      0.367      -3.846      10.324\n",
       "topic_49      -4.8195      4.381     -1.100      0.273     -13.491       3.852\n",
       "topic_50       2.1313      4.172      0.511      0.610      -6.126      10.388\n",
       "topic_51       1.4804      3.514      0.421      0.674      -5.475       8.435\n",
       "topic_52       2.4097      3.006      0.802      0.424      -3.539       8.358\n",
       "topic_53       0.1457      3.233      0.045      0.964      -6.254       6.545\n",
       "topic_54       1.4647      3.651      0.401      0.689      -5.761       8.691\n",
       "topic_55      -0.1817      3.856     -0.047      0.962      -7.814       7.450\n",
       "topic_56      -1.9924      3.968     -0.502      0.616      -9.846       5.861\n",
       "topic_57      -0.4789      4.497     -0.106      0.915      -9.378       8.420\n",
       "topic_58      -0.1716      3.812     -0.045      0.964      -7.715       7.372\n",
       "topic_59      -0.9196      4.745     -0.194      0.847     -10.311       8.471\n",
       "topic_60       2.0201      3.969      0.509      0.612      -5.836       9.876\n",
       "topic_61       3.7996      4.113      0.924      0.357      -4.341      11.940\n",
       "topic_62      -9.2275      4.570     -2.019      0.046     -18.272      -0.183\n",
       "topic_63       5.3156      5.013      1.060      0.291      -4.606      15.237\n",
       "topic_64      -2.4010      4.363     -0.550      0.583     -11.036       6.234\n",
       "topic_65      -0.6657      1.877     -0.355      0.723      -4.380       3.049\n",
       "topic_66       6.4875      4.197      1.546      0.125      -1.819      14.794\n",
       "topic_67      -0.6136      3.572     -0.172      0.864      -7.683       6.456\n",
       "topic_68      -1.8175      5.110     -0.356      0.723     -11.932       8.296\n",
       "topic_69       0.6419      4.212      0.152      0.879      -7.695       8.979\n",
       "topic_70      -3.5535      5.501     -0.646      0.519     -14.441       7.334\n",
       "topic_71      -3.4764      4.497     -0.773      0.441     -12.377       5.424\n",
       "topic_72      -6.3149      3.987     -1.584      0.116     -14.206       1.576\n",
       "topic_73      -5.0869      4.725     -1.077      0.284     -14.438       4.264\n",
       "==============================================================================\n",
       "Omnibus:                      131.147   Durbin-Watson:                   1.896\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1296.854\n",
       "Skew:                          -2.368   Prob(JB):                    2.46e-282\n",
       "Kurtosis:                      14.541   Cond. No.                         156.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(topics_df.astype(float))\n",
    "old_reg = sm.OLS(\n",
    "    tar_b_df.astype(float),\n",
    "    X,\n",
    "# ).fit(maxiter=1000, method='bfgs') # bfgs nm\n",
    ").fit(maxiter=1000) # bfgs nm\n",
    "old_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d6346105-bd6b-45b4-8b67-b72b2ff830b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with statistical significance with alpha of 0.1 (p-values less than this):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const       4.091209\n",
       "topic_6    -4.202477\n",
       "topic_14   -5.785704\n",
       "topic_21   -7.442972\n",
       "topic_31   -8.069259\n",
       "topic_62   -9.227525\n",
       "dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"variables with statistical significance with alpha of {p_alpha} (p-values less than this):\")\n",
    "significant_var_coeffs = old_reg.params[old_reg.pvalues < p_alpha]\n",
    "significant_var_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b0457800-c2c0-47be-be59-8b168d15f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with negative effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_6    -4.202477\n",
       "topic_14   -5.785704\n",
       "topic_21   -7.442972\n",
       "topic_31   -8.069259\n",
       "topic_62   -9.227525\n",
       "dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with negative effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs < 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7006c074-f893-46e0-9b7d-39d366a10693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with positive effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const    4.091209\n",
       "dtype: float64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with positive effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b68fc-be7b-46d3-a667-6a5fcf2145c5",
   "metadata": {},
   "source": [
    "## Using *boolean* topics as inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70156d-2687-4364-b690-421210b6d2c3",
   "metadata": {},
   "source": [
    "### predicting *normalized* GPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922432e3-a3d7-4d9d-aa03-298ebc7d2e98",
   "metadata": {},
   "source": [
    "#### predicting **overall** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5119592e-0ee0-47c0-8cc0-ad8df3e44a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>overall_gpa</td>   <th>  R-squared:         </th> <td>   0.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.9985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 15 Feb 2023</td> <th>  Prob (F-statistic):</th>  <td> 0.496</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:50:19</td>     <th>  Log-Likelihood:    </th> <td> -236.84</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   623.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   125</td>      <th>  BIC:               </th> <td>   871.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    74</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    0.4559</td> <td>    0.414</td> <td>    1.100</td> <td> 0.273</td> <td>   -0.364</td> <td>    1.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_0</th>  <td>   -0.1586</td> <td>    0.439</td> <td>   -0.361</td> <td> 0.719</td> <td>   -1.028</td> <td>    0.711</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_1</th>  <td>   -0.0737</td> <td>    0.182</td> <td>   -0.404</td> <td> 0.687</td> <td>   -0.435</td> <td>    0.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_2</th>  <td>    0.3145</td> <td>    0.194</td> <td>    1.620</td> <td> 0.108</td> <td>   -0.070</td> <td>    0.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_3</th>  <td>   -0.0340</td> <td>    0.203</td> <td>   -0.168</td> <td> 0.867</td> <td>   -0.435</td> <td>    0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_4</th>  <td>   -0.4153</td> <td>    0.195</td> <td>   -2.135</td> <td> 0.035</td> <td>   -0.800</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_5</th>  <td>   -0.0654</td> <td>    0.196</td> <td>   -0.333</td> <td> 0.740</td> <td>   -0.454</td> <td>    0.323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_6</th>  <td>   -0.2393</td> <td>    0.223</td> <td>   -1.071</td> <td> 0.286</td> <td>   -0.682</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_7</th>  <td>    0.1208</td> <td>    0.218</td> <td>    0.553</td> <td> 0.581</td> <td>   -0.311</td> <td>    0.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_8</th>  <td>    0.1822</td> <td>    0.212</td> <td>    0.862</td> <td> 0.391</td> <td>   -0.236</td> <td>    0.601</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_9</th>  <td>   -0.0665</td> <td>    0.212</td> <td>   -0.314</td> <td> 0.754</td> <td>   -0.486</td> <td>    0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_10</th> <td>   -0.1508</td> <td>    0.236</td> <td>   -0.639</td> <td> 0.524</td> <td>   -0.618</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_11</th> <td>    0.0528</td> <td>    0.222</td> <td>    0.237</td> <td> 0.813</td> <td>   -0.387</td> <td>    0.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_12</th> <td>   -0.3205</td> <td>    0.229</td> <td>   -1.397</td> <td> 0.165</td> <td>   -0.775</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_13</th> <td>    0.2232</td> <td>    0.232</td> <td>    0.964</td> <td> 0.337</td> <td>   -0.235</td> <td>    0.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_14</th> <td>   -0.4939</td> <td>    0.226</td> <td>   -2.186</td> <td> 0.031</td> <td>   -0.941</td> <td>   -0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_15</th> <td>   -0.0576</td> <td>    0.234</td> <td>   -0.246</td> <td> 0.806</td> <td>   -0.521</td> <td>    0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_16</th> <td>    0.5395</td> <td>    0.257</td> <td>    2.103</td> <td> 0.037</td> <td>    0.032</td> <td>    1.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_17</th> <td>    0.3845</td> <td>    0.252</td> <td>    1.527</td> <td> 0.129</td> <td>   -0.114</td> <td>    0.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_18</th> <td>   -0.3337</td> <td>    0.229</td> <td>   -1.458</td> <td> 0.147</td> <td>   -0.787</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_19</th> <td>    0.1345</td> <td>    0.248</td> <td>    0.542</td> <td> 0.589</td> <td>   -0.357</td> <td>    0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_20</th> <td>   -0.3176</td> <td>    0.245</td> <td>   -1.299</td> <td> 0.196</td> <td>   -0.802</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_21</th> <td>   -0.5525</td> <td>    0.252</td> <td>   -2.194</td> <td> 0.030</td> <td>   -1.051</td> <td>   -0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_22</th> <td>    0.1822</td> <td>    0.242</td> <td>    0.752</td> <td> 0.453</td> <td>   -0.297</td> <td>    0.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_23</th> <td>   -0.0291</td> <td>    0.259</td> <td>   -0.112</td> <td> 0.911</td> <td>   -0.542</td> <td>    0.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_24</th> <td>   -0.3013</td> <td>    0.261</td> <td>   -1.154</td> <td> 0.251</td> <td>   -0.818</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_25</th> <td>   -0.0931</td> <td>    0.253</td> <td>   -0.368</td> <td> 0.714</td> <td>   -0.594</td> <td>    0.408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_26</th> <td>   -0.1185</td> <td>    0.257</td> <td>   -0.461</td> <td> 0.646</td> <td>   -0.627</td> <td>    0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_27</th> <td>   -0.2577</td> <td>    0.246</td> <td>   -1.046</td> <td> 0.298</td> <td>   -0.745</td> <td>    0.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_28</th> <td>    0.0242</td> <td>    0.270</td> <td>    0.090</td> <td> 0.929</td> <td>   -0.510</td> <td>    0.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_29</th> <td>    0.0997</td> <td>    0.274</td> <td>    0.364</td> <td> 0.717</td> <td>   -0.442</td> <td>    0.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_30</th> <td>   -0.1698</td> <td>    0.294</td> <td>   -0.578</td> <td> 0.564</td> <td>   -0.751</td> <td>    0.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_31</th> <td>   -0.6068</td> <td>    0.279</td> <td>   -2.177</td> <td> 0.031</td> <td>   -1.159</td> <td>   -0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_32</th> <td>   -0.4184</td> <td>    0.253</td> <td>   -1.652</td> <td> 0.101</td> <td>   -0.920</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_33</th> <td>    0.5101</td> <td>    0.279</td> <td>    1.826</td> <td> 0.070</td> <td>   -0.043</td> <td>    1.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_34</th> <td>    0.1480</td> <td>    0.288</td> <td>    0.514</td> <td> 0.608</td> <td>   -0.422</td> <td>    0.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_35</th> <td>    0.3211</td> <td>    0.288</td> <td>    1.113</td> <td> 0.268</td> <td>   -0.250</td> <td>    0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_36</th> <td>   -0.2471</td> <td>    0.271</td> <td>   -0.910</td> <td> 0.364</td> <td>   -0.784</td> <td>    0.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_37</th> <td>   -0.3751</td> <td>    0.285</td> <td>   -1.317</td> <td> 0.190</td> <td>   -0.939</td> <td>    0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_38</th> <td>    0.3180</td> <td>    0.283</td> <td>    1.123</td> <td> 0.264</td> <td>   -0.242</td> <td>    0.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_39</th> <td>    0.6972</td> <td>    0.271</td> <td>    2.569</td> <td> 0.011</td> <td>    0.160</td> <td>    1.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_40</th> <td>   -0.1445</td> <td>    0.280</td> <td>   -0.516</td> <td> 0.607</td> <td>   -0.698</td> <td>    0.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_41</th> <td>    0.3633</td> <td>    0.273</td> <td>    1.331</td> <td> 0.185</td> <td>   -0.177</td> <td>    0.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_42</th> <td>    0.1379</td> <td>    0.290</td> <td>    0.475</td> <td> 0.635</td> <td>   -0.436</td> <td>    0.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_43</th> <td>   -0.1178</td> <td>    0.271</td> <td>   -0.434</td> <td> 0.665</td> <td>   -0.654</td> <td>    0.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_44</th> <td>    0.3195</td> <td>    0.292</td> <td>    1.093</td> <td> 0.276</td> <td>   -0.259</td> <td>    0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_45</th> <td>   -0.1156</td> <td>    0.291</td> <td>   -0.397</td> <td> 0.692</td> <td>   -0.692</td> <td>    0.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_46</th> <td>    0.0604</td> <td>    0.285</td> <td>    0.212</td> <td> 0.833</td> <td>   -0.504</td> <td>    0.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_47</th> <td>   -0.1144</td> <td>    0.329</td> <td>   -0.348</td> <td> 0.728</td> <td>   -0.765</td> <td>    0.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_48</th> <td>    0.5982</td> <td>    0.318</td> <td>    1.883</td> <td> 0.062</td> <td>   -0.031</td> <td>    1.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_49</th> <td>   -0.5898</td> <td>    0.365</td> <td>   -1.618</td> <td> 0.108</td> <td>   -1.311</td> <td>    0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_50</th> <td>   -0.0419</td> <td>    0.331</td> <td>   -0.126</td> <td> 0.900</td> <td>   -0.697</td> <td>    0.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_51</th> <td>    0.0740</td> <td>    0.318</td> <td>    0.233</td> <td> 0.816</td> <td>   -0.555</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_52</th> <td>    0.2083</td> <td>    0.316</td> <td>    0.659</td> <td> 0.511</td> <td>   -0.417</td> <td>    0.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_53</th> <td>    0.0455</td> <td>    0.328</td> <td>    0.139</td> <td> 0.890</td> <td>   -0.604</td> <td>    0.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_54</th> <td>    0.1826</td> <td>    0.381</td> <td>    0.480</td> <td> 0.632</td> <td>   -0.570</td> <td>    0.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_55</th> <td>    0.1811</td> <td>    0.310</td> <td>    0.584</td> <td> 0.561</td> <td>   -0.433</td> <td>    0.795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_56</th> <td>   -0.0414</td> <td>    0.344</td> <td>   -0.120</td> <td> 0.904</td> <td>   -0.722</td> <td>    0.639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_57</th> <td>   -0.4194</td> <td>    0.318</td> <td>   -1.321</td> <td> 0.189</td> <td>   -1.048</td> <td>    0.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_58</th> <td>   -0.0927</td> <td>    0.313</td> <td>   -0.296</td> <td> 0.768</td> <td>   -0.712</td> <td>    0.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_59</th> <td>   -0.5605</td> <td>    0.360</td> <td>   -1.555</td> <td> 0.122</td> <td>   -1.274</td> <td>    0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_60</th> <td>    0.2432</td> <td>    0.312</td> <td>    0.779</td> <td> 0.437</td> <td>   -0.375</td> <td>    0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_61</th> <td>    0.2789</td> <td>    0.345</td> <td>    0.808</td> <td> 0.421</td> <td>   -0.404</td> <td>    0.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_62</th> <td>   -0.3860</td> <td>    0.320</td> <td>   -1.208</td> <td> 0.229</td> <td>   -1.018</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_63</th> <td>    0.6079</td> <td>    0.366</td> <td>    1.660</td> <td> 0.099</td> <td>   -0.117</td> <td>    1.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_64</th> <td>   -0.2427</td> <td>    0.343</td> <td>   -0.707</td> <td> 0.481</td> <td>   -0.922</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_65</th> <td>   -0.2176</td> <td>    0.329</td> <td>   -0.662</td> <td> 0.509</td> <td>   -0.868</td> <td>    0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_66</th> <td>    0.6018</td> <td>    0.349</td> <td>    1.727</td> <td> 0.087</td> <td>   -0.088</td> <td>    1.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_67</th> <td>   -0.1993</td> <td>    0.356</td> <td>   -0.560</td> <td> 0.576</td> <td>   -0.904</td> <td>    0.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_68</th> <td>   -0.2317</td> <td>    0.417</td> <td>   -0.556</td> <td> 0.579</td> <td>   -1.056</td> <td>    0.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_69</th> <td>    0.3759</td> <td>    0.357</td> <td>    1.052</td> <td> 0.295</td> <td>   -0.331</td> <td>    1.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_70</th> <td>   -0.0382</td> <td>    0.450</td> <td>   -0.085</td> <td> 0.932</td> <td>   -0.928</td> <td>    0.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_71</th> <td>    0.0605</td> <td>    0.387</td> <td>    0.156</td> <td> 0.876</td> <td>   -0.705</td> <td>    0.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_72</th> <td>   -0.4626</td> <td>    0.428</td> <td>   -1.082</td> <td> 0.281</td> <td>   -1.309</td> <td>    0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_73</th> <td>   -0.3897</td> <td>    0.417</td> <td>   -0.934</td> <td> 0.352</td> <td>   -1.216</td> <td>    0.437</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>113.919</td> <th>  Durbin-Watson:     </th> <td>   2.027</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 971.118</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.011</td>  <th>  Prob(JB):          </th> <td>1.33e-211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>13.018</td>  <th>  Cond. No.          </th> <td>    18.3</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            overall_gpa   R-squared:                       0.372\n",
       "Model:                            OLS   Adj. R-squared:                 -0.001\n",
       "Method:                 Least Squares   F-statistic:                    0.9985\n",
       "Date:                Wed, 15 Feb 2023   Prob (F-statistic):              0.496\n",
       "Time:                        13:50:19   Log-Likelihood:                -236.84\n",
       "No. Observations:                 200   AIC:                             623.7\n",
       "Df Residuals:                     125   BIC:                             871.1\n",
       "Df Model:                          74                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.4559      0.414      1.100      0.273      -0.364       1.276\n",
       "topic_0       -0.1586      0.439     -0.361      0.719      -1.028       0.711\n",
       "topic_1       -0.0737      0.182     -0.404      0.687      -0.435       0.287\n",
       "topic_2        0.3145      0.194      1.620      0.108      -0.070       0.699\n",
       "topic_3       -0.0340      0.203     -0.168      0.867      -0.435       0.367\n",
       "topic_4       -0.4153      0.195     -2.135      0.035      -0.800      -0.030\n",
       "topic_5       -0.0654      0.196     -0.333      0.740      -0.454       0.323\n",
       "topic_6       -0.2393      0.223     -1.071      0.286      -0.682       0.203\n",
       "topic_7        0.1208      0.218      0.553      0.581      -0.311       0.553\n",
       "topic_8        0.1822      0.212      0.862      0.391      -0.236       0.601\n",
       "topic_9       -0.0665      0.212     -0.314      0.754      -0.486       0.353\n",
       "topic_10      -0.1508      0.236     -0.639      0.524      -0.618       0.316\n",
       "topic_11       0.0528      0.222      0.237      0.813      -0.387       0.493\n",
       "topic_12      -0.3205      0.229     -1.397      0.165      -0.775       0.134\n",
       "topic_13       0.2232      0.232      0.964      0.337      -0.235       0.682\n",
       "topic_14      -0.4939      0.226     -2.186      0.031      -0.941      -0.047\n",
       "topic_15      -0.0576      0.234     -0.246      0.806      -0.521       0.406\n",
       "topic_16       0.5395      0.257      2.103      0.037       0.032       1.047\n",
       "topic_17       0.3845      0.252      1.527      0.129      -0.114       0.883\n",
       "topic_18      -0.3337      0.229     -1.458      0.147      -0.787       0.119\n",
       "topic_19       0.1345      0.248      0.542      0.589      -0.357       0.626\n",
       "topic_20      -0.3176      0.245     -1.299      0.196      -0.802       0.166\n",
       "topic_21      -0.5525      0.252     -2.194      0.030      -1.051      -0.054\n",
       "topic_22       0.1822      0.242      0.752      0.453      -0.297       0.661\n",
       "topic_23      -0.0291      0.259     -0.112      0.911      -0.542       0.484\n",
       "topic_24      -0.3013      0.261     -1.154      0.251      -0.818       0.215\n",
       "topic_25      -0.0931      0.253     -0.368      0.714      -0.594       0.408\n",
       "topic_26      -0.1185      0.257     -0.461      0.646      -0.627       0.390\n",
       "topic_27      -0.2577      0.246     -1.046      0.298      -0.745       0.230\n",
       "topic_28       0.0242      0.270      0.090      0.929      -0.510       0.558\n",
       "topic_29       0.0997      0.274      0.364      0.717      -0.442       0.642\n",
       "topic_30      -0.1698      0.294     -0.578      0.564      -0.751       0.411\n",
       "topic_31      -0.6068      0.279     -2.177      0.031      -1.159      -0.055\n",
       "topic_32      -0.4184      0.253     -1.652      0.101      -0.920       0.083\n",
       "topic_33       0.5101      0.279      1.826      0.070      -0.043       1.063\n",
       "topic_34       0.1480      0.288      0.514      0.608      -0.422       0.718\n",
       "topic_35       0.3211      0.288      1.113      0.268      -0.250       0.892\n",
       "topic_36      -0.2471      0.271     -0.910      0.364      -0.784       0.290\n",
       "topic_37      -0.3751      0.285     -1.317      0.190      -0.939       0.189\n",
       "topic_38       0.3180      0.283      1.123      0.264      -0.242       0.878\n",
       "topic_39       0.6972      0.271      2.569      0.011       0.160       1.234\n",
       "topic_40      -0.1445      0.280     -0.516      0.607      -0.698       0.409\n",
       "topic_41       0.3633      0.273      1.331      0.185      -0.177       0.903\n",
       "topic_42       0.1379      0.290      0.475      0.635      -0.436       0.712\n",
       "topic_43      -0.1178      0.271     -0.434      0.665      -0.654       0.419\n",
       "topic_44       0.3195      0.292      1.093      0.276      -0.259       0.898\n",
       "topic_45      -0.1156      0.291     -0.397      0.692      -0.692       0.460\n",
       "topic_46       0.0604      0.285      0.212      0.833      -0.504       0.625\n",
       "topic_47      -0.1144      0.329     -0.348      0.728      -0.765       0.537\n",
       "topic_48       0.5982      0.318      1.883      0.062      -0.031       1.227\n",
       "topic_49      -0.5898      0.365     -1.618      0.108      -1.311       0.132\n",
       "topic_50      -0.0419      0.331     -0.126      0.900      -0.697       0.614\n",
       "topic_51       0.0740      0.318      0.233      0.816      -0.555       0.703\n",
       "topic_52       0.2083      0.316      0.659      0.511      -0.417       0.834\n",
       "topic_53       0.0455      0.328      0.139      0.890      -0.604       0.695\n",
       "topic_54       0.1826      0.381      0.480      0.632      -0.570       0.936\n",
       "topic_55       0.1811      0.310      0.584      0.561      -0.433       0.795\n",
       "topic_56      -0.0414      0.344     -0.120      0.904      -0.722       0.639\n",
       "topic_57      -0.4194      0.318     -1.321      0.189      -1.048       0.209\n",
       "topic_58      -0.0927      0.313     -0.296      0.768      -0.712       0.527\n",
       "topic_59      -0.5605      0.360     -1.555      0.122      -1.274       0.153\n",
       "topic_60       0.2432      0.312      0.779      0.437      -0.375       0.861\n",
       "topic_61       0.2789      0.345      0.808      0.421      -0.404       0.962\n",
       "topic_62      -0.3860      0.320     -1.208      0.229      -1.018       0.246\n",
       "topic_63       0.6079      0.366      1.660      0.099      -0.117       1.333\n",
       "topic_64      -0.2427      0.343     -0.707      0.481      -0.922       0.436\n",
       "topic_65      -0.2176      0.329     -0.662      0.509      -0.868       0.433\n",
       "topic_66       0.6018      0.349      1.727      0.087      -0.088       1.292\n",
       "topic_67      -0.1993      0.356     -0.560      0.576      -0.904       0.505\n",
       "topic_68      -0.2317      0.417     -0.556      0.579      -1.056       0.593\n",
       "topic_69       0.3759      0.357      1.052      0.295      -0.331       1.083\n",
       "topic_70      -0.0382      0.450     -0.085      0.932      -0.928       0.851\n",
       "topic_71       0.0605      0.387      0.156      0.876      -0.705       0.826\n",
       "topic_72      -0.4626      0.428     -1.082      0.281      -1.309       0.384\n",
       "topic_73      -0.3897      0.417     -0.934      0.352      -1.216       0.437\n",
       "==============================================================================\n",
       "Omnibus:                      113.919   Durbin-Watson:                   2.027\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              971.118\n",
       "Skew:                          -2.011   Prob(JB):                    1.33e-211\n",
       "Kurtosis:                      13.018   Cond. No.                         18.3\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(topics_bool_df.astype(float))\n",
    "old_reg = sm.OLS(\n",
    "    tar_a_norm_df.astype(float),\n",
    "    X,\n",
    "# ).fit(maxiter=1000, method='bfgs') # bfgs nm\n",
    ").fit(maxiter=1000) # bfgs nm\n",
    "old_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c111be43-8c35-4602-bc4c-b61153dc9779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with statistical significance with alpha of 0.1 (p-values less than this):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_4    -0.415300\n",
       "topic_14   -0.493928\n",
       "topic_16    0.539529\n",
       "topic_21   -0.552485\n",
       "topic_31   -0.606812\n",
       "topic_33    0.510122\n",
       "topic_39    0.697219\n",
       "topic_48    0.598212\n",
       "topic_63    0.607904\n",
       "topic_66    0.601778\n",
       "dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"variables with statistical significance with alpha of {p_alpha} (p-values less than this):\")\n",
    "significant_var_coeffs = old_reg.params[old_reg.pvalues < p_alpha]\n",
    "significant_var_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ebdbd717-d4d5-4c78-835e-19c49c0434a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with negative effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_4    -0.415300\n",
       "topic_14   -0.493928\n",
       "topic_16    0.539529\n",
       "topic_21   -0.552485\n",
       "topic_31   -0.606812\n",
       "topic_33    0.510122\n",
       "topic_39    0.697219\n",
       "topic_48    0.598212\n",
       "topic_63    0.607904\n",
       "topic_66    0.601778\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with negative effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs < 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1ed1ef6a-1ac1-459e-a1a6-60262845f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with positive effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with positive effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b1fc9-33e0-411b-bb1e-43b63125630a",
   "metadata": {},
   "source": [
    "#### predicting **major** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "43c0497e-44f5-4c16-98bd-6f95ae302d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>major_gpa</td>    <th>  R-squared:         </th> <td>   0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 15 Feb 2023</td> <th>  Prob (F-statistic):</th>  <td>0.0700</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:50:19</td>     <th>  Log-Likelihood:    </th> <td> -224.56</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   599.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   125</td>      <th>  BIC:               </th> <td>   846.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    74</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    0.3366</td> <td>    0.390</td> <td>    0.864</td> <td> 0.389</td> <td>   -0.435</td> <td>    1.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_0</th>  <td>    0.1710</td> <td>    0.413</td> <td>    0.414</td> <td> 0.680</td> <td>   -0.646</td> <td>    0.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_1</th>  <td>   -0.1680</td> <td>    0.172</td> <td>   -0.979</td> <td> 0.330</td> <td>   -0.508</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_2</th>  <td>   -0.1396</td> <td>    0.183</td> <td>   -0.765</td> <td> 0.446</td> <td>   -0.501</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_3</th>  <td>    0.1605</td> <td>    0.191</td> <td>    0.843</td> <td> 0.401</td> <td>   -0.217</td> <td>    0.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_4</th>  <td>   -0.2006</td> <td>    0.183</td> <td>   -1.097</td> <td> 0.275</td> <td>   -0.563</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_5</th>  <td>   -0.4366</td> <td>    0.185</td> <td>   -2.363</td> <td> 0.020</td> <td>   -0.802</td> <td>   -0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_6</th>  <td>   -0.4024</td> <td>    0.210</td> <td>   -1.915</td> <td> 0.058</td> <td>   -0.818</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_7</th>  <td>    0.0622</td> <td>    0.205</td> <td>    0.303</td> <td> 0.762</td> <td>   -0.344</td> <td>    0.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_8</th>  <td>    0.4147</td> <td>    0.199</td> <td>    2.084</td> <td> 0.039</td> <td>    0.021</td> <td>    0.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_9</th>  <td>   -0.3235</td> <td>    0.199</td> <td>   -1.623</td> <td> 0.107</td> <td>   -0.718</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_10</th> <td>   -0.1497</td> <td>    0.222</td> <td>   -0.674</td> <td> 0.501</td> <td>   -0.589</td> <td>    0.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_11</th> <td>   -0.2756</td> <td>    0.209</td> <td>   -1.318</td> <td> 0.190</td> <td>   -0.690</td> <td>    0.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_12</th> <td>   -0.0800</td> <td>    0.216</td> <td>   -0.371</td> <td> 0.711</td> <td>   -0.507</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_13</th> <td>    0.2448</td> <td>    0.218</td> <td>    1.124</td> <td> 0.263</td> <td>   -0.186</td> <td>    0.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_14</th> <td>   -0.7228</td> <td>    0.212</td> <td>   -3.402</td> <td> 0.001</td> <td>   -1.143</td> <td>   -0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_15</th> <td>    0.3709</td> <td>    0.220</td> <td>    1.684</td> <td> 0.095</td> <td>   -0.065</td> <td>    0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_16</th> <td>    0.4681</td> <td>    0.241</td> <td>    1.941</td> <td> 0.055</td> <td>   -0.009</td> <td>    0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_17</th> <td>    0.4468</td> <td>    0.237</td> <td>    1.887</td> <td> 0.062</td> <td>   -0.022</td> <td>    0.915</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_18</th> <td>   -0.2270</td> <td>    0.215</td> <td>   -1.055</td> <td> 0.294</td> <td>   -0.653</td> <td>    0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_19</th> <td>    0.0259</td> <td>    0.233</td> <td>    0.111</td> <td> 0.912</td> <td>   -0.436</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_20</th> <td>   -0.1904</td> <td>    0.230</td> <td>   -0.828</td> <td> 0.409</td> <td>   -0.645</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_21</th> <td>   -0.6382</td> <td>    0.237</td> <td>   -2.695</td> <td> 0.008</td> <td>   -1.107</td> <td>   -0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_22</th> <td>    0.2439</td> <td>    0.228</td> <td>    1.071</td> <td> 0.286</td> <td>   -0.207</td> <td>    0.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_23</th> <td>    0.2064</td> <td>    0.244</td> <td>    0.846</td> <td> 0.399</td> <td>   -0.276</td> <td>    0.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_24</th> <td>   -0.2203</td> <td>    0.245</td> <td>   -0.897</td> <td> 0.371</td> <td>   -0.706</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_25</th> <td>   -0.2756</td> <td>    0.238</td> <td>   -1.157</td> <td> 0.249</td> <td>   -0.747</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_26</th> <td>   -0.0975</td> <td>    0.242</td> <td>   -0.403</td> <td> 0.688</td> <td>   -0.576</td> <td>    0.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_27</th> <td>   -0.2372</td> <td>    0.232</td> <td>   -1.024</td> <td> 0.308</td> <td>   -0.696</td> <td>    0.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_28</th> <td>   -0.2687</td> <td>    0.254</td> <td>   -1.059</td> <td> 0.292</td> <td>   -0.771</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_29</th> <td>    0.0606</td> <td>    0.258</td> <td>    0.235</td> <td> 0.814</td> <td>   -0.449</td> <td>    0.570</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_30</th> <td>   -0.3855</td> <td>    0.276</td> <td>   -1.396</td> <td> 0.165</td> <td>   -0.932</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_31</th> <td>   -0.7424</td> <td>    0.262</td> <td>   -2.831</td> <td> 0.005</td> <td>   -1.261</td> <td>   -0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_32</th> <td>   -0.1468</td> <td>    0.238</td> <td>   -0.616</td> <td> 0.539</td> <td>   -0.618</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_33</th> <td>    0.1632</td> <td>    0.263</td> <td>    0.621</td> <td> 0.536</td> <td>   -0.357</td> <td>    0.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_34</th> <td>    0.0557</td> <td>    0.271</td> <td>    0.206</td> <td> 0.837</td> <td>   -0.480</td> <td>    0.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_35</th> <td>    0.2968</td> <td>    0.271</td> <td>    1.094</td> <td> 0.276</td> <td>   -0.240</td> <td>    0.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_36</th> <td>    0.3014</td> <td>    0.255</td> <td>    1.181</td> <td> 0.240</td> <td>   -0.204</td> <td>    0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_37</th> <td>   -0.4382</td> <td>    0.268</td> <td>   -1.636</td> <td> 0.104</td> <td>   -0.968</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_38</th> <td>    0.3849</td> <td>    0.266</td> <td>    1.445</td> <td> 0.151</td> <td>   -0.142</td> <td>    0.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_39</th> <td>    0.2363</td> <td>    0.255</td> <td>    0.926</td> <td> 0.356</td> <td>   -0.269</td> <td>    0.741</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_40</th> <td>   -0.1046</td> <td>    0.263</td> <td>   -0.398</td> <td> 0.692</td> <td>   -0.625</td> <td>    0.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_41</th> <td>    0.2207</td> <td>    0.257</td> <td>    0.860</td> <td> 0.391</td> <td>   -0.287</td> <td>    0.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_42</th> <td>    0.2570</td> <td>    0.273</td> <td>    0.942</td> <td> 0.348</td> <td>   -0.283</td> <td>    0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_43</th> <td>   -0.2366</td> <td>    0.255</td> <td>   -0.928</td> <td> 0.355</td> <td>   -0.741</td> <td>    0.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_44</th> <td>    0.1741</td> <td>    0.275</td> <td>    0.633</td> <td> 0.528</td> <td>   -0.370</td> <td>    0.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_45</th> <td>   -0.0242</td> <td>    0.274</td> <td>   -0.088</td> <td> 0.930</td> <td>   -0.566</td> <td>    0.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_46</th> <td>    0.1821</td> <td>    0.268</td> <td>    0.679</td> <td> 0.499</td> <td>   -0.349</td> <td>    0.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_47</th> <td>   -0.4172</td> <td>    0.309</td> <td>   -1.349</td> <td> 0.180</td> <td>   -1.029</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_48</th> <td>    0.2998</td> <td>    0.299</td> <td>    1.003</td> <td> 0.318</td> <td>   -0.292</td> <td>    0.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_49</th> <td>   -0.0650</td> <td>    0.343</td> <td>   -0.190</td> <td> 0.850</td> <td>   -0.744</td> <td>    0.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_50</th> <td>    0.0996</td> <td>    0.311</td> <td>    0.320</td> <td> 0.750</td> <td>   -0.517</td> <td>    0.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_51</th> <td>    0.0658</td> <td>    0.299</td> <td>    0.220</td> <td> 0.826</td> <td>   -0.526</td> <td>    0.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_52</th> <td>    0.1330</td> <td>    0.297</td> <td>    0.447</td> <td> 0.656</td> <td>   -0.456</td> <td>    0.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_53</th> <td>   -0.0299</td> <td>    0.309</td> <td>   -0.097</td> <td> 0.923</td> <td>   -0.641</td> <td>    0.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_54</th> <td>    0.1844</td> <td>    0.358</td> <td>    0.515</td> <td> 0.607</td> <td>   -0.524</td> <td>    0.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_55</th> <td>    0.2506</td> <td>    0.292</td> <td>    0.859</td> <td> 0.392</td> <td>   -0.327</td> <td>    0.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_56</th> <td>   -0.1633</td> <td>    0.323</td> <td>   -0.505</td> <td> 0.615</td> <td>   -0.803</td> <td>    0.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_57</th> <td>    0.0461</td> <td>    0.299</td> <td>    0.154</td> <td> 0.878</td> <td>   -0.545</td> <td>    0.637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_58</th> <td>    0.0958</td> <td>    0.295</td> <td>    0.325</td> <td> 0.746</td> <td>   -0.487</td> <td>    0.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_59</th> <td>   -0.2223</td> <td>    0.339</td> <td>   -0.656</td> <td> 0.513</td> <td>   -0.893</td> <td>    0.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_60</th> <td>    0.1304</td> <td>    0.294</td> <td>    0.444</td> <td> 0.658</td> <td>   -0.451</td> <td>    0.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_61</th> <td>    0.6447</td> <td>    0.325</td> <td>    1.986</td> <td> 0.049</td> <td>    0.002</td> <td>    1.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_62</th> <td>   -0.6701</td> <td>    0.301</td> <td>   -2.230</td> <td> 0.028</td> <td>   -1.265</td> <td>   -0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_63</th> <td>    0.5960</td> <td>    0.344</td> <td>    1.731</td> <td> 0.086</td> <td>   -0.085</td> <td>    1.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_64</th> <td>   -0.1025</td> <td>    0.323</td> <td>   -0.318</td> <td> 0.751</td> <td>   -0.741</td> <td>    0.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_65</th> <td>   -0.1551</td> <td>    0.309</td> <td>   -0.502</td> <td> 0.617</td> <td>   -0.767</td> <td>    0.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_66</th> <td>    0.6313</td> <td>    0.328</td> <td>    1.926</td> <td> 0.056</td> <td>   -0.017</td> <td>    1.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_67</th> <td>    0.0730</td> <td>    0.335</td> <td>    0.218</td> <td> 0.828</td> <td>   -0.589</td> <td>    0.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_68</th> <td>   -0.3080</td> <td>    0.392</td> <td>   -0.786</td> <td> 0.433</td> <td>   -1.083</td> <td>    0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_69</th> <td>    0.2839</td> <td>    0.336</td> <td>    0.845</td> <td> 0.400</td> <td>   -0.381</td> <td>    0.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_70</th> <td>   -0.0914</td> <td>    0.423</td> <td>   -0.216</td> <td> 0.829</td> <td>   -0.928</td> <td>    0.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_71</th> <td>   -0.0641</td> <td>    0.364</td> <td>   -0.176</td> <td> 0.860</td> <td>   -0.784</td> <td>    0.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_72</th> <td>   -0.3728</td> <td>    0.402</td> <td>   -0.927</td> <td> 0.356</td> <td>   -1.169</td> <td>    0.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_73</th> <td>   -0.0622</td> <td>    0.393</td> <td>   -0.158</td> <td> 0.874</td> <td>   -0.839</td> <td>    0.715</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>96.030</td> <th>  Durbin-Watson:     </th> <td>   1.877</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 588.629</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.736</td> <th>  Prob(JB):          </th> <td>1.52e-128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>10.653</td> <th>  Cond. No.          </th> <td>    18.3</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              major_gpa   R-squared:                       0.444\n",
       "Model:                            OLS   Adj. R-squared:                  0.115\n",
       "Method:                 Least Squares   F-statistic:                     1.350\n",
       "Date:                Wed, 15 Feb 2023   Prob (F-statistic):             0.0700\n",
       "Time:                        13:50:19   Log-Likelihood:                -224.56\n",
       "No. Observations:                 200   AIC:                             599.1\n",
       "Df Residuals:                     125   BIC:                             846.5\n",
       "Df Model:                          74                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3366      0.390      0.864      0.389      -0.435       1.108\n",
       "topic_0        0.1710      0.413      0.414      0.680      -0.646       0.988\n",
       "topic_1       -0.1680      0.172     -0.979      0.330      -0.508       0.172\n",
       "topic_2       -0.1396      0.183     -0.765      0.446      -0.501       0.222\n",
       "topic_3        0.1605      0.191      0.843      0.401      -0.217       0.538\n",
       "topic_4       -0.2006      0.183     -1.097      0.275      -0.563       0.161\n",
       "topic_5       -0.4366      0.185     -2.363      0.020      -0.802      -0.071\n",
       "topic_6       -0.4024      0.210     -1.915      0.058      -0.818       0.014\n",
       "topic_7        0.0622      0.205      0.303      0.762      -0.344       0.469\n",
       "topic_8        0.4147      0.199      2.084      0.039       0.021       0.808\n",
       "topic_9       -0.3235      0.199     -1.623      0.107      -0.718       0.071\n",
       "topic_10      -0.1497      0.222     -0.674      0.501      -0.589       0.290\n",
       "topic_11      -0.2756      0.209     -1.318      0.190      -0.690       0.138\n",
       "topic_12      -0.0800      0.216     -0.371      0.711      -0.507       0.347\n",
       "topic_13       0.2448      0.218      1.124      0.263      -0.186       0.676\n",
       "topic_14      -0.7228      0.212     -3.402      0.001      -1.143      -0.302\n",
       "topic_15       0.3709      0.220      1.684      0.095      -0.065       0.807\n",
       "topic_16       0.4681      0.241      1.941      0.055      -0.009       0.946\n",
       "topic_17       0.4468      0.237      1.887      0.062      -0.022       0.915\n",
       "topic_18      -0.2270      0.215     -1.055      0.294      -0.653       0.199\n",
       "topic_19       0.0259      0.233      0.111      0.912      -0.436       0.488\n",
       "topic_20      -0.1904      0.230     -0.828      0.409      -0.645       0.265\n",
       "topic_21      -0.6382      0.237     -2.695      0.008      -1.107      -0.170\n",
       "topic_22       0.2439      0.228      1.071      0.286      -0.207       0.695\n",
       "topic_23       0.2064      0.244      0.846      0.399      -0.276       0.689\n",
       "topic_24      -0.2203      0.245     -0.897      0.371      -0.706       0.266\n",
       "topic_25      -0.2756      0.238     -1.157      0.249      -0.747       0.196\n",
       "topic_26      -0.0975      0.242     -0.403      0.688      -0.576       0.381\n",
       "topic_27      -0.2372      0.232     -1.024      0.308      -0.696       0.221\n",
       "topic_28      -0.2687      0.254     -1.059      0.292      -0.771       0.233\n",
       "topic_29       0.0606      0.258      0.235      0.814      -0.449       0.570\n",
       "topic_30      -0.3855      0.276     -1.396      0.165      -0.932       0.161\n",
       "topic_31      -0.7424      0.262     -2.831      0.005      -1.261      -0.223\n",
       "topic_32      -0.1468      0.238     -0.616      0.539      -0.618       0.325\n",
       "topic_33       0.1632      0.263      0.621      0.536      -0.357       0.683\n",
       "topic_34       0.0557      0.271      0.206      0.837      -0.480       0.591\n",
       "topic_35       0.2968      0.271      1.094      0.276      -0.240       0.834\n",
       "topic_36       0.3014      0.255      1.181      0.240      -0.204       0.807\n",
       "topic_37      -0.4382      0.268     -1.636      0.104      -0.968       0.092\n",
       "topic_38       0.3849      0.266      1.445      0.151      -0.142       0.912\n",
       "topic_39       0.2363      0.255      0.926      0.356      -0.269       0.741\n",
       "topic_40      -0.1046      0.263     -0.398      0.692      -0.625       0.416\n",
       "topic_41       0.2207      0.257      0.860      0.391      -0.287       0.729\n",
       "topic_42       0.2570      0.273      0.942      0.348      -0.283       0.797\n",
       "topic_43      -0.2366      0.255     -0.928      0.355      -0.741       0.268\n",
       "topic_44       0.1741      0.275      0.633      0.528      -0.370       0.718\n",
       "topic_45      -0.0242      0.274     -0.088      0.930      -0.566       0.518\n",
       "topic_46       0.1821      0.268      0.679      0.499      -0.349       0.713\n",
       "topic_47      -0.4172      0.309     -1.349      0.180      -1.029       0.195\n",
       "topic_48       0.2998      0.299      1.003      0.318      -0.292       0.891\n",
       "topic_49      -0.0650      0.343     -0.190      0.850      -0.744       0.614\n",
       "topic_50       0.0996      0.311      0.320      0.750      -0.517       0.716\n",
       "topic_51       0.0658      0.299      0.220      0.826      -0.526       0.658\n",
       "topic_52       0.1330      0.297      0.447      0.656      -0.456       0.721\n",
       "topic_53      -0.0299      0.309     -0.097      0.923      -0.641       0.581\n",
       "topic_54       0.1844      0.358      0.515      0.607      -0.524       0.893\n",
       "topic_55       0.2506      0.292      0.859      0.392      -0.327       0.828\n",
       "topic_56      -0.1633      0.323     -0.505      0.615      -0.803       0.477\n",
       "topic_57       0.0461      0.299      0.154      0.878      -0.545       0.637\n",
       "topic_58       0.0958      0.295      0.325      0.746      -0.487       0.679\n",
       "topic_59      -0.2223      0.339     -0.656      0.513      -0.893       0.449\n",
       "topic_60       0.1304      0.294      0.444      0.658      -0.451       0.712\n",
       "topic_61       0.6447      0.325      1.986      0.049       0.002       1.287\n",
       "topic_62      -0.6701      0.301     -2.230      0.028      -1.265      -0.075\n",
       "topic_63       0.5960      0.344      1.731      0.086      -0.085       1.277\n",
       "topic_64      -0.1025      0.323     -0.318      0.751      -0.741       0.536\n",
       "topic_65      -0.1551      0.309     -0.502      0.617      -0.767       0.457\n",
       "topic_66       0.6313      0.328      1.926      0.056      -0.017       1.280\n",
       "topic_67       0.0730      0.335      0.218      0.828      -0.589       0.735\n",
       "topic_68      -0.3080      0.392     -0.786      0.433      -1.083       0.467\n",
       "topic_69       0.2839      0.336      0.845      0.400      -0.381       0.949\n",
       "topic_70      -0.0914      0.423     -0.216      0.829      -0.928       0.745\n",
       "topic_71      -0.0641      0.364     -0.176      0.860      -0.784       0.655\n",
       "topic_72      -0.3728      0.402     -0.927      0.356      -1.169       0.423\n",
       "topic_73      -0.0622      0.393     -0.158      0.874      -0.839       0.715\n",
       "==============================================================================\n",
       "Omnibus:                       96.030   Durbin-Watson:                   1.877\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              588.629\n",
       "Skew:                          -1.736   Prob(JB):                    1.52e-128\n",
       "Kurtosis:                      10.653   Cond. No.                         18.3\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(topics_bool_df.astype(float))\n",
    "old_reg = sm.OLS(\n",
    "    tar_b_norm_df.astype(float),\n",
    "    X,\n",
    "# ).fit(maxiter=1000, method='bfgs') # bfgs nm\n",
    ").fit(maxiter=1000) # bfgs nm\n",
    "old_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "54d38d78-16bb-4f77-8963-cc14ffa5d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with statistical significance with alpha of 0.1 (p-values less than this):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_5    -0.436598\n",
       "topic_6    -0.402437\n",
       "topic_8     0.414679\n",
       "topic_14   -0.722829\n",
       "topic_15    0.370927\n",
       "topic_16    0.468143\n",
       "topic_17    0.446799\n",
       "topic_21   -0.638247\n",
       "topic_31   -0.742375\n",
       "topic_61    0.644733\n",
       "topic_62   -0.670150\n",
       "topic_63    0.595991\n",
       "topic_66    0.631340\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"variables with statistical significance with alpha of {p_alpha} (p-values less than this):\")\n",
    "significant_var_coeffs = old_reg.params[old_reg.pvalues < p_alpha]\n",
    "significant_var_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "abd29766-8d15-4806-b277-c14bda0e8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with negative effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_5    -0.436598\n",
       "topic_6    -0.402437\n",
       "topic_8     0.414679\n",
       "topic_14   -0.722829\n",
       "topic_15    0.370927\n",
       "topic_16    0.468143\n",
       "topic_17    0.446799\n",
       "topic_21   -0.638247\n",
       "topic_31   -0.742375\n",
       "topic_61    0.644733\n",
       "topic_62   -0.670150\n",
       "topic_63    0.595991\n",
       "topic_66    0.631340\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with negative effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs < 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1ded539c-f66c-4180-accb-c95c9861582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with positive effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with positive effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7d9af-e3d3-4dc4-819e-4a4267e072dd",
   "metadata": {},
   "source": [
    "### predicting *raw* GPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e1bc2-9887-4f82-b497-afe7a6541912",
   "metadata": {},
   "source": [
    "#### predicting **overall** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "280bb79f-d872-43e4-a739-3f41a5bab61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>Overall GPA (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")</td> <th>  R-squared:         </th> <td>   0.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                                                      <td>OLS</td>                                           <th>  Adj. R-squared:    </th> <td>  -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                                                <td>Least Squares</td>                                      <th>  F-statistic:       </th> <td>  0.9985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                                                <td>Wed, 15 Feb 2023</td>                                     <th>  Prob (F-statistic):</th>  <td> 0.496</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                                                    <td>13:50:19</td>                                         <th>  Log-Likelihood:    </th> <td> -90.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>                                         <td>   200</td>                                          <th>  AIC:               </th> <td>   331.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>                                             <td>   125</td>                                          <th>  BIC:               </th> <td>   578.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                                                 <td>    74</td>                                          <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>                                         <td>nonrobust</td>                                        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    3.7864</td> <td>    0.199</td> <td>   18.984</td> <td> 0.000</td> <td>    3.392</td> <td>    4.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_0</th>  <td>   -0.0764</td> <td>    0.211</td> <td>   -0.361</td> <td> 0.719</td> <td>   -0.495</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_1</th>  <td>   -0.0355</td> <td>    0.088</td> <td>   -0.404</td> <td> 0.687</td> <td>   -0.209</td> <td>    0.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_2</th>  <td>    0.1514</td> <td>    0.093</td> <td>    1.620</td> <td> 0.108</td> <td>   -0.034</td> <td>    0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_3</th>  <td>   -0.0164</td> <td>    0.098</td> <td>   -0.168</td> <td> 0.867</td> <td>   -0.209</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_4</th>  <td>   -0.1999</td> <td>    0.094</td> <td>   -2.135</td> <td> 0.035</td> <td>   -0.385</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_5</th>  <td>   -0.0315</td> <td>    0.095</td> <td>   -0.333</td> <td> 0.740</td> <td>   -0.219</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_6</th>  <td>   -0.1152</td> <td>    0.108</td> <td>   -1.071</td> <td> 0.286</td> <td>   -0.328</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_7</th>  <td>    0.0581</td> <td>    0.105</td> <td>    0.553</td> <td> 0.581</td> <td>   -0.150</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_8</th>  <td>    0.0877</td> <td>    0.102</td> <td>    0.862</td> <td> 0.391</td> <td>   -0.114</td> <td>    0.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_9</th>  <td>   -0.0320</td> <td>    0.102</td> <td>   -0.314</td> <td> 0.754</td> <td>   -0.234</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_10</th> <td>   -0.0726</td> <td>    0.114</td> <td>   -0.639</td> <td> 0.524</td> <td>   -0.298</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_11</th> <td>    0.0254</td> <td>    0.107</td> <td>    0.237</td> <td> 0.813</td> <td>   -0.186</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_12</th> <td>   -0.1543</td> <td>    0.110</td> <td>   -1.397</td> <td> 0.165</td> <td>   -0.373</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_13</th> <td>    0.1074</td> <td>    0.111</td> <td>    0.964</td> <td> 0.337</td> <td>   -0.113</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_14</th> <td>   -0.2378</td> <td>    0.109</td> <td>   -2.186</td> <td> 0.031</td> <td>   -0.453</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_15</th> <td>   -0.0277</td> <td>    0.113</td> <td>   -0.246</td> <td> 0.806</td> <td>   -0.251</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_16</th> <td>    0.2597</td> <td>    0.123</td> <td>    2.103</td> <td> 0.037</td> <td>    0.015</td> <td>    0.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_17</th> <td>    0.1851</td> <td>    0.121</td> <td>    1.527</td> <td> 0.129</td> <td>   -0.055</td> <td>    0.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_18</th> <td>   -0.1606</td> <td>    0.110</td> <td>   -1.458</td> <td> 0.147</td> <td>   -0.379</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_19</th> <td>    0.0648</td> <td>    0.120</td> <td>    0.542</td> <td> 0.589</td> <td>   -0.172</td> <td>    0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_20</th> <td>   -0.1529</td> <td>    0.118</td> <td>   -1.299</td> <td> 0.196</td> <td>   -0.386</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_21</th> <td>   -0.2659</td> <td>    0.121</td> <td>   -2.194</td> <td> 0.030</td> <td>   -0.506</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_22</th> <td>    0.0877</td> <td>    0.117</td> <td>    0.752</td> <td> 0.453</td> <td>   -0.143</td> <td>    0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_23</th> <td>   -0.0140</td> <td>    0.125</td> <td>   -0.112</td> <td> 0.911</td> <td>   -0.261</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_24</th> <td>   -0.1450</td> <td>    0.126</td> <td>   -1.154</td> <td> 0.251</td> <td>   -0.394</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_25</th> <td>   -0.0448</td> <td>    0.122</td> <td>   -0.368</td> <td> 0.714</td> <td>   -0.286</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_26</th> <td>   -0.0570</td> <td>    0.124</td> <td>   -0.461</td> <td> 0.646</td> <td>   -0.302</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_27</th> <td>   -0.1241</td> <td>    0.119</td> <td>   -1.046</td> <td> 0.298</td> <td>   -0.359</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_28</th> <td>    0.0116</td> <td>    0.130</td> <td>    0.090</td> <td> 0.929</td> <td>   -0.245</td> <td>    0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_29</th> <td>    0.0480</td> <td>    0.132</td> <td>    0.364</td> <td> 0.717</td> <td>   -0.213</td> <td>    0.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_30</th> <td>   -0.0817</td> <td>    0.141</td> <td>   -0.578</td> <td> 0.564</td> <td>   -0.361</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_31</th> <td>   -0.2921</td> <td>    0.134</td> <td>   -2.177</td> <td> 0.031</td> <td>   -0.558</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_32</th> <td>   -0.2014</td> <td>    0.122</td> <td>   -1.652</td> <td> 0.101</td> <td>   -0.443</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_33</th> <td>    0.2456</td> <td>    0.134</td> <td>    1.826</td> <td> 0.070</td> <td>   -0.021</td> <td>    0.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_34</th> <td>    0.0712</td> <td>    0.139</td> <td>    0.514</td> <td> 0.608</td> <td>   -0.203</td> <td>    0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_35</th> <td>    0.1546</td> <td>    0.139</td> <td>    1.113</td> <td> 0.268</td> <td>   -0.120</td> <td>    0.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_36</th> <td>   -0.1189</td> <td>    0.131</td> <td>   -0.910</td> <td> 0.364</td> <td>   -0.378</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_37</th> <td>   -0.1806</td> <td>    0.137</td> <td>   -1.317</td> <td> 0.190</td> <td>   -0.452</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_38</th> <td>    0.1531</td> <td>    0.136</td> <td>    1.123</td> <td> 0.264</td> <td>   -0.117</td> <td>    0.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_39</th> <td>    0.3356</td> <td>    0.131</td> <td>    2.569</td> <td> 0.011</td> <td>    0.077</td> <td>    0.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_40</th> <td>   -0.0695</td> <td>    0.135</td> <td>   -0.516</td> <td> 0.607</td> <td>   -0.336</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_41</th> <td>    0.1749</td> <td>    0.131</td> <td>    1.331</td> <td> 0.185</td> <td>   -0.085</td> <td>    0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_42</th> <td>    0.0664</td> <td>    0.140</td> <td>    0.475</td> <td> 0.635</td> <td>   -0.210</td> <td>    0.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_43</th> <td>   -0.0567</td> <td>    0.131</td> <td>   -0.434</td> <td> 0.665</td> <td>   -0.315</td> <td>    0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_44</th> <td>    0.1538</td> <td>    0.141</td> <td>    1.093</td> <td> 0.276</td> <td>   -0.125</td> <td>    0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_45</th> <td>   -0.0557</td> <td>    0.140</td> <td>   -0.397</td> <td> 0.692</td> <td>   -0.333</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_46</th> <td>    0.0291</td> <td>    0.137</td> <td>    0.212</td> <td> 0.833</td> <td>   -0.243</td> <td>    0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_47</th> <td>   -0.0551</td> <td>    0.158</td> <td>   -0.348</td> <td> 0.728</td> <td>   -0.368</td> <td>    0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_48</th> <td>    0.2880</td> <td>    0.153</td> <td>    1.883</td> <td> 0.062</td> <td>   -0.015</td> <td>    0.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_49</th> <td>   -0.2839</td> <td>    0.175</td> <td>   -1.618</td> <td> 0.108</td> <td>   -0.631</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_50</th> <td>   -0.0201</td> <td>    0.159</td> <td>   -0.126</td> <td> 0.900</td> <td>   -0.336</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_51</th> <td>    0.0356</td> <td>    0.153</td> <td>    0.233</td> <td> 0.816</td> <td>   -0.267</td> <td>    0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_52</th> <td>    0.1003</td> <td>    0.152</td> <td>    0.659</td> <td> 0.511</td> <td>   -0.201</td> <td>    0.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_53</th> <td>    0.0219</td> <td>    0.158</td> <td>    0.139</td> <td> 0.890</td> <td>   -0.291</td> <td>    0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_54</th> <td>    0.0879</td> <td>    0.183</td> <td>    0.480</td> <td> 0.632</td> <td>   -0.275</td> <td>    0.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_55</th> <td>    0.0872</td> <td>    0.149</td> <td>    0.584</td> <td> 0.561</td> <td>   -0.208</td> <td>    0.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_56</th> <td>   -0.0199</td> <td>    0.166</td> <td>   -0.120</td> <td> 0.904</td> <td>   -0.348</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_57</th> <td>   -0.2019</td> <td>    0.153</td> <td>   -1.321</td> <td> 0.189</td> <td>   -0.504</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_58</th> <td>   -0.0446</td> <td>    0.151</td> <td>   -0.296</td> <td> 0.768</td> <td>   -0.343</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_59</th> <td>   -0.2698</td> <td>    0.174</td> <td>   -1.555</td> <td> 0.122</td> <td>   -0.613</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_60</th> <td>    0.1171</td> <td>    0.150</td> <td>    0.779</td> <td> 0.437</td> <td>   -0.180</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_61</th> <td>    0.1343</td> <td>    0.166</td> <td>    0.808</td> <td> 0.421</td> <td>   -0.195</td> <td>    0.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_62</th> <td>   -0.1858</td> <td>    0.154</td> <td>   -1.208</td> <td> 0.229</td> <td>   -0.490</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_63</th> <td>    0.2926</td> <td>    0.176</td> <td>    1.660</td> <td> 0.099</td> <td>   -0.056</td> <td>    0.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_64</th> <td>   -0.1168</td> <td>    0.165</td> <td>   -0.707</td> <td> 0.481</td> <td>   -0.444</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_65</th> <td>   -0.1047</td> <td>    0.158</td> <td>   -0.662</td> <td> 0.509</td> <td>   -0.418</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_66</th> <td>    0.2897</td> <td>    0.168</td> <td>    1.727</td> <td> 0.087</td> <td>   -0.042</td> <td>    0.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_67</th> <td>   -0.0959</td> <td>    0.171</td> <td>   -0.560</td> <td> 0.576</td> <td>   -0.435</td> <td>    0.243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_68</th> <td>   -0.1115</td> <td>    0.201</td> <td>   -0.556</td> <td> 0.579</td> <td>   -0.508</td> <td>    0.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_69</th> <td>    0.1810</td> <td>    0.172</td> <td>    1.052</td> <td> 0.295</td> <td>   -0.159</td> <td>    0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_70</th> <td>   -0.0184</td> <td>    0.216</td> <td>   -0.085</td> <td> 0.932</td> <td>   -0.447</td> <td>    0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_71</th> <td>    0.0291</td> <td>    0.186</td> <td>    0.156</td> <td> 0.876</td> <td>   -0.339</td> <td>    0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_72</th> <td>   -0.2227</td> <td>    0.206</td> <td>   -1.082</td> <td> 0.281</td> <td>   -0.630</td> <td>    0.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_73</th> <td>   -0.1876</td> <td>    0.201</td> <td>   -0.934</td> <td> 0.352</td> <td>   -0.585</td> <td>    0.210</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>113.919</td> <th>  Durbin-Watson:     </th> <td>   2.027</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 971.118</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-2.011</td>  <th>  Prob(JB):          </th> <td>1.33e-211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>13.018</td>  <th>  Cond. No.          </th> <td>    18.3</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                                               OLS Regression Results                                                              \n",
       "===================================================================================================================================================\n",
       "Dep. Variable:     Overall GPA (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")   R-squared:                       0.372\n",
       "Model:                                                                                                 OLS   Adj. R-squared:                 -0.001\n",
       "Method:                                                                                      Least Squares   F-statistic:                    0.9985\n",
       "Date:                                                                                     Wed, 15 Feb 2023   Prob (F-statistic):              0.496\n",
       "Time:                                                                                             13:50:19   Log-Likelihood:                -90.618\n",
       "No. Observations:                                                                                      200   AIC:                             331.2\n",
       "Df Residuals:                                                                                          125   BIC:                             578.6\n",
       "Df Model:                                                                                               74                                         \n",
       "Covariance Type:                                                                                 nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.7864      0.199     18.984      0.000       3.392       4.181\n",
       "topic_0       -0.0764      0.211     -0.361      0.719      -0.495       0.342\n",
       "topic_1       -0.0355      0.088     -0.404      0.687      -0.209       0.138\n",
       "topic_2        0.1514      0.093      1.620      0.108      -0.034       0.336\n",
       "topic_3       -0.0164      0.098     -0.168      0.867      -0.209       0.177\n",
       "topic_4       -0.1999      0.094     -2.135      0.035      -0.385      -0.015\n",
       "topic_5       -0.0315      0.095     -0.333      0.740      -0.219       0.156\n",
       "topic_6       -0.1152      0.108     -1.071      0.286      -0.328       0.098\n",
       "topic_7        0.0581      0.105      0.553      0.581      -0.150       0.266\n",
       "topic_8        0.0877      0.102      0.862      0.391      -0.114       0.289\n",
       "topic_9       -0.0320      0.102     -0.314      0.754      -0.234       0.170\n",
       "topic_10      -0.0726      0.114     -0.639      0.524      -0.298       0.152\n",
       "topic_11       0.0254      0.107      0.237      0.813      -0.186       0.237\n",
       "topic_12      -0.1543      0.110     -1.397      0.165      -0.373       0.064\n",
       "topic_13       0.1074      0.111      0.964      0.337      -0.113       0.328\n",
       "topic_14      -0.2378      0.109     -2.186      0.031      -0.453      -0.023\n",
       "topic_15      -0.0277      0.113     -0.246      0.806      -0.251       0.195\n",
       "topic_16       0.2597      0.123      2.103      0.037       0.015       0.504\n",
       "topic_17       0.1851      0.121      1.527      0.129      -0.055       0.425\n",
       "topic_18      -0.1606      0.110     -1.458      0.147      -0.379       0.057\n",
       "topic_19       0.0648      0.120      0.542      0.589      -0.172       0.301\n",
       "topic_20      -0.1529      0.118     -1.299      0.196      -0.386       0.080\n",
       "topic_21      -0.2659      0.121     -2.194      0.030      -0.506      -0.026\n",
       "topic_22       0.0877      0.117      0.752      0.453      -0.143       0.318\n",
       "topic_23      -0.0140      0.125     -0.112      0.911      -0.261       0.233\n",
       "topic_24      -0.1450      0.126     -1.154      0.251      -0.394       0.104\n",
       "topic_25      -0.0448      0.122     -0.368      0.714      -0.286       0.196\n",
       "topic_26      -0.0570      0.124     -0.461      0.646      -0.302       0.188\n",
       "topic_27      -0.1241      0.119     -1.046      0.298      -0.359       0.111\n",
       "topic_28       0.0116      0.130      0.090      0.929      -0.245       0.269\n",
       "topic_29       0.0480      0.132      0.364      0.717      -0.213       0.309\n",
       "topic_30      -0.0817      0.141     -0.578      0.564      -0.361       0.198\n",
       "topic_31      -0.2921      0.134     -2.177      0.031      -0.558      -0.027\n",
       "topic_32      -0.2014      0.122     -1.652      0.101      -0.443       0.040\n",
       "topic_33       0.2456      0.134      1.826      0.070      -0.021       0.512\n",
       "topic_34       0.0712      0.139      0.514      0.608      -0.203       0.345\n",
       "topic_35       0.1546      0.139      1.113      0.268      -0.120       0.429\n",
       "topic_36      -0.1189      0.131     -0.910      0.364      -0.378       0.140\n",
       "topic_37      -0.1806      0.137     -1.317      0.190      -0.452       0.091\n",
       "topic_38       0.1531      0.136      1.123      0.264      -0.117       0.423\n",
       "topic_39       0.3356      0.131      2.569      0.011       0.077       0.594\n",
       "topic_40      -0.0695      0.135     -0.516      0.607      -0.336       0.197\n",
       "topic_41       0.1749      0.131      1.331      0.185      -0.085       0.435\n",
       "topic_42       0.0664      0.140      0.475      0.635      -0.210       0.343\n",
       "topic_43      -0.0567      0.131     -0.434      0.665      -0.315       0.202\n",
       "topic_44       0.1538      0.141      1.093      0.276      -0.125       0.432\n",
       "topic_45      -0.0557      0.140     -0.397      0.692      -0.333       0.222\n",
       "topic_46       0.0291      0.137      0.212      0.833      -0.243       0.301\n",
       "topic_47      -0.0551      0.158     -0.348      0.728      -0.368       0.258\n",
       "topic_48       0.2880      0.153      1.883      0.062      -0.015       0.591\n",
       "topic_49      -0.2839      0.175     -1.618      0.108      -0.631       0.063\n",
       "topic_50      -0.0201      0.159     -0.126      0.900      -0.336       0.295\n",
       "topic_51       0.0356      0.153      0.233      0.816      -0.267       0.339\n",
       "topic_52       0.1003      0.152      0.659      0.511      -0.201       0.402\n",
       "topic_53       0.0219      0.158      0.139      0.890      -0.291       0.335\n",
       "topic_54       0.0879      0.183      0.480      0.632      -0.275       0.450\n",
       "topic_55       0.0872      0.149      0.584      0.561      -0.208       0.383\n",
       "topic_56      -0.0199      0.166     -0.120      0.904      -0.348       0.308\n",
       "topic_57      -0.2019      0.153     -1.321      0.189      -0.504       0.101\n",
       "topic_58      -0.0446      0.151     -0.296      0.768      -0.343       0.254\n",
       "topic_59      -0.2698      0.174     -1.555      0.122      -0.613       0.074\n",
       "topic_60       0.1171      0.150      0.779      0.437      -0.180       0.414\n",
       "topic_61       0.1343      0.166      0.808      0.421      -0.195       0.463\n",
       "topic_62      -0.1858      0.154     -1.208      0.229      -0.490       0.119\n",
       "topic_63       0.2926      0.176      1.660      0.099      -0.056       0.641\n",
       "topic_64      -0.1168      0.165     -0.707      0.481      -0.444       0.210\n",
       "topic_65      -0.1047      0.158     -0.662      0.509      -0.418       0.208\n",
       "topic_66       0.2897      0.168      1.727      0.087      -0.042       0.622\n",
       "topic_67      -0.0959      0.171     -0.560      0.576      -0.435       0.243\n",
       "topic_68      -0.1115      0.201     -0.556      0.579      -0.508       0.285\n",
       "topic_69       0.1810      0.172      1.052      0.295      -0.159       0.521\n",
       "topic_70      -0.0184      0.216     -0.085      0.932      -0.447       0.410\n",
       "topic_71       0.0291      0.186      0.156      0.876      -0.339       0.397\n",
       "topic_72      -0.2227      0.206     -1.082      0.281      -0.630       0.185\n",
       "topic_73      -0.1876      0.201     -0.934      0.352      -0.585       0.210\n",
       "==============================================================================\n",
       "Omnibus:                      113.919   Durbin-Watson:                   2.027\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              971.118\n",
       "Skew:                          -2.011   Prob(JB):                    1.33e-211\n",
       "Kurtosis:                      13.018   Cond. No.                         18.3\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(topics_bool_df.astype(float))\n",
    "old_reg = sm.OLS(\n",
    "    tar_a_df.astype(float),\n",
    "    X,\n",
    "# ).fit(maxiter=1000, method='bfgs') # bfgs nm\n",
    ").fit(maxiter=1000) # bfgs nm\n",
    "old_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "580cec0f-6c1c-4dcd-b300-4f0c6e5d687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with statistical significance with alpha of 0.1 (p-values less than this):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const       3.786437\n",
       "topic_4    -0.199912\n",
       "topic_14   -0.237761\n",
       "topic_16    0.259712\n",
       "topic_21   -0.265948\n",
       "topic_31   -0.292100\n",
       "topic_33    0.245556\n",
       "topic_39    0.335619\n",
       "topic_48    0.287960\n",
       "topic_63    0.292625\n",
       "topic_66    0.289676\n",
       "dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"variables with statistical significance with alpha of {p_alpha} (p-values less than this):\")\n",
    "significant_var_coeffs = old_reg.params[old_reg.pvalues < p_alpha]\n",
    "significant_var_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6efc1590-a6fd-4a72-b931-bb7699b32f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with negative effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_4    -0.199912\n",
       "topic_14   -0.237761\n",
       "topic_16    0.259712\n",
       "topic_21   -0.265948\n",
       "topic_31   -0.292100\n",
       "topic_33    0.245556\n",
       "topic_39    0.335619\n",
       "topic_48    0.287960\n",
       "topic_63    0.292625\n",
       "topic_66    0.289676\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with negative effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs < 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ec150ce8-aa8f-422d-a9d7-1e9f16aab0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with positive effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const    3.786437\n",
       "dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with positive effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74270c-a7d8-489b-97eb-27a811e108ce",
   "metadata": {},
   "source": [
    "#### predicting **major** GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d1afcfe8-07c0-41cc-94e6-61469f164618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>GPA for your Major (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")</td> <th>  R-squared:         </th> <td>   0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                                                          <td>OLS</td>                                              <th>  Adj. R-squared:    </th> <td>   0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                                                    <td>Least Squares</td>                                         <th>  F-statistic:       </th> <td>   1.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                                                    <td>Wed, 15 Feb 2023</td>                                        <th>  Prob (F-statistic):</th>  <td>0.0700</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                                                        <td>13:50:19</td>                                            <th>  Log-Likelihood:    </th> <td> -125.66</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>                                             <td>   200</td>                                             <th>  AIC:               </th> <td>   401.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>                                                 <td>   125</td>                                             <th>  BIC:               </th> <td>   648.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                                                     <td>    74</td>                                             <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>                                             <td>nonrobust</td>                                           <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    3.7391</td> <td>    0.238</td> <td>   15.733</td> <td> 0.000</td> <td>    3.269</td> <td>    4.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_0</th>  <td>    0.1043</td> <td>    0.252</td> <td>    0.414</td> <td> 0.680</td> <td>   -0.394</td> <td>    0.603</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_1</th>  <td>   -0.1025</td> <td>    0.105</td> <td>   -0.979</td> <td> 0.330</td> <td>   -0.310</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_2</th>  <td>   -0.0851</td> <td>    0.111</td> <td>   -0.765</td> <td> 0.446</td> <td>   -0.305</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_3</th>  <td>    0.0979</td> <td>    0.116</td> <td>    0.843</td> <td> 0.401</td> <td>   -0.132</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_4</th>  <td>   -0.1224</td> <td>    0.112</td> <td>   -1.097</td> <td> 0.275</td> <td>   -0.343</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_5</th>  <td>   -0.2663</td> <td>    0.113</td> <td>   -2.363</td> <td> 0.020</td> <td>   -0.489</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_6</th>  <td>   -0.2454</td> <td>    0.128</td> <td>   -1.915</td> <td> 0.058</td> <td>   -0.499</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_7</th>  <td>    0.0379</td> <td>    0.125</td> <td>    0.303</td> <td> 0.762</td> <td>   -0.210</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_8</th>  <td>    0.2529</td> <td>    0.121</td> <td>    2.084</td> <td> 0.039</td> <td>    0.013</td> <td>    0.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_9</th>  <td>   -0.1973</td> <td>    0.122</td> <td>   -1.623</td> <td> 0.107</td> <td>   -0.438</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_10</th> <td>   -0.0913</td> <td>    0.135</td> <td>   -0.674</td> <td> 0.501</td> <td>   -0.359</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_11</th> <td>   -0.1681</td> <td>    0.128</td> <td>   -1.318</td> <td> 0.190</td> <td>   -0.421</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_12</th> <td>   -0.0488</td> <td>    0.132</td> <td>   -0.371</td> <td> 0.711</td> <td>   -0.309</td> <td>    0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_13</th> <td>    0.1493</td> <td>    0.133</td> <td>    1.124</td> <td> 0.263</td> <td>   -0.114</td> <td>    0.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_14</th> <td>   -0.4408</td> <td>    0.130</td> <td>   -3.402</td> <td> 0.001</td> <td>   -0.697</td> <td>   -0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_15</th> <td>    0.2262</td> <td>    0.134</td> <td>    1.684</td> <td> 0.095</td> <td>   -0.040</td> <td>    0.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_16</th> <td>    0.2855</td> <td>    0.147</td> <td>    1.941</td> <td> 0.055</td> <td>   -0.006</td> <td>    0.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_17</th> <td>    0.2725</td> <td>    0.144</td> <td>    1.887</td> <td> 0.062</td> <td>   -0.013</td> <td>    0.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_18</th> <td>   -0.1384</td> <td>    0.131</td> <td>   -1.055</td> <td> 0.294</td> <td>   -0.398</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_19</th> <td>    0.0158</td> <td>    0.142</td> <td>    0.111</td> <td> 0.912</td> <td>   -0.266</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_20</th> <td>   -0.1161</td> <td>    0.140</td> <td>   -0.828</td> <td> 0.409</td> <td>   -0.394</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_21</th> <td>   -0.3892</td> <td>    0.144</td> <td>   -2.695</td> <td> 0.008</td> <td>   -0.675</td> <td>   -0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_22</th> <td>    0.1487</td> <td>    0.139</td> <td>    1.071</td> <td> 0.286</td> <td>   -0.126</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_23</th> <td>    0.1259</td> <td>    0.149</td> <td>    0.846</td> <td> 0.399</td> <td>   -0.168</td> <td>    0.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_24</th> <td>   -0.1344</td> <td>    0.150</td> <td>   -0.897</td> <td> 0.371</td> <td>   -0.431</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_25</th> <td>   -0.1681</td> <td>    0.145</td> <td>   -1.157</td> <td> 0.249</td> <td>   -0.456</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_26</th> <td>   -0.0595</td> <td>    0.147</td> <td>   -0.403</td> <td> 0.688</td> <td>   -0.351</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_27</th> <td>   -0.1446</td> <td>    0.141</td> <td>   -1.024</td> <td> 0.308</td> <td>   -0.424</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_28</th> <td>   -0.1639</td> <td>    0.155</td> <td>   -1.059</td> <td> 0.292</td> <td>   -0.470</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_29</th> <td>    0.0369</td> <td>    0.157</td> <td>    0.235</td> <td> 0.814</td> <td>   -0.274</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_30</th> <td>   -0.2351</td> <td>    0.168</td> <td>   -1.396</td> <td> 0.165</td> <td>   -0.568</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_31</th> <td>   -0.4528</td> <td>    0.160</td> <td>   -2.831</td> <td> 0.005</td> <td>   -0.769</td> <td>   -0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_32</th> <td>   -0.0895</td> <td>    0.145</td> <td>   -0.616</td> <td> 0.539</td> <td>   -0.377</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_33</th> <td>    0.0995</td> <td>    0.160</td> <td>    0.621</td> <td> 0.536</td> <td>   -0.218</td> <td>    0.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_34</th> <td>    0.0340</td> <td>    0.165</td> <td>    0.206</td> <td> 0.837</td> <td>   -0.293</td> <td>    0.361</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_35</th> <td>    0.1810</td> <td>    0.165</td> <td>    1.094</td> <td> 0.276</td> <td>   -0.146</td> <td>    0.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_36</th> <td>    0.1838</td> <td>    0.156</td> <td>    1.181</td> <td> 0.240</td> <td>   -0.124</td> <td>    0.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_37</th> <td>   -0.2673</td> <td>    0.163</td> <td>   -1.636</td> <td> 0.104</td> <td>   -0.591</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_38</th> <td>    0.2347</td> <td>    0.162</td> <td>    1.445</td> <td> 0.151</td> <td>   -0.087</td> <td>    0.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_39</th> <td>    0.1441</td> <td>    0.156</td> <td>    0.926</td> <td> 0.356</td> <td>   -0.164</td> <td>    0.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_40</th> <td>   -0.0638</td> <td>    0.160</td> <td>   -0.398</td> <td> 0.692</td> <td>   -0.381</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_41</th> <td>    0.1346</td> <td>    0.157</td> <td>    0.860</td> <td> 0.391</td> <td>   -0.175</td> <td>    0.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_42</th> <td>    0.1568</td> <td>    0.166</td> <td>    0.942</td> <td> 0.348</td> <td>   -0.173</td> <td>    0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_43</th> <td>   -0.1443</td> <td>    0.156</td> <td>   -0.928</td> <td> 0.355</td> <td>   -0.452</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_44</th> <td>    0.1062</td> <td>    0.168</td> <td>    0.633</td> <td> 0.528</td> <td>   -0.226</td> <td>    0.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_45</th> <td>   -0.0147</td> <td>    0.167</td> <td>   -0.088</td> <td> 0.930</td> <td>   -0.345</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_46</th> <td>    0.1111</td> <td>    0.164</td> <td>    0.679</td> <td> 0.499</td> <td>   -0.213</td> <td>    0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_47</th> <td>   -0.2544</td> <td>    0.189</td> <td>   -1.349</td> <td> 0.180</td> <td>   -0.628</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_48</th> <td>    0.1828</td> <td>    0.182</td> <td>    1.003</td> <td> 0.318</td> <td>   -0.178</td> <td>    0.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_49</th> <td>   -0.0397</td> <td>    0.209</td> <td>   -0.190</td> <td> 0.850</td> <td>   -0.454</td> <td>    0.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_50</th> <td>    0.0607</td> <td>    0.190</td> <td>    0.320</td> <td> 0.750</td> <td>   -0.315</td> <td>    0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_51</th> <td>    0.0401</td> <td>    0.182</td> <td>    0.220</td> <td> 0.826</td> <td>   -0.321</td> <td>    0.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_52</th> <td>    0.0811</td> <td>    0.181</td> <td>    0.447</td> <td> 0.656</td> <td>   -0.278</td> <td>    0.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_53</th> <td>   -0.0183</td> <td>    0.188</td> <td>   -0.097</td> <td> 0.923</td> <td>   -0.391</td> <td>    0.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_54</th> <td>    0.1125</td> <td>    0.218</td> <td>    0.515</td> <td> 0.607</td> <td>   -0.319</td> <td>    0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_55</th> <td>    0.1528</td> <td>    0.178</td> <td>    0.859</td> <td> 0.392</td> <td>   -0.199</td> <td>    0.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_56</th> <td>   -0.0996</td> <td>    0.197</td> <td>   -0.505</td> <td> 0.615</td> <td>   -0.490</td> <td>    0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_57</th> <td>    0.0281</td> <td>    0.182</td> <td>    0.154</td> <td> 0.878</td> <td>   -0.332</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_58</th> <td>    0.0584</td> <td>    0.180</td> <td>    0.325</td> <td> 0.746</td> <td>   -0.297</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_59</th> <td>   -0.1356</td> <td>    0.207</td> <td>   -0.656</td> <td> 0.513</td> <td>   -0.545</td> <td>    0.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_60</th> <td>    0.0796</td> <td>    0.179</td> <td>    0.444</td> <td> 0.658</td> <td>   -0.275</td> <td>    0.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_61</th> <td>    0.3932</td> <td>    0.198</td> <td>    1.986</td> <td> 0.049</td> <td>    0.001</td> <td>    0.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_62</th> <td>   -0.4087</td> <td>    0.183</td> <td>   -2.230</td> <td> 0.028</td> <td>   -0.771</td> <td>   -0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_63</th> <td>    0.3635</td> <td>    0.210</td> <td>    1.731</td> <td> 0.086</td> <td>   -0.052</td> <td>    0.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_64</th> <td>   -0.0625</td> <td>    0.197</td> <td>   -0.318</td> <td> 0.751</td> <td>   -0.452</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_65</th> <td>   -0.0946</td> <td>    0.188</td> <td>   -0.502</td> <td> 0.617</td> <td>   -0.468</td> <td>    0.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_66</th> <td>    0.3850</td> <td>    0.200</td> <td>    1.926</td> <td> 0.056</td> <td>   -0.011</td> <td>    0.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_67</th> <td>    0.0445</td> <td>    0.204</td> <td>    0.218</td> <td> 0.828</td> <td>   -0.359</td> <td>    0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_68</th> <td>   -0.1879</td> <td>    0.239</td> <td>   -0.786</td> <td> 0.433</td> <td>   -0.661</td> <td>    0.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_69</th> <td>    0.1732</td> <td>    0.205</td> <td>    0.845</td> <td> 0.400</td> <td>   -0.232</td> <td>    0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_70</th> <td>   -0.0558</td> <td>    0.258</td> <td>   -0.216</td> <td> 0.829</td> <td>   -0.566</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_71</th> <td>   -0.0391</td> <td>    0.222</td> <td>   -0.176</td> <td> 0.860</td> <td>   -0.478</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_72</th> <td>   -0.2273</td> <td>    0.245</td> <td>   -0.927</td> <td> 0.356</td> <td>   -0.713</td> <td>    0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>topic_73</th> <td>   -0.0379</td> <td>    0.239</td> <td>   -0.158</td> <td> 0.874</td> <td>   -0.512</td> <td>    0.436</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>96.030</td> <th>  Durbin-Watson:     </th> <td>   1.877</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 588.629</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.736</td> <th>  Prob(JB):          </th> <td>1.52e-128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>10.653</td> <th>  Cond. No.          </th> <td>    18.3</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                                                  OLS Regression Results                                                                  \n",
       "==========================================================================================================================================================\n",
       "Dep. Variable:     GPA for your Major (For calculating instructions go to: \"http://www.back2college.com/gpa.htm\")   R-squared:                       0.444\n",
       "Model:                                                                                                        OLS   Adj. R-squared:                  0.115\n",
       "Method:                                                                                             Least Squares   F-statistic:                     1.350\n",
       "Date:                                                                                            Wed, 15 Feb 2023   Prob (F-statistic):             0.0700\n",
       "Time:                                                                                                    13:50:19   Log-Likelihood:                -125.66\n",
       "No. Observations:                                                                                             200   AIC:                             401.3\n",
       "Df Residuals:                                                                                                 125   BIC:                             648.7\n",
       "Df Model:                                                                                                      74                                         \n",
       "Covariance Type:                                                                                        nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.7391      0.238     15.733      0.000       3.269       4.209\n",
       "topic_0        0.1043      0.252      0.414      0.680      -0.394       0.603\n",
       "topic_1       -0.1025      0.105     -0.979      0.330      -0.310       0.105\n",
       "topic_2       -0.0851      0.111     -0.765      0.446      -0.305       0.135\n",
       "topic_3        0.0979      0.116      0.843      0.401      -0.132       0.328\n",
       "topic_4       -0.1224      0.112     -1.097      0.275      -0.343       0.098\n",
       "topic_5       -0.2663      0.113     -2.363      0.020      -0.489      -0.043\n",
       "topic_6       -0.2454      0.128     -1.915      0.058      -0.499       0.008\n",
       "topic_7        0.0379      0.125      0.303      0.762      -0.210       0.286\n",
       "topic_8        0.2529      0.121      2.084      0.039       0.013       0.493\n",
       "topic_9       -0.1973      0.122     -1.623      0.107      -0.438       0.043\n",
       "topic_10      -0.0913      0.135     -0.674      0.501      -0.359       0.177\n",
       "topic_11      -0.1681      0.128     -1.318      0.190      -0.421       0.084\n",
       "topic_12      -0.0488      0.132     -0.371      0.711      -0.309       0.212\n",
       "topic_13       0.1493      0.133      1.124      0.263      -0.114       0.412\n",
       "topic_14      -0.4408      0.130     -3.402      0.001      -0.697      -0.184\n",
       "topic_15       0.2262      0.134      1.684      0.095      -0.040       0.492\n",
       "topic_16       0.2855      0.147      1.941      0.055      -0.006       0.577\n",
       "topic_17       0.2725      0.144      1.887      0.062      -0.013       0.558\n",
       "topic_18      -0.1384      0.131     -1.055      0.294      -0.398       0.121\n",
       "topic_19       0.0158      0.142      0.111      0.912      -0.266       0.298\n",
       "topic_20      -0.1161      0.140     -0.828      0.409      -0.394       0.161\n",
       "topic_21      -0.3892      0.144     -2.695      0.008      -0.675      -0.103\n",
       "topic_22       0.1487      0.139      1.071      0.286      -0.126       0.424\n",
       "topic_23       0.1259      0.149      0.846      0.399      -0.168       0.420\n",
       "topic_24      -0.1344      0.150     -0.897      0.371      -0.431       0.162\n",
       "topic_25      -0.1681      0.145     -1.157      0.249      -0.456       0.119\n",
       "topic_26      -0.0595      0.147     -0.403      0.688      -0.351       0.232\n",
       "topic_27      -0.1446      0.141     -1.024      0.308      -0.424       0.135\n",
       "topic_28      -0.1639      0.155     -1.059      0.292      -0.470       0.142\n",
       "topic_29       0.0369      0.157      0.235      0.814      -0.274       0.348\n",
       "topic_30      -0.2351      0.168     -1.396      0.165      -0.568       0.098\n",
       "topic_31      -0.4528      0.160     -2.831      0.005      -0.769      -0.136\n",
       "topic_32      -0.0895      0.145     -0.616      0.539      -0.377       0.198\n",
       "topic_33       0.0995      0.160      0.621      0.536      -0.218       0.417\n",
       "topic_34       0.0340      0.165      0.206      0.837      -0.293       0.361\n",
       "topic_35       0.1810      0.165      1.094      0.276      -0.146       0.508\n",
       "topic_36       0.1838      0.156      1.181      0.240      -0.124       0.492\n",
       "topic_37      -0.2673      0.163     -1.636      0.104      -0.591       0.056\n",
       "topic_38       0.2347      0.162      1.445      0.151      -0.087       0.556\n",
       "topic_39       0.1441      0.156      0.926      0.356      -0.164       0.452\n",
       "topic_40      -0.0638      0.160     -0.398      0.692      -0.381       0.254\n",
       "topic_41       0.1346      0.157      0.860      0.391      -0.175       0.444\n",
       "topic_42       0.1568      0.166      0.942      0.348      -0.173       0.486\n",
       "topic_43      -0.1443      0.156     -0.928      0.355      -0.452       0.163\n",
       "topic_44       0.1062      0.168      0.633      0.528      -0.226       0.438\n",
       "topic_45      -0.0147      0.167     -0.088      0.930      -0.345       0.316\n",
       "topic_46       0.1111      0.164      0.679      0.499      -0.213       0.435\n",
       "topic_47      -0.2544      0.189     -1.349      0.180      -0.628       0.119\n",
       "topic_48       0.1828      0.182      1.003      0.318      -0.178       0.543\n",
       "topic_49      -0.0397      0.209     -0.190      0.850      -0.454       0.374\n",
       "topic_50       0.0607      0.190      0.320      0.750      -0.315       0.437\n",
       "topic_51       0.0401      0.182      0.220      0.826      -0.321       0.401\n",
       "topic_52       0.0811      0.181      0.447      0.656      -0.278       0.440\n",
       "topic_53      -0.0183      0.188     -0.097      0.923      -0.391       0.355\n",
       "topic_54       0.1125      0.218      0.515      0.607      -0.319       0.544\n",
       "topic_55       0.1528      0.178      0.859      0.392      -0.199       0.505\n",
       "topic_56      -0.0996      0.197     -0.505      0.615      -0.490       0.291\n",
       "topic_57       0.0281      0.182      0.154      0.878      -0.332       0.389\n",
       "topic_58       0.0584      0.180      0.325      0.746      -0.297       0.414\n",
       "topic_59      -0.1356      0.207     -0.656      0.513      -0.545       0.274\n",
       "topic_60       0.0796      0.179      0.444      0.658      -0.275       0.434\n",
       "topic_61       0.3932      0.198      1.986      0.049       0.001       0.785\n",
       "topic_62      -0.4087      0.183     -2.230      0.028      -0.771      -0.046\n",
       "topic_63       0.3635      0.210      1.731      0.086      -0.052       0.779\n",
       "topic_64      -0.0625      0.197     -0.318      0.751      -0.452       0.327\n",
       "topic_65      -0.0946      0.188     -0.502      0.617      -0.468       0.278\n",
       "topic_66       0.3850      0.200      1.926      0.056      -0.011       0.781\n",
       "topic_67       0.0445      0.204      0.218      0.828      -0.359       0.448\n",
       "topic_68      -0.1879      0.239     -0.786      0.433      -0.661       0.285\n",
       "topic_69       0.1732      0.205      0.845      0.400      -0.232       0.579\n",
       "topic_70      -0.0558      0.258     -0.216      0.829      -0.566       0.455\n",
       "topic_71      -0.0391      0.222     -0.176      0.860      -0.478       0.400\n",
       "topic_72      -0.2273      0.245     -0.927      0.356      -0.713       0.258\n",
       "topic_73      -0.0379      0.239     -0.158      0.874      -0.512       0.436\n",
       "==============================================================================\n",
       "Omnibus:                       96.030   Durbin-Watson:                   1.877\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              588.629\n",
       "Skew:                          -1.736   Prob(JB):                    1.52e-128\n",
       "Kurtosis:                      10.653   Cond. No.                         18.3\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(topics_bool_df.astype(float))\n",
    "old_reg = sm.OLS(\n",
    "    tar_b_df.astype(float),\n",
    "    X,\n",
    "# ).fit(maxiter=1000, method='bfgs') # bfgs nm\n",
    ").fit(maxiter=1000) # bfgs nm\n",
    "old_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cef99586-22ad-4dd8-ac9a-857b4087d816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with statistical significance with alpha of 0.1 (p-values less than this):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const       3.739071\n",
       "topic_5    -0.266268\n",
       "topic_6    -0.245434\n",
       "topic_8     0.252900\n",
       "topic_14   -0.440832\n",
       "topic_15    0.226218\n",
       "topic_16    0.285507\n",
       "topic_17    0.272489\n",
       "topic_21   -0.389247\n",
       "topic_31   -0.452752\n",
       "topic_61    0.393203\n",
       "topic_62   -0.408704\n",
       "topic_63    0.363477\n",
       "topic_66    0.385035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"variables with statistical significance with alpha of {p_alpha} (p-values less than this):\")\n",
    "significant_var_coeffs = old_reg.params[old_reg.pvalues < p_alpha]\n",
    "significant_var_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d68599bd-738e-4171-971f-675dbbbc65c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with negative effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_5    -0.266268\n",
       "topic_6    -0.245434\n",
       "topic_8     0.252900\n",
       "topic_14   -0.440832\n",
       "topic_15    0.226218\n",
       "topic_16    0.285507\n",
       "topic_17    0.272489\n",
       "topic_21   -0.389247\n",
       "topic_31   -0.452752\n",
       "topic_61    0.393203\n",
       "topic_62   -0.408704\n",
       "topic_63    0.363477\n",
       "topic_66    0.385035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with negative effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs < 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b96e605-3381-42a6-8d13-59e135e45a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables with positive effects and their coefficients:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const    3.739071\n",
       "dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"variables with positive effects and their coefficients:\")\n",
    "significant_var_coeffs[significant_var_coeffs > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9ab8d-b623-4881-8676-5251cd7e6404",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f4186-455b-4189-bb1a-c99382b4bb73",
   "metadata": {},
   "source": [
    "The variable to look at is the   \n",
    "**Adj. R-squared**  \n",
    "variable.  \n",
    "This will tell us what proportion of the variation in the GPA is explained by the Topics.  \n",
    "Nearly all are 0 or below 0.  \n",
    "There are a couple of tests we ran that gave us a value of ~.115.  \n",
    "This was when using the boolean topics table as input (indicating if the topic was included at all in their essay, rather than the proportion of sentences in a topic),  \n",
    "and for predicting major GPA.  Whether the GPA was normalized or in raw format did not matter.  \n",
    "Because the Adjusted R-squared value is so low, we cannot conclude with that the topics are highly indicative of GPA outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdea5e2-c30c-4fc1-839a-d8c8d0f313d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
